# 无监督图像融合总结
## Github推荐
| **说明** | **链接**
| --- | --- |
| **推荐github(23年之前)** | https://github.com/Linfeng-Tang  
| **综述代码(评价指标matlab, 部分数据集)** | https://github.com/Linfeng-Tang/Image-Fusion  
| **红外-可见光图像融合(数据集总结较新)** | https://github.com/liushh39/IVIF-Code-Interpretation  
| **CV大类arxiv(最新，包括融合)** | https://github.com/cuixing158/Awesome-CV-MasterHub 
| **融合(接上)** | https://github.com/cuixing158/Awesome-CV-MasterHub/blob/main/docs/Image-Fusion.md  


## 测试出来主客观都比较SOTA的论文（训练后不一定SOTA）
| **说明** | **论文**
| --- | --- |
| **SwinFusion(通用图像融合, 22IJAS)** | [SwinFusion: Cross-domain Long-range Learning for General Image Fusion via Swin Transformer](https://ieeexplore.ieee.org/document/9812535)  
| **TC-MoA(通用图像融合, 23CVPR)** | [Task-Customized Mixture of Adapters for General Image Fusion](https://arxiv.org/abs/2403.12494v2) 
| **CDDFuse(无监督, 23CVPR, 部分python指标)** | [CDDFuse: Correlation-Driven Dual-Branch Feature Decomposition for Multi-Modality Image Fusion](https://openaccess.thecvf.com/content/CVPR2023/html/Zhao_CDDFuse_Correlation-Driven_Dual-Branch_Feature_Decomposition_for_Multi-Modality_Image_Fusion_CVPR_2023_paper.html)  |
| **GeSeNet(医学图像融合, 23TNNLS)** | [GeSeNet: A General Semantic-Guided Network With Couple Mask Ensemble for Medical Image Fusion](https://ieeexplore.ieee.org/abstract/document/10190200)  
| **ALMFNet(医学图像融合, 23TCSVT)** | [Learning to Search a Lightweight Generalized Network for Medical Image Fusion](https://ieeexplore.ieee.org/abstract/document/10360160)  
| **DCEvo(红外-可见光图像融合, 25CVPR)** | [DCEvo: Discriminative Cross-Dimensional Evolutionary Learning for Infrared and Visible Image Fusion](https://arxiv.org/abs/2503.17673v1)  
| **SHIP(红外-可见光图像融合, 24CVPR)** | [Probing Synergistic High-Order Interaction in Infrared and Visible Image Fusion (CVPR)](https://openaccess.thecvf.com/content/CVPR2024/html/Zheng_Probing_Synergistic_High-Order_Interaction_in_Infrared_and_Visible_Image_Fusion_CVPR_2024_paper.html) 
| **SHIP(红外-可见光图像融合, 24TPAMI)** | [Probing Synergistic High-Order Interaction for Multi-Modal Image Fusion (TPAMI)](https://ieeexplore.ieee.org/abstract/document/10706703)  
| **LDFusion(红外-可见光图像融合)** | [Infrared and visible Image Fusion with Language-driven Loss in CLIP Embedding Space](https://arxiv.org/abs/2402.16267)

	
## 论文归类(不完全)
### 多任务联合优化
#### 融合+分割/目标检测 (语义信息)
| **题目** | **日期** | **摘要** | **录用信息** | **代码** | **备注**
| --- | --- | --- | --- | --- | --- |
| **[SegMiF: 用于图像融合和分割的多交互式特征学习和全职多模态基准测试(Arxiv)](http://arxiv.org/abs/2308.02097v1)** | 2023-08-04 | <details><summary>Show</summary><p>多模态图像融合和分割在自动驾驶和机器人作中起着至关重要的作用。早期的工作只集中在提高一项任务的性能，\emph{eg.，} 融合或分割，这使得它很难达到~'两全其美'。为了克服这个问题，在本文中，我们提出了一种用于图像融合和 Segmentation 的 Multi-interactive Feature 学习架构，即 SegMiF，并利用双任务相关性来促进这两个任务的性能。SegMiF 采用级联结构，包含一个融合子网络和一个常用的分割子网络。通过巧妙地桥接两个组件之间的中间特征，从分割任务中学到的知识可以有效地协助融合任务。此外，受益的融合网络支持分割网络执行得更自命不凡。此外，建立了分层交互式注意力块，以确保两个任务之间所有重要信息的细粒度映射，从而使模态/语义特征完全互交互。此外，还引入了动态权重因子，可以自动调整每个任务的相应权重，可以平衡交互特征的对应关系，突破费力调优的限制。此外，我们构建了一个智能多波双目成像系统，并收集了一个全职多模态基准，具有 15 个带注释的像素级类别，用于图像融合和分割。对几个公共数据集和我们的基准测试进行的广泛实验表明，所提出的方法输出视觉上吸引人的融合图像，并且性能一般在现实世界中，分割 mIoU 高于最先进的方法。</p></details> | ICCV2023 | [Code Link](https://github.com/JinyuanLiu-CV/SegMiF) | 刘晋源 |
| **[MetaFusion：基于目标检测的元特征嵌入的红外和可见光图像融合](https://paperswithcode.com/paper/metafusion-infrared-and-visible-image-fusion)** | 2023 | <details><summary>Show</summary><p> 融合红外和可见光图像可以为后续的目标检测任务提供更多的纹理细节。相反，检测任务提供对象语义信息来改进红外和可见光图像的融合。因此，使用相互促进的联合融合和检测学习越来越受到关注。然而，这两个不同级别任务之间的特征差距阻碍了进展。针对这一问题，本文提出了一种基于目标检测元特征嵌入的红外和可见光图像融合。核心思想是元特征嵌入模型旨在根据融合网络能力生成对象语义特征，因此语义特征自然与融合特征兼容。它通过模拟元学习进行了优化。此外，我们进一步实现了融合和检测任务之间的相互促进学习，以提高其性能。在三个公共数据集上的综合实验证明了我们方法的有效性。 </p></details> | CVPR2023 |  [Code Link](https://github.com/wdzhao123/MetaFusion) | 对比实验 |
| **[KDFuse：一种基于跨领域知识蒸馏的高级视觉任务驱动的红外与可见光图像融合方法](https://www.sciencedirect.com/science/article/pii/S156625352500017X)** | 2025-01 | <details><summary>Show</summary><p> 了增强融合特征的全面性并满足高级视觉任务的要求，一些融合方法试图通过与高级语义特征直接交互来协调融合过程。然而，由于高级语义域和融合表示域之间的显著差异，有可能提高直接交互的协作方法的有效性。为了克服这一障碍，该文提出一种基于跨域知识蒸馏的高级视觉任务驱动的红外与可见光图像融合方法，简称KDFuse。KDFuse 通过跨域知识蒸馏将多任务感知表示引入同一领域。通过促进语义信息和融合信息在等效级别的交互，它有效地缩小了语义域和融合域之间的差距，实现了多任务协同融合。具体来说，为了获得指导融合网络所必需的高级语义表示，通过多域交互蒸馏模块 （MIDM） 建立教学关系以实现多任务协作。多尺度语义感知模块 （MSPM） 旨在通过跨领域知识蒸馏来学习捕获语义信息的能力，构建语义细节集成模块 （SDIM） 以将融合级语义表示与融合级视觉表示集成。此外，为了平衡融合过程中的语义和视觉表示，在损失函数中引入了傅里叶变换。广泛的综合实验证明了所提出的方法在图像融合和下游任务中的有效性。 </p></details> | Inf. Fusion2025 |  [Code](https://github.com/lxq-jnu/KDFuse) |
| **[SSDFusion：一种用于可见光和红外图像融合的场景语义分解方法](https://www-sciencedirect-com-s-225.libdb.csu.edu.cn/science/article/pii/S0031320325001177?via%3Dihub)** | 2025-02 | <details><summary>Show</summary><p> 可见光和红外图像融合旨在生成具有全面场景理解和详细上下文信息的融合图像。然而，现有方法通常难以充分处理不同模态之间的关系并针对下游应用进行优化。为了应对这些挑战，我们提出了一种基于场景语义分解的新型可见光和红外图像融合方法，称为 SSDFusion。我们的方法采用多级编码器融合网络，融合模块实现所提出的场景语义分解和融合策略，分别提取和融合场景相关和语义相关组件，并将融合的语义注入场景特征中，丰富融合特征中的上下文信息，同时保持融合图像的保真度。此外，在训练过程中，我们进一步融入了元特征嵌入，将编码器融合网络与下游应用网络连接起来，增强了我们的方法提取语义、优化融合效果以及服务语义分割等任务的能力。大量实验表明，SSDFusion 实现了最先进的图像融合性能，同时增强了语义分割任务的结果。我们的方法弥合了基于特征分解的图像融合与高级视觉应用之间的差距，为多模态图像融合提供了更有效的范式。 </p></details> | PR2025 | [Code](https://github.com/YiXian-Xiao/SSDFusion) |
| **[基于模型的红外可见光图像融合网络，协同优化](https://www.sciencedirect.com/science/article/pii/S0957417424025065)** | 2024-11 | <details><summary>Show</summary><p> 红外和可见光图像融合的主要目标是将来自多模态图像的信息融合成包含突出目标和丰富细节的融合图像。现有的融合方法要么主要依赖于缺乏可解释性和可推广性的 “黑盒 ”模型，要么专注于提高图像的视觉吸引力，而忽略了语义信息。为了解决这些缺点，本文介绍了一种面向任务的红外可见光图像融合网络，该网络将基于模型和数据驱动的正则化与协同优化相结合。首先，基于展开的协同优化公式，我们设计了一个循环协同训练策略，将图像融合和语义分割模块级联，从而用语义信息增强内容信息。然后，考虑到深度神经网络的高计算效率和传统优化模型的可解释性，我们将一个经过充分研究的融合算法显式编码到一个展开的自编码器网络中，该网络具有用于融合模块的双私有编码器。此外，我们的模型很紧凑，可以在资源有限的情况下提供有效的解决方案。对图像融合和语义感知评估的实验结果进行全面的定性和定量分析表明，所提出的方法完全保留了热目标和背景纹理细节，在图像质量和高级语义方面超越了最先进的替代方案。 </p></details> | ESWA2025 |  []() |
| **[SFINet：用于全时红外和可见光图像融合的语义特征交互式学习网络](https://www.sciencedirect.com/science/article/pii/S095741742402339X)** | 2024-10 | <details><summary>Show</summary><p> 红外和可见光图像融合旨在组合来自各种源图像的数据以生成高质量的图像。然而，许多融合方法通常将视觉质量置于语义信息之上。为了解决这个问题，我们提出了一个用于全时红外和可见光图像的语义特征交互式学习网络 （SFINet）。SFINet 通过语义特征交互 （SFI） 模块包含图像融合网络和图像分割网络。图像融合网络采用多尺度特征提取 （MFE） 模块来捕获多个尺度的全局和局部信息。同时，它使用双注意力特征融合 （DAFF） 模块执行互补信息的自适应融合。图像分割网络使用 SFI 模块引导图像融合网络进行语义特征交互。对比结果表明，所提方法在图像融合和语义分割任务中优于最先进的 （SOTA） 模型。 </p></details> | ESWA2025 |  [Code](https://github.com/songwenhao123/SFINet) |
| **[用于红外-可见光图像融合的语义感知和多导向网络(Arxiv)](http://arxiv.org/abs/2407.06159v2)** | 2024-08-03 | <details><summary>Show</summary><p>多模态图像融合旨在融合来自两个源图像的特定模态和共享模态信息。针对复杂场景特征提取不足和语义感知不足的问题，该文重点介绍了如何通过高效提取互补特征和多导向特征聚合来对关联驱动的分解特征进行建模并推理高级图表示。我们提出了一种三分支编码器-解码器架构以及相应的融合层作为融合策略。使用具有 Multi-Dconv Transposed Attention 和 Local-enhanced Feed Forward 网络的 transformer 在深度卷积后提取浅层特征。在三个并行分支编码器中，交叉注意和可逆块 （CAI） 可以提取局部特征并保留高频纹理细节。具有残差连接的基本特征提取模块 （BFE） 可以捕获长程依赖性并增强共享模态表达能力。引入图推理模块 （GR） 来推理高级跨模态关系，同时提取低级细节特征作为 CAI 的特定模态互补信息。实验表明，与最先进的方法相比，我们的方法在可见光/红外图像融合和医学图像融合任务中获得了有竞争力的结果。此外，我们在后续任务方面超过了其他融合方法，在目标检测方面平均得分高出 8.27% mAP@0.5，在语义分割方面高出 5.85% mIoU。</p></details> |  | None |
| **[Target-aware Dual Adversarial Learning and a Multi-scenario Multi-Modality Benchmark to Fuse Infrared and Visible for Object Detection]()** | 2022 | <details><summary>Show</summary><p> 本研究探讨了融合红外和可见光图像的问题，这些图像在目标检测中表现出不同的特征。为了生成高视觉质量的图像，以前的方法发现了两种模态的共同点，并通过迭代优化或深度网络在共同空间上进行融合。这些方法忽略了模态差异意味着互补的形成，这对融合和随后的目标检测任务都极其重要。本文提出了融合和检测联合问题的双层优化公式，然后展开了一个用于融合的目标感知双对抗学习 (TarDAL) 网络和一个常用的检测网络。具有一个生成器和双判别器的融合网络在寻找共同点的同时从差异中学习，这保留了红外目标的结构信息和可见光的纹理细节。此外，我们构建了一个具有校准红外和光学传感器的同步成像系统，并收集了目前涵盖广泛场景的最全面的基准。在几个公共数据集上进行的广泛实验和我们的基准测试表明，我们的方法不仅能产生视觉上吸引人的融合，而且比最先进的方法具有更高的检测 mAP。 </p></details> | cvpr2022 |  [Code](https://github.com/JinyuanLiu-CV/TarDAL) | 刘晋源
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |

#### 融合+配准 (包含部分语义信息)
| **题目** | **日期** | **摘要** | **录用信息** | **代码** | **备注**
| --- | --- | --- | --- | --- | --- |
| **[BSAFusion：用于未对齐医学图像融合的双向逐步特征对齐网络(Arxiv)](http://arxiv.org/abs/2412.08050v2)** | 2024-12-13 | <details><summary>Show</summary><p>如果未对齐的多模态医学图像可以在统一的处理框架内使用单阶段方法同时对齐和融合，不仅可以实现双重任务的相互促进，还有助于降低模型的复杂性。然而，该模型的设计面临着特征融合和对齐要求不兼容的挑战;具体来说，特征对齐需要相应特征之间的一致性，而特征融合需要特征之间相互补充。为了应对这一挑战，本文提出了一种称为双向逐步特征对齐和融合 （BSFA-F） 策略的未对齐医学图像融合方法。为了减少模态差异对跨模态特征匹配的负面影响，我们将模态无差异特征表示 （MDF-FR） 方法纳入 BSFA-F 中。MDF-FR 利用模态特征表示头 （MFRH） 来集成输入图像的全局信息。通过将当前图像的 MFRH 中包含的信息注入到其他模态图像中，在保留不同图像携带的互补信息的同时，有效地减少了模态差异对特征对齐的影响。在特征对齐方面，BSFA-F 采用基于两点之间矢量位移路径独立性的双向逐步对齐变形场预测策略。该策略解决了单步对齐中大跨度和变形场预测不准确的问题。最后，Multi-Modal Feature Fusion 模块实现了对齐特征的融合。多个数据集的实验结果证明了我们方法的有效性。源代码可在 https://github.com/slrl123/BSAFusion 上获得。</p></details> | AAAI2025 | [Code Link](https://github.com/slrl123/BSAFusion) | Huafeng Li |
| **[MulFS-CAP： 用于未配准红外可见光图像融合的多模态融合监督跨模态对齐感知](https://ieeexplore-ieee-org-s-225.libdb.csu.edu.cn/document/10856402)** | 2025-01 | <details><summary>Show</summary><p> 在这项研究中，我们提出了多模态融合监督跨模态对齐感知 （MulFS-CAP），这是一种用于未配准红外可见光图像单阶段融合的新框架。传统的两阶段方法依赖于显式配准算法在空间上对齐源图像，这通常会增加复杂性。相比之下，MulFS-CAP 将隐式配准与熔融无缝融合，简化了流程并增强了对实际应用的适用性。MulFS-CAP 利用共享的浅层特征编码器将未配准的红外可见光图像合并到一个阶段中。为了满足特征级对齐和融合的特定要求，我们通过可学习的模态字典开发了一种一致的特征学习方法。该字典为单峰特征提供补充信息，从而保持单个和融合多模态特征之间的一致性。因此，MulFS-CAP 有效降低了模态方差对跨模态特征对齐的影响，从而允许同时配准和融合。此外，在 MulFS-CAP 中，我们提出了一种新的跨模态对齐方法，创建了一个相关矩阵来详细说明源图像之间的像素关系。该矩阵有助于在红外和可见光图像中对齐特征，进一步完善融合过程。上述设计使 MulFS-CAP 更加轻量级、有效且无需显式注册。来自不同数据集的实验结果表明了我们提出的方法的有效性及其相对于最先进的两阶段方法的优越性。 </p></details> | TPAMI2025 |  [Code](https://github.com/YR0211/MulFS-CAP) |
| **[通过一步式渐进密集配准改进错位多模态图像融合(Arxiv)](http://arxiv.org/abs/2308.11165v1)** | 2023-08-22 | <details><summary>Show</summary><p>多模态图像之间的错位给图像融合带来了挑战，表现为结构失真和边缘重影。现有的工作通常采用先注册后融合的方式，通常采用两个级联注册阶段，即粗注册和精细注册。这两个阶段都直接估计相应的目标变形场。在本文中，我们认为分离的两阶段配准不紧凑，目标变形场的直接估计不够准确。为了应对这些挑战，我们提出了一种跨模态多尺度渐进式密集配准 （C-MPDR） 方案，该方案仅使用一阶段优化完成从粗到精的配准，从而提高了未对准的多模态图像的融合性能。具体来说，涉及两个关键组件，一个密集变形场融合 （DFF） 模块和一个渐进特征精细 （PFF） 模块。DFF 在当前尺度上聚合预测的多尺度变形子场，而 PFF 逐步细化剩余的未对准特征。两者协同工作以准确估计最终变形场。此外，我们还开发了一个基于 Transformer-Conv 的融合 （TCF） 子网络，该子网络考虑了局部和远程特征依赖性，使我们能够从注册的红外和可见光图像中捕获更多信息丰富的特征，以生成高质量的融合图像。广泛的实验分析证明了所提出的方法在未对准的跨模态图像融合方面的优越性。</p></details> | TCSVT2024 | [Code Link](https://github.com/wdhudiekou/IMF) | 刘晋源 |
| **[C2RF: Bridging Multi-modal Image Registration and Fusion via Commonality Mining and Contrastive Learning](https://link.springer.com/article/10.1007/s11263-025-02427-1)** | 2025-04-15 | <details><summary>Show</summary><p> 现有的图像融合方法通常仅适用于严格对齐的源图像，当源图像未对齐时，它们会引入不需要的伪影，从而影响视觉感知和下游应用程序。在这项工作中，我们提出了一个基于共性挖掘和对比学习的相互促进的多模态图像配准和融合框架，命名为 C2RF。我们将多模态图像自适应地分解为模态不变的共同特征和模态特定的独特特征。有效的解开不仅降低了跨模式配准的难度，还促进了有目的的信息聚合。此外，C2RF 结合了基于融合的对比学习，以明确建模对配准的融合要求，这打破了配准和融合相互独立的困境。对齐和未对齐的融合结果充当正样本和负样本，以指导配准优化。特别是，使用硬负样本挖掘生成的负样本使我们的融合结果远离伪影。大量实验表明，C2RF 在多模态图像配准和融合方面都优于其他竞争对手，特别是在增强图像融合对错位的鲁棒性方面。源代码已于 https://github.com/QinglongYan-hub/C2RF 发布 </p></details> | IJCV2025 |  [Code Link](https://github.com/Linfeng-Tang/C2RF) | 唐霖峰
| **[通过相互增强的跨模态图像生成和配准对错位的PAT和MRI图像进行无监督融合](https://ieeexplore.ieee.org/document/10374392)** | 2023-12-26 | <details><summary>Show</summary><p> 光声断层扫描 （PAT） 和磁共振成像 （MRI） 是临床前研究中广泛使用的两种先进成像技术。PAT 具有高光学对比度和较深的成像范围，但软组织对比度较差，而 MRI 提供出色的软组织信息，但时间分辨率较差。尽管最近在使用预对准多模态数据的医学图像融合方面取得了进展，但由于图像未对准和空间失真，PAT-MRI 图像融合仍然具有挑战性。为了解决这些问题，我们提出了一种名为 PAMRFuse 的无监督多阶段深度学习框架，用于未对准的 PAT 和 MRI 图像融合。PAMRFuse 包括一个多模态到单峰配准网络，用于精确对齐输入 PAT-MRI 图像对和一个自聚焦融合网络，用于选择信息丰富的特征进行融合。我们在配准网络中采用了端到端的相互加强模式，实现了跨模态图像生成和配准的联合优化。据我们所知，这是 PAT 和 MRI 错位信息融合的首次尝试。定性和定量实验结果表明，我们的方法在融合从商业成像系统捕获的小动物 PAT-MRI 图像方面的优异性能。 </p></details> | TMI2025 |  [Code](https://github.com/zhongniuniu/PAMRFuse) |
| **[通过跨模态图像生成和配准实现无监督错位红外和可见光图像融合](https://arxiv.org/abs/2205.11876)** | 2022-05 | <details><summary>Show</summary><p> 近年来，基于学习的图像融合方法在预配多模态数据方面取得了许多进展，但由于空间变形和难以缩小跨模态差异，在处理错位的多模态数据时遇到了严重的重影。为了克服这些障碍，在本文中，我们提出了一种强大的跨模态生成-配准范式，用于无监督未对准红外和可见光图像融合 （IVIF）。具体来说，我们提出了一种跨模态感知风格传输网络 （CPSTN） 来生成一个以可见图像作为输入的伪红外图像。得益于 CPSTN 良好的几何保持能力，生成的伪红外图像具有清晰的结构，更有利于将跨模态图像对准转化为单模态配准，并结合红外图像的结构敏感性。在这种情况下，我们引入了一种多级细化配准网络 （MRRN） 来预测畸变和伪红外图像之间的位移矢量场，并在单模态设置下重建配准红外图像。此外，为了更好地融合注册的红外图像和可见光图像，我们提出了一种交互融合模块 （IFM） 功能，以自适应地选择更有意义的特征在双路径交互融合网络 （DIFN） 中进行融合。大量的实验结果表明，所提出的方法在错位跨模态图像融合方面具有优异的能力。 </p></details> | IJCAI2022 |  [Code](https://github.com/wdhudiekou/UMF-CMGR) | 刘晋源
| **[SuperFusion：具有语义感知功能的多功能图像配准和融合网络](https://ieeexplore.ieee.org/document/9970457)** | 2022-12-05 | <details><summary>Show</summary><p> Image fusion 旨在整合源图像中的互补信息，以合成综合表征成像场景的融合图像。但是，现有的图像融合算法仅适用于严格对齐的源图像，当输入图像有轻微的偏移或变形时，会导致融合结果中出现严重的伪影。此外，融合结果通常只具有良好的视觉效果，而忽略了高级视觉任务的语义要求。本研究将图像配准、图像融合和高级视觉任务的语义要求整合到一个框架中，并提出了一种名为 SuperFusion 的新型图像配准和融合方法。具体来说，我们设计了一个配准网络来估计双向变形场，以在光度和终点约束的监督下纠正输入图像的几何失真。配准和融合组合在一个对称方案中，虽然可以通过优化朴素融合损失来实现相互促进，但对称融合输出的单模态一致约束进一步增强了这种融合。此外，图像融合网络配备了全局空间注意力机制，以实现自适应特征集成。此外，部署了基于预训练分割模型和 Lovasz-Softmax 损失的语义约束，以指导融合网络更多地关注高级视觉任务的语义需求。对图像配准、图像融合和语义分割任务的广泛实验表明，与最先进的替代方案相比，我们的 SuperFusion 具有优越性。 </p></details> | IEEE/CAA Journal of Automatica Sinica2022 |  [Code](https://github.com/wdhudiekou/UMF-CMGR) | 唐霖峰/Jiayi Ma
| **[语义引领一切：从语义角度迈向统一的图像配准和融合](https://www.sciencedirect.com/science/article/abs/pii/S1566253523001513)** | 2023-05-25 | <details><summary>Show</summary><p> 红外-可见光图像配准和融合是密切相关的过程，在统一的框架中实施协调配准和融合是一个有吸引力的问题。现有方法的配准精度在某些场景下无法满足融合需求，影响了融合的视觉表现。此外，作为图像预处理步骤，级联配准和融合后的网络速度不足以完成更高级的任务，从而限制了这些方法的可用性。为了解决上述问题，我们提出了一个使用语义来引导所有人的网络，称为 SemLA，能够以高效和稳健的方式统一配准和融合过程。我们的关键思想是在网络的所有阶段显式嵌入语义信息。特别是，SemLA 采用协调方法，包括 注册和语义特征的联合训练，以确保高效的网络运行。语义感知地图的校准和它们的空间结构信息的描述相辅相成，以获得更准确的配准。此外，语义引导的融合过程增强了语义对象内互补信息的表示，同时有效抑制了对齐图像重叠区域分界线引起的视觉干扰。不同实验的结果表明，与最先进的方法相比，我们的 SemLA 在性能和效率之间有更好的权衡，并且适应高级视觉任务的语义需求。 </p></details> | Inf. fusion2023 |  [Code](https://github.com/xiehousheng/SemLA?tab=readme-ov-file) |
| **[MURF：相辅相成的多模态图像配准和融合](https://ieeexplore.ieee.org/document/10145843)** | 2023-06-07 | <details><summary>Show</summary><p> 现有的图像融合方法通常仅限于对齐的源图像，并且在图像未对齐时必须“容忍”视差。同时，不同模态之间的巨大差异对多模态图像配准构成了重大挑战。本研究提出了一种称为 MURF 的新方法，其中图像配准和融合首次相辅相成，而不是被视为单独的问题。MURF 利用三个模块：共享信息提取模块 （SIEM）、多尺度粗调模块 （MCRM） 和精细配准与融合模块 （F2M）。登记以粗到细的方式进行。在粗略配准期间，SIEM 首先将多模态图像转换为单模态共享信息，以消除模态差异。然后，MCRM 逐步校正全局刚性视差。随后，在 F2M 中统一实现精细配准以修复局部非刚性偏移和图像融合。融合图像提供反馈以提高配准精度，改进的配准结果进一步改善融合结果。对于图像融合，我们尝试将纹理增强纳入图像融合中，而不是仅仅保留现有方法中的原始源信息。我们测试了四种类型的多模态数据 （RGB-IR、RGB-NIR、PET-MRI 和 CT-MRI）。广泛的配准和融合结果验证了 MURF 的优越性和普遍性。 </p></details> | TPAMI2023 |  [Code](https://github.com/hanna-xu/MURF) | Han Xu/Jiayi Ma
| **[无需严格配准即可实现红外可见光图像融合的深度学习框架](https://link.springer.com/article/10.1007/s11263-023-01948-x)** | 2023-11-30 | <details><summary>Show</summary><p> 近年来，尽管红外和可见光图像融合取得了重大进展，但现有方法通常假设在图像融合之前已经对源图像进行了严格的配准或对齐。然而，红外和可见光图像模态的差异对实现自动严格对准提出了巨大挑战，影响了后续融合程序的质量。针对这一问题，本文提出了一种用于未对准红外和可见光图像融合的深度学习框架，旨在将融合算法从严格的配准中解放出来。从技术上讲，我们设计了一个卷积神经网络 （CNN）-Transformer 分层交互嵌入 （CTHIE） 模块，该模块可以结合 CNN 和 Transformer 各自的优点，从源图像中提取特征。此外，通过表征从未对齐的源图像中提取的特征之间的相关性，设计了动态再聚合特征表示 （DRFR） 模块，以将特征与基于自注意力的特征再聚合方案进行对齐。最后，为了有效利用网络不同层次的特征，该文引入一种全感知前向融合（FPFF）模块，通过多模态特征的交互传输进行特征融合，以重建融合后的图像。合成数据和真实数据的实验结果证明了所提方法的有效性，验证了无需严格配准即可直接融合红外和可见光图像的可行性。 </p></details> | IJCV2024 |  [Code](https://github.com/yuliu316316/IVF-WoReg) | Huafeng Li/刘晋源
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |

### 大模型应用
| **题目** | **日期** | **摘要** | **录用信息** | **代码** | **备注**
| --- | --- | --- | --- | --- | --- |
| **[SpTFuse: 用于红外和可见光图像融合的 SAM 引导多级协同Transformer](https://www.sciencedirect.com/science/article/abs/pii/S0031320325000512)** | 2025-01 | <details><summary>Show</summary><p> 图像融合的主要价值在于更有效地支持下游任务。然而，现有方法的融合表示包含的语义信息不足，从而削弱了与后续任务的兼容性。为了克服这个问题，本稿提出了一种用于红外和可见光图像融合的 SAM 引导多级协同变压器，称为 SpTFuse。考虑到 Segment Anything Model （SAM） 强大的零镜头泛化能力，引入基于 SAM 的语义先验分支与多尺度视觉表示分支交互，以提高融合表示的完整性和兼容性。交互过程分为三个级别，以逐步集成多分支信息。在第一级，设计了一个模态融合模块（IEB），带有一个单步协作变压器（SCT）和一个模态集成模块（MIM）。SCT 聚合了语义先验和视觉表示的相关特征。然后，MIM 旨在融合 SAM 语义先验引导的多模态视觉表示。为了平衡视觉和语义表示以获得完整的融合表示，在以下级别构建了模态内交互块 （IAB）。具体来说，IAB 由双通道协作变压器 （DCT） 和语义增强模块 （SEM） 组成。DCT 以级联方式构建两条路径，其中前一条协作路径继续获取语义先验，而视觉细化路径在保持语义完整性的同时平衡视觉信息。随后，SEM 进一步结合 Semantic Before 以增强融合表示的完整性。为了减少图像恢复过程中丢弃的语义信息，通过语义补偿块将前一层的协同信息纳入相应的解码器层。最后，所提出的损失函数包括语义先验损失、梯度损失和强度损失。实验表明，SpTFuse 不仅取得了有效的融合结果，而且在分割和检测等下游任务中也显示出明显的优势。 </p></details> | PR2025 |  [Code](https://github.com/lxq-jnu/SpTFuse) |
| **[Text-IF：利用语义文本指南进行降级感知和交互式图像融合(Arxiv)](http://arxiv.org/abs/2403.16387v1)** | 2024-03-25 | <details><summary>Show</summary><p>Image fusion 旨在将来自不同源图像的信息组合在一起，以创建具有全面代表性的图像。现有的融合方法通常无法处理低质量源图像的退化，并且无法满足多个主观和客观需求。为了解决这些问题，我们引入了一种新方法，该方法利用语义文本引导图像融合模型进行降级感知和交互式图像融合任务，称为 Text-IF。它创新性地将经典图像融合扩展到文本引导图像融合，并能够和谐地解决融合过程中的退化和交互问题。通过文本语义编码器和语义交互融合解码器，Text-IF 可用于一体化的红外和可见光图像退化感知处理和交互式柔性融合结果。通过这种方式，Text-IF 不仅实现了多模态图像融合，而且实现了多模态信息融合。大量实验证明，所提出的文本引导图像融合策略在图像融合性能和退化处理方面优于SOTA方法。该代码可在 https://github.com/XunpengYi/Text-IF 获取。</p></details> | CVPR2024 | [Code Link](https://github.com/XunpengYi/Text-IF) | Jiayi Ma |
| **[Every SAM Drop Counts: Embracing Semantic Priors for Multi-Modality Image Fusion and Beyond](https://arxiv.org/abs/2503.01210)** | 2025 | <details><summary>Show</summary><p> 多模态图像融合，尤其是红外和可见光，在集成各种模态以增强场景理解方面发挥着至关重要的作用。尽管早期研究优先考虑视觉质量，但保留精细细节和适应下游任务仍然具有挑战性。最近的方法尝试了特定于任务的设计，但由于优化目标不一致，很少实现 “两全其美”。为了解决这些问题，我们提出了一种新的方法，利用 Segment Anything Model （SAM） 的语义知识来提高融合结果的质量并启用下游任务适应性，即 SAGE。具体来说，我们设计了一个语义持续注意力 （SPA） 模块，它通过持久存储库有效地维护源信息，同时从 SAM 中提取高级语义先验。更重要的是，为了消除推理过程中对 SAM 的不切实际的依赖，我们引入了一种具有三元组损失的双层优化驱动蒸馏机制，使学生网络能够有效地提取知识。大量实验表明，我们的方法在保持实际部署效率的同时，实现了高质量的可视化结果和下游任务适应性之间的平衡。 </p></details> | CVPR2025 |  [Code Link](https://github.com/RollingPlain/SAGE_IVIF) |
| **[Image Fusion via Vision-Language Model](https://arxiv.org/abs/2402.02235)** | 2024 | <details><summary>Show</summary><p> 图像融合将来自多个图像的基本信息集成到单个合成中，从而增强结构、纹理并优化缺陷。现有方法主要关注用于识别的像素级和语义视觉特征，但往往忽略了视觉之外更深层次的文本级语义信息。因此，我们首次引入了一种名为 image Fusion via vIsion-Language Model （FILM） 的新型融合范式，利用源图像中的显式文本信息来指导融合过程。具体来说，FILM 从图像中生成语义提示并将其输入到 ChatGPT 中以进行全面的文本描述。这些描述融合在文本域内，并指导视觉信息融合，增强特征提取和上下文理解，由文本语义信息通过交叉注意指导。FILM 在红外-可见光、医疗、多重曝光和多焦点图像融合四个图像融合任务中显示出可喜的成果。我们还提出了一个视觉语言数据集，其中包含 ChatGPT 为四个融合任务中的八个图像融合数据集生成的段落描述，以促进未来基于视觉语言模型的图像融合的研究。 </p></details> | ICML2024 |  [Code](https://github.com/Zhaozixiang1228/IF-FILM) | 赵子祥
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |


### 退化感知引导（去噪、去雾、低照度、曝光）
| **题目** | **日期** | **摘要** | **录用信息** | **代码** | **备注**
| --- | --- | --- | --- | --- | --- |
| **[LEFuse：夜间红外和可见光图像的联合低光增强和图像融合](https://www.sciencedirect.com/science/article/pii/S0925231225002644)** | 2025-02 | <details><summary>Show</summary><p> 红外和可见光图像融合 （IVIF） 旨在通过整合来自两种模式的信息来更丰富、更准确地表示场景。然而，现有的 IVIF 方法通常是为正常照明条件设计的，旨在通过保持与源图像的紧密相似性来获得更高的分数。在夜景中，由于昏暗的环境和局部光源的干扰，可见光图像经常受到低光和局部过度曝光的影响。这些方法无法探索隐藏在可见图像暗区中的信息，导致注入图像缺乏纹理细节，整体显得较暗，视觉质量较差。为了解决这个问题，我们提出了一种名为 LEFuse 的新型图像融合网络。LEFuse 不仅集成了来自可见光和红外图像的互补信息，还专注于恢复可见光图像中隐藏的纹理细节。通过这样做，LEFuse 增强了融合图像的可见性和对比度，从而产生更明亮、更生动的表现。为了实现这一目标，我们提出了一组无监督损失函数来驱动网络的学习。这组包括用于图像融合和低光增强的最大基于熵的融合增强损失，以及用于减轻局部过度曝光不可见图像对融合结果的影响的感知损失。这些损失可以应用于任何现有的图像融合网络，从而在不影响融合性能的情况下增强融合图像。广泛的实验表明，我们的 LEFuse 在视觉质量和定量评估方面取得了可喜的结果，尤其是在夜间环境中。 </p></details> | NEUROCOMPUTING25 | [Code](https://github.com/cheng411523/LEFuse) | 可以看，效果一般 |
| **[动态亮度适应，实现稳健的多模态图像融合(Arxiv)](http://arxiv.org/abs/2411.04697v1)** | 2024-11-07 | <details><summary>Show</summary><p>红外和可见光图像融合旨在整合模态优势，以获得视觉增强、信息丰富的图像。在真实场景中，可见光成像容易受到动态环境亮度波动的影响，从而导致纹理退化。现有的融合方法缺乏对这种亮度扰动的鲁棒性，从而严重影响了融合影像的视觉保真度。为了应对这一挑战，我们提出了亮度自适应多模态动态融合框架 （BA-Fusion），该框架在动态亮度波动的情况下实现了稳健的图像融合。具体来说，我们引入了亮度自适应门 （BAG） 模块，该模块旨在从与亮度相关的通道中动态选择特征进行归一化，同时在源图像中保留与亮度无关的结构信息。此外，我们提出了一个亮度一致性损失函数来优化 BAG 模块。整个框架通过交替训练策略进行调整。广泛的实验验证了我们的方法在保留多模态图像信息和视觉保真度方面超越了最先进的方法，同时在不同亮度水平下表现出非凡的稳健性。我们的代码可用：https://github.com/SunYM2020/BA-Fusion。</p></details> | IJCAI2024 | [Code Link](https://github.com/SunYM2020/BA-Fusion) |
| **[VIIS：用于严重低光图像增强的可见光和红外信息合成(Arxiv)](http://arxiv.org/abs/2412.13655v2)** | 2025-02-13 | <details><summary>Show</summary><p>在严重的低光环境下拍摄的图像通常会出现大量信息缺失的情况。现有的奇异模态图像增强方法难以恢复缺乏有效信息的图像区域。通过利用不透光的红外图像，可见光和红外图像融合方法有可能揭示隐藏在黑暗中的信息。然而，它们主要强调模态间互补，而忽视了模态内增强，限制了输出图像的感知质量。为了解决这些限制，我们提出了一项称为可见光和红外信息合成 （VIIS） 的新任务，旨在实现两种模式的信息增强和融合。鉴于在 VIIS 任务中难以获得地面实况，我们设计了一个基于图像增强的信息合成前置任务 （ISPT）。我们采用扩散模型作为框架，设计了一种稀疏基于注意力的双模态残差 （SADMR） 条件反射机制，以增强两种模态之间的信息交互。这种机制使具有两种模态先验知识的特征能够在去噪过程中自适应和迭代地关注每个模态的信息。我们广泛的实验表明，我们的模型不仅在定性和定量上优于相关领域最先进的方法，而且优于能够进行信息增强和融合的新设计的基线。该代码可在 https://github.com/Chenz418/VIIS 获取。</p></details> | WACV2025 | [Code Link](https://github.com/Chenz418/VIIS) |
| **[SCDFuse：用于联合红外和可见光图像融合和去噪的语义互补蒸馏框架](https://www-sciencedirect-com-s-225.libdb.csu.edu.cn/science/article/pii/S0950705125003090?via%3Dihub)** | 2025-03 | <details><summary>Show</summary><p> 红外和可见光融合因其在计算机视觉领域的广泛应用而受到前所未有的关注。然而，现有的算法单方面专注于清晰场景图像的融合，容易受到噪声干扰。虽然可以通过部署独立的预降噪模块来缓解这个问题，但具有不同功能的其他模块的级联会带来补充复杂性、计算开销，甚至模块间干扰。为了克服这一限制并实现多任务统一，我们提出了一个用于端到端同步特征去噪和聚合的知识蒸馏框架。在这个框架中，我们利用蒸馏架构的优势来生成软标签，减轻因缺乏标签指导而导致的不稳定融合性能。为了实现训练过程中函数学习的准确指导，该文设计了一种非对称噪声感知训练策略，以促进聚合鲁棒性和去噪能力。此外，为保证特征挖掘和语义互补能力，构建了一种混合串并联 CNN 变压器双分支 En-Decoder。所提出的编码器结合了自主设计的纹理感知 ConvNextV2、条带池注意力和渐进式剩余变压器，以组成双分支架构。此外，还开发了语义互补特征聚合 （SCFA） 模块，以实现从粗到细的特征增强。对规则和噪声熔融材料进行了广泛的实验，以验证所提出的方法的积分和去噪性能。值得注意的是，在 TNO 数据集上，与次优算法相比，所提出的方法在 MSSIM 和 UQI 指标上分别提高了 4% 和 4.2%。此外，我们还通过对象检测实验研究了它对高级视觉任务的便利性。 </p></details> |  |  []() |
| **[IAIFNet：照明感知红外和可见光图像融合网络(Arxiv)](http://arxiv.org/abs/2309.14997v3)** | 2024-05-26 | <details><summary>Show</summary><p>红外可见光图像融合 （IVIF） 用于生成具有两种图像综合特征的融合图像，有利于下游视觉任务。然而，目前的方法很少考虑弱光环境下的照度条件，融合图像中的目标往往不突出。为了解决上述问题，我们提出了一种照明感知红外和可见光图像融合网络，命名为 IAIFNet。在我们的框架中，照明增强网络首先估计输入图像的入射照明图。随后，借助所提出的自适应差分融合模块（ADFM）和显著目标感知模块（STAM），图像融合网络将照明增强红外和可见光图像的显著特征有效地融合成高视觉质量的融合图像。大量的实验结果验证了我们的方法优于五种最先进的红外和可见光图像融合方法。</p></details> | IEEE Signal Processing Letters| None |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |

### 综述
| **题目** | **日期** | **摘要** | **录用信息** | **代码** | **备注**
| --- | --- | --- | --- | --- | --- |
| **[红外和可见光图像融合：从数据兼容性到任务适应(Arxiv)](http://arxiv.org/abs/2501.10761v1)** | 2025-01-18 | <details><summary>Show</summary><p>红外-可见光图像融合 （IVIF） 是计算机视觉中的一项关键任务，旨在将红外光谱和可见光谱的独特特征集成到一个统一的表示中。自 2018 年以来，该领域已进入深度学习时代，越来越多的方法引入了一系列网络和损失函数来提高视觉性能。然而，数据兼容性、感知准确性和效率等挑战仍然存在。不幸的是，最近缺乏针对这个迅速扩张的领域的综合调查。本白皮书通过提供涵盖广泛主题的全面调查来填补这一空白。我们引入了一个多维框架来阐明常见的基于学习的 IVIF 方法，从视觉增强策略到数据兼容性和任务适应性。我们还对这些方法进行了详细分析，并附有一个查找表来阐明它们的核心思想。此外，我们总结了定量和定性的性能比较，重点关注配准、融合和随后的高级任务。除了技术分析之外，我们还讨论了该领域的潜在未来方向和开放问题。有关更多详细信息，请访问我们的 GitHub 存储库：https://github.com/RollingPlain/IVIF_ZOO.</p></details> | TPAMI2025 | [Code Link](https://github.com/RollingPlain/IVIF_ZOO) | 刘晋源 |
| **[基于深度学习的图像融合方法综述-中文](https://txtx.publish.founderss.cn/zh/article/doi/10.11834/jig.220422/)** | 2022-07-18 | <details><summary>Show</summary><p> 图像融合技术旨在将不同源图像中的互补信息整合到单幅融合图像中以全面表征成像场景，并促进后续的视觉任务。随着深度学习的兴起，基于深度学习的图像融合算法如雨后春笋般涌现，特别是自编码器、生成对抗网络以及Transformer等技术的出现使图像融合性能产生了质的飞跃。本文对不同融合任务场景下的前沿深度融合算法进行全面论述和分析。首先，介绍图像融合的基本概念以及不同融合场景的定义。针对多模图像融合、数字摄影图像融合以及遥感影像融合等不同的融合场景，从网络架构和监督范式等角度全面阐述各类方法的基本思想，并讨论各类方法的特点。其次，总结各类算法的局限性，并给出进一步的改进方向。再次，简要介绍不同融合场景中常用的数据集，并给出各种评估指标的具体定义。对于每一种融合任务，从定性评估、定量评估和运行效率等多角度全面比较其中代表性算法的性能。本文提及的算法、数据集和评估指标已汇总至https://github.com/Linfeng-Tang/Image-Fusion。最后，给出了本文结论以及图像融合研究中存在的一些严峻挑战，并对未来可能的研究方向进行了展望。 </p></details> | 中国图像图形学报 |  [Code Link](https://github.com/Linfeng-Tang/Image-Fusion) | 唐霖峰/Jiayi Ma
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |


### 其他
| **题目** | **日期** | **摘要** | **录用信息** | **代码** | **备注**
| --- | --- | --- | --- | --- | --- |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |
