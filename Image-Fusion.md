# 图像融合(偏向红外-可见光图像融合和医学图像融合)
## 目录
- [Arxiv](#Arxiv(未录用))
- [期刊](#期刊)
- [会议](#会议)
- [推荐](#推荐)

## Arxiv(未录用)
| **题目** | **日期** | **摘要** | **录用信息** | **代码** | **备注**
| --- | --- | --- | --- | --- | --- |
| **[MaeFuse：通过引导式培训，使用预训练的掩蔽自动编码器传输 Omni 特征以实现红外和可见光图像融合(Arxiv)](http://arxiv.org/abs/2404.11016v2)** | 2025-02-09 | <details><summary>Show</summary><p>在本文中，我们介绍了 MaeFuse，这是一种专为红外和可见光图像融合 （IVIF） 设计的新型自动编码器模型。现有的图像融合方法通常依赖于训练与下游任务相结合来获得高级视觉信息，这对于强调目标对象并在视觉质量和特定任务应用中提供令人印象深刻的结果非常有效。我们的模型名为 MaeFuse ，它不是由下游任务驱动，而是利用了掩码自动编码器 （MAE） 的预训练编码器，该编码器为低级重建和高级视觉任务提供了全向特征提取，以低成本获得感知友好的特征。为了消除不同模态特征的域差距和 MAE 编码器引起的阻塞效应，我们进一步开发了一种引导式训练策略。这种策略经过精心设计，以确保融合层无缝适应编码器的特征空间，从而逐渐增强融合性能。所提出的方法可以促进来自红外和可见光模态的特征向量的全面整合，从而保留每个模态中固有的丰富细节。MaeFuse 不仅在融合技术领域引入了新颖的视角，而且在各种公共数据集中都以令人印象深刻的性能脱颖而出。</p></details> |  | None | Jiayi Ma |
| **[Hazy条件下可见光和红外图像联合复原融合的红外辅助单阶段框架(Arxiv)](http://arxiv.org/abs/2411.12586v2)** | 2025-02-08 | <details><summary>Show</summary><p>红外和可见光 （IR-VIS） 图像融合因其广泛的应用价值而受到广泛关注。然而，现有的方法往往忽视了红外图像在朦胧条件下恢复可见光图像特征的互补作用。为了解决这个问题，我们提出了一个联合学习框架，利用红外图像来恢复和融合朦胧的 IR-VIS 图像。为了减轻 IR-VIS 图像之间特征多样性的不利影响，我们引入了一种提示生成机制来调节特定于模态的特征不兼容性。这将从非共享图像信息创建一个提示选择矩阵，然后是从提示池生成的提示嵌入。这些嵌入有助于生成用于去雾的候选特征。我们进一步设计了一种红外辅助特征恢复机制，根据雾度选择候选特征，从而在单阶段框架内实现同步恢复和融合。为了提高融合质量，我们构建了一个多阶段的提示嵌入融合模块，该模块利用了提示生成模块的特征补充。我们的方法有效地融合 IR-VIS 图像，同时去除雾霾，产生清晰、无雾的融合结果。与先去雾再融合的两阶段方法相比，我们的方法支持在单阶段框架中进行协同训练，使模型相对轻量级，适合实际部署。实验结果验证了其有效性，并证明了与现有方法相比的优势。论文的源代码可以在 https://github.com/fangjiaqi0909/IASSF。 |  | [Code Link](https://github.com/fangjiaqi0909/IASSF) |
| **[HetSSNet： 用于全色和多光谱图像融合的时空-光谱异构图学习网络(Arxiv)](http://arxiv.org/abs/2502.04623v1)** | 2025-02-07 | <details><summary>Show</summary><p>遥感全色锐化旨在重建全色 （PAN） 图像和低分辨率多光谱 （LR-MS） 图像融合过程中的时空光谱特性，最终生成高分辨率多光谱 （HR-MS） 图像。在主流建模策略中，即 CNN 和 Transformer，输入图像被视为欧几里得空间中大小相等的像素网格。它们在面对具有不规则地面物体的遥感图像时存在局限性。图是更灵活的结构，但是，使用图对空间-光谱属性进行建模时，存在两个主要挑战：\emph{1） 为空间-光谱关系先验构建自定义的图形结构};\emph{2） 通过图学习统一的时空-谱表示}。为了应对这些挑战，我们提出了名为 \textbf{HetSSNet} 的时空-光谱异构图学习网络。具体来说， HetSSNet 最初构建了用于全色锐化的异构图形结构，它明确描述了特定于全色锐化的关系。随后，设计了基本关系模式生成模块，从异构图中提取多种关系模式。最后，利用关系模式聚合模块协同学习节点间不同关系的统一时空-谱表示，并从局部和全局角度进行自适应重要性学习。广泛的实验证明了 HetSSNet 的显著优越性和泛化性。</p></details> |  | None |
| **[MATCNN： 基于多尺度 CNN 的带注意力变换器的红外可见光图像融合方法(Arxiv)](http://arxiv.org/abs/2502.01959v1)** | 2025-02-04 | <details><summary>Show</summary><p>虽然基于注意力的方法在增强图像融合和解决长距离特征依赖性带来的挑战方面取得了相当大的进展，但它们在捕获局部特征方面的功效因缺乏多样化的感受野提取技术而受到损害。为了克服现有融合方法在提取多尺度局部特征和保留全局特征方面的不足，该文提出了一种基于注意力变换器的多尺度卷积神经网络 （MATCNN） 的新型跨模态图像融合方法。MATCNN 利用多尺度融合模块 （MSFM） 提取不同尺度的局部特征，并利用全局特征提取模块 （GFEM） 提取全局特征。将两者结合起来可以减少细节特征的损失，并提高全局特征表示的能力。同时，使用信息掩码标记图像中的相关细节，旨在提高融合图像中红外图像中重要信息和可见光图像中背景纹理的保留比例。随后，开发了一种新的优化算法，利用掩码通过内容整合、结构相似性指数测量和全局特征损失来指导特征提取。对各种数据集进行了定量和定性评估，表明 MATCNN 有效地突出了红外突出目标，保留了可见光图像中的其他细节，并为跨模态图像实现了更好的融合结果。MATCNN 的代码将在 https://github.com/zhang3849/MATCNN.git 上提供。</p></details> |  | [Code Link](https://github.com/zhang3849/MATCNN.git) |
| **[使用多模态图像融合进行深度集成，用于肺癌的高效分类(Arxiv)](http://arxiv.org/abs/2502.00078v1)** | 2025-01-31 | <details><summary>Show</summary><p>本研究的重点是从多模态肺部图像中对癌性和健康切片进行分类。研究中使用的数据包括计算机断层扫描 （CT） 和正电子发射断层扫描 （PET） 图像。所提出的策略通过利用主成分分析 （PCA） 和自动编码器实现了 PET 和 CT 图像的融合。随后，开发了一种新的基于集成的分类器，即深度集成多模态融合 （DEMF），采用多数投票对正在检查的样本图像进行分类。采用梯度加权类激活映射 （Grad-CAM） 来可视化受癌症影响图像的分类准确性。鉴于样本量有限，在训练阶段采用了随机图像增广策略。DEMF 网络有助于缓解计算机辅助医学图像分析中稀缺数据的挑战。拟议的网络与三个公开可用的数据集中最先进的网络进行了比较。该网络在准确性、F1-Score、Accuracy 和 Recall 等指标上优于其他网络。调查结果突出了所提出的网络的有效性。</p></details> |  | None |
| **[作为图像的任意数据：跨模态和不规则间隔的患者数据与 Vision Transformer 的融合(Arxiv)](http://arxiv.org/abs/2501.18237v1)** | 2025-01-30 | <details><summary>Show</summary><p>患者在每次住院时会接受多次检查，每次检查提供不同的健康状态信息。这些检查包括不同采样率的时间序列数据、离散的单点测量、药物治疗等干预，以及影像数据。尽管医生能够直观地处理和整合不同的模态，但神经网络需要为每个模态进行特定建模，增加了训练的复杂性。我们展示了将所有信息作为图像与非结构化文本进行可视化，并随后训练常规的视觉-文本变换器，可以显著减少这种复杂性。我们的方法——基于视觉变换器的非规则采样多模态数据融合（ViTiMM）——不仅简化了数据预处理和建模过程，还在预测住院期间死亡率和表型分析方面超越了当前最先进的方法。该方法在MIMIC-IV数据集上的6,175名患者数据上进行评估，包括临床测量、药物、X光影像和心电图扫描。我们希望我们的工作能激发多模态医学人工智能的进展，通过将训练复杂性简化为（视觉）提示工程，降低了进入门槛，并启用了无代码解决方案的训练。源代码将公开发布</p></details> |  | None |
| **[E2E-MFD：迈向端到端同步多模态融合检测(Arxiv)](http://arxiv.org/abs/2403.09323v4)** | 2025-01-27 | <details><summary>Show</summary><p>多模态图像融合和物体检测在自动驾驶中至关重要。虽然现有方法在融合纹理细节和语义信息方面取得了进展，但它们复杂的训练过程限制了更广泛的应用。为了解决这一挑战，我们提出了E2E-MFD，一种新颖的端到端多模态融合检测算法。E2E-MFD简化了这一过程，通过单一的训练阶段实现了高性能。它通过同步联合优化各个组件，避免了与单独任务相关的次优解决方案。此外，它在梯度矩阵中实现了一种全面的优化策略，用于共享参数，确保收敛到最优的融合检测配置。我们在多个公开数据集上的广泛测试展示了E2E-MFD的卓越能力，既展示了视觉上令人满意的图像融合，也展示了令人印象深刻的检测结果，例如在水平物体检测数据集M3FD和定向物体检测数据集DroneVehicle上分别提高了3.9%和2.0%的mAP50，相较于最先进的其他方法。代码已发布：https://github.com/icey-zhang/E2E-MFD。</p></details> |  | [Code Link](https://github.com/icey-zhang/E2E-MFD) |
| **[红外和可见光图像融合：从数据兼容性到任务适应(Arxiv)](http://arxiv.org/abs/2501.10761v1)** | 2025-01-18 | <details><summary>Show</summary><p>红外-可见光图像融合 （IVIF） 是计算机视觉中的一项关键任务，旨在将红外光谱和可见光谱的独特特征集成到一个统一的表示中。自 2018 年以来，该领域已进入深度学习时代，越来越多的方法引入了一系列网络和损失函数来提高视觉性能。然而，数据兼容性、感知准确性和效率等挑战仍然存在。不幸的是，最近缺乏针对这个迅速扩张的领域的综合调查。本白皮书通过提供涵盖广泛主题的全面调查来填补这一空白。我们引入了一个多维框架来阐明常见的基于学习的 IVIF 方法，从视觉增强策略到数据兼容性和任务适应性。我们还对这些方法进行了详细分析，并附有一个查找表来阐明它们的核心思想。此外，我们总结了定量和定性的性能比较，重点关注配准、融合和随后的高级任务。除了技术分析之外，我们还讨论了该领域的潜在未来方向和开放问题。有关更多详细信息，请访问我们的 GitHub 存储库：https://github.com/RollingPlain/IVIF_ZOO.</p></details> |  | [Code Link](https://github.com/RollingPlain/IVIF_ZOO) | Jinyuan Liu |
| **[HyFusion：用于高光谱图像融合的增强型接收场变压器(Arxiv)](http://arxiv.org/abs/2501.04665v3)** | 2025-01-14 | <details><summary>Show</summary><p>高光谱图像 （HSI） 融合解决了从高分辨率多光谱图像 （HR-MSI） 和低分辨率 HSI （LR-HSI） 重建高分辨率 HSI （HR-HSI） 的挑战，鉴于与获取高质量 HSI 相关的高成本和硬件限制，这是一项关键任务。虽然现有方法利用空间和光谱关系，但它们通常受到感受野有限和特征利用率不足的问题，导致性能欠佳。此外，高质量 HSI 数据的稀缺凸显了有效利用数据以最大限度地提高重建质量的重要性。为了解决这些问题，我们提出了 HyFusion，这是一种新颖的双耦合网络 （DCN） 框架，旨在增强跨域特征提取并实现有效的特征图重用。该框架首先通过专门的子网络处理 HR-MSI 和 LR-HSI 输入，这些子网络在特征提取过程中相互增强，保留互补的空间和光谱细节。HyFusion 的核心是利用增强型接收场块 （ERFB），它结合了移动窗口注意力和密集连接来扩大感受野，有效捕获长距离依赖性，同时最大限度地减少信息损失。大量实验表明，HyFusion 在 HR-MSI/LR-HSI 融合中实现了最先进的性能，在保持紧凑的模型尺寸和计算效率的同时，显著提高了重建质量。通过将增强的感受野和特征图重用集成到耦合网络架构中，HyFusion 为资源受限场景中的 HSI 融合提供了实用有效的解决方案，为高光谱成像树立了新的标杆。我们的代码将公开可用。</p></details> |  | None |
| **[DAE-Fuse：用于多模态图像融合的自适应判别自动编码(Arxiv)](http://arxiv.org/abs/2409.10080v2)** | 2024-12-24 | <details><summary>Show</summary><p>在夜间或低能见度环境等极端场景中，实现可靠的感知对于自动驾驶、机器人和监控等应用至关重要。多模态图像融合，特别是集成红外成像，通过结合来自不同模态的互补信息来增强场景理解和决策，从而提供强大的解决方案。然而，目前的方法面临重大限制：基于 GAN 的方法经常产生缺乏精细细节的模糊图像，而基于 AE 的方法可能会偏向于特定模式，从而导致不自然的融合结果。为了应对这些挑战，我们提出了 DAE-Fuse，这是一种新颖的两相判别自动编码器框架，可生成清晰自然的融合图像。此外，我们率先将图像融合技术从静态图像扩展到视频领域，同时保持帧之间的时间一致性，从而提高自主导航所需的感知能力。对公共数据集的广泛实验表明，DAE-Fuse 在多个基准测试中实现了最先进的性能，对医学图像融合等任务具有卓越的泛化性。</p></details> |  | None |
| **[互补优势：利用交叉场频率相关技术进行 NIR 辅助图像去噪(Arxiv)](http://arxiv.org/abs/2412.16645v1)** | 2024-12-21 | <details><summary>Show</summary><p>现有的单图像去噪算法在处理复杂的噪点图像时通常难以恢复细节。近红外 （NIR） 图像的引入为 RGB 图像去噪提供了新的可能性。然而，由于 NIR 和 RGB 图像之间的不一致，现有工作在图像融合过程中仍然难以平衡两个场的贡献。为此，在本文中，我们开发了一种用于 NIR 辅助图像去噪的跨场频率相关开发网络 （FCENet）。我们首先基于对 NIR-RGB 图像对的深入统计频率分析提出了频率相关性先验。先验揭示了 NIR 和 RGB 图像在频域中的互补相关性。利用频率相关先验，我们建立了一个由频率动态选择机制 （FDSM） 和频率穷举融合机制 （FEFM） 组成的频率学习框架。FDSM 在频域中从 NIR 和 RGB 图像中动态选择互补信息，FEFM 加强了 NIR 和 RGB 特征融合过程中对共性和差分特征的控制。对模拟和真实数据的广泛实验验证了我们的方法在图像质量和计算效率方面优于各种最先进的方法。该代码将向公众发布。</p></details> |  | None |
| **[通过具有可编辑模式的 Distilled 3D LUT 网格进行多重曝光图像融合(Arxiv)](http://arxiv.org/abs/2412.13749v1)** | 2024-12-18 | <details><summary>Show</summary><p>随着手持设备成像分辨率的提高，现有的多重曝光图像融合算法难以实时生成具有超高分辨率的高动态范围图像。除此之外，还有一种趋势是设计一个可管理和可编辑的算法，以满足实际应用场景的不同需求。为了解决这些问题，我们引入了 3D LUT 技术，该技术可以在资源受限的设备上实时增强具有超高清 （UHD） 分辨率的图像。然而，由于来自不同曝光率的多个图像的信息融合是不确定的，而这种不确定性极大地考验了 3D LUT 网格的泛化能力。为了解决这个问题并确保模型的稳健学习空间，我们建议使用师生网络来模拟 3D LUT 网格上的不确定性。此外，我们使用隐式表示函数为多重曝光图像融合算法提供了一种可编辑模式，以匹配不同场景下的需求。大量实验表明，我们提出的方法在效率和准确性方面具有很强的竞争力。</p></details> |  | None |
| **[模态解耦就是您所需要的：无监督高光谱图像融合的简单解决方案(Arxiv)](http://arxiv.org/abs/2412.04802v1)** | 2024-12-06 | <details><summary>Show</summary><p>高光谱图像融合 （HIF） 旨在融合低分辨率高光谱图像 （LR-HSI） 和高分辨率多光谱图像 （HR-MSI） 以重建高空间和高光谱分辨率图像。目前的方法通常在没有有效监督的情况下应用两种模态的直接融合，未能完全感知深层模态互补信息，因此，导致对模态间连接的理解肤浅。为了弥合这一差距，我们为无监督 HIF 提出了一种简单有效的解决方案，并假设模态解耦对 HIF 至关重要。我们引入了模态聚类损失，它确保了模态的清晰指导，向模态共享特征解耦，同时避开模态互补特征。此外，我们还提出了一个端到端模态解耦空间光谱融合 （MossFuse） 框架，该框架将跨模态的共享和互补信息解耦，并聚合 LR-HSI 和 HR-MSI 的简洁表示，以减少模态冗余。在多个数据集上的系统实验表明，我们简单有效的方法始终优于现有的 HIF 方法，同时需要的参数要少得多，推理时间更短。</p></details> |  | None |
| **[Hipandas：通过全色图像的图像融合实现高光谱图像联合去噪和超分辨率(Arxiv)](http://arxiv.org/abs/2412.04201v1)** | 2024-12-05 | <details><summary>Show</summary><p>高光谱图像（HSIs）由于成像设备的限制，通常噪声较大且分辨率较低。最近发射的卫星能够同时获取高光谱（HSI）和全色（PAN）图像，从而通过融合全色图像进行去噪和超分辨率处理，恢复高分辨率的清晰高光谱图像。然而，之前的研究将这两个任务视为独立的过程，导致误差积累。本文提出了一个新颖的学习范式——Hyperspectral Image Joint Pan denoising and Pan sharpening (Hipandas)，该方法通过融合噪声较大的低分辨率高光谱图像（LRHS）和高分辨率全色图像（PAN）来重建高分辨率高光谱图像（HRHS）。所提出的零-shot Hipandas框架由引导去噪网络、引导超分辨率网络和全色图像重建网络组成，利用高光谱图像低秩先验和新提出的注重细节的低秩先验。由于这些网络之间的相互连接，训练过程变得复杂，需要采用两阶段训练策略以确保有效训练。通过在模拟和真实数据集上的实验结果表明，所提出的方法优于当前最先进的算法，能够生成更准确、视觉上更令人满意的高分辨率高光谱图像。</p></details> |  | None |
| **[具有可学习融合损失的任务驱动图像融合(Arxiv)](http://arxiv.org/abs/2412.03240v1)** | 2024-12-04 | <details><summary>Show</summary><p>多模态图像融合聚合来自多个传感器源的信息，与任何单一源相比，可实现卓越的视觉质量和感知特性，从而增强下游任务。然而，当前下游任务的融合方法仍然使用预定义的融合目标，这可能会与下游任务不匹配，从而限制自适应指导并降低模型灵活性。为了解决这个问题，我们提出了任务驱动图像融合 （TDFusion），这是一个融合框架，其中包含由任务损失指导的可学习融合损失。具体来说，我们的融合损失包括由称为损失生成模块的神经网络建模的可学习参数。该模块以元学习的方式由下游任务的丢失进行监督。学习目标是在融合模块通过融合损失进行优化后，最大限度地减少融合图像的任务损失。融合模块和损失模块之间的迭代更新确保融合网络朝着最小化任务损失的方向发展，引导融合过程朝着任务目标发展。TDFusion 的训练完全依赖于下游任务的损失，使其能够适应任何特定任务。它可以应用于融合和任务网络的任何架构。实验证明了 TDFusion 在融合和任务相关应用中的性能，包括四个公共融合数据集、语义分割和对象检测。代码将被发布。</p></details> |  | None |
| **[用于红外和可见光图像融合的多尺度信息集成框架(Arxiv)](http://arxiv.org/abs/2312.04328v2)** | 2024-11-20 | <details><summary>Show</summary><p>红外可见光图像融合旨在生成包含源图像强度和细节信息的融合图像，关键问题是有效测量和整合来自同一场景的多模态图像的互补信息。现有的方法大多采用损失函数中的简单权重来确定每种模态的信息保留，而不是自适应地测量不同图像对的互补信息。在这项研究中，我们提出了一个用于红外和可见光图像融合的多尺度双注意力 （MDA） 框架，该框架旨在测量和整合图像和斑块级别结构和损失函数中的互补信息。在我们的方法中，残差下采样块首先将源图像分解为三个尺度。然后，双注意力融合块整合互补信息，并在每个尺度上生成空间和通道注意力图，用于特征融合。最后，通过残差重建块重建输出图像。损失函数由图像级、特征级和补丁级三部分组成，其中图像级和补丁级两部分的计算基于互补信息测量产生的权重。实际上，为了限制输出和红外图像之间的像素强度分布，添加了样式损失。我们的融合结果在不同场景中表现稳健且信息丰富。两个数据集的定性和定量结果表明，我们的方法能够保留来自两种模式的热辐射和详细信息，并与其他最先进的方法相比获得可比的结果。消融实验显示了我们的信息集成架构的有效性，并自适应地测量损失函数中的互补信息保留。</p></details> |  | None | Xinbo Gao |
| **[Rethinking Normalization Strategies and Convolutional Kernels for Multimodal Image Fusion(Arxiv)](http://arxiv.org/abs/2411.10036v1)** | 2024-11-15 | <details><summary>Show</summary><p>多模态图像融合 （MMIF） 旨在整合来自不同模态的信息以获得全面的图像，从而辅助下游任务。然而，现有方法往往优先考虑自然图像融合，并侧重于信息互补和网络训练策略。他们忽视了自然图像融合和医学图像融合之间的本质区别以及基础成分的影响。本文剖析了这两项任务在融合目标、统计特性和数据分布方面的显著差异。基于此，我们重新思考了归一化策略和卷积内核对端到端 MMIF 的适用性。具体来说，本文提出了实例归一化和组归一化的混合，以保持样本独立性并加强内在特征相关性。这种策略促进了丰富特征图的潜力，从而提高了融合性能。为此，我们进一步引入了大核卷积，有效地扩大了感受野，增强了图像细节的保留。此外，所提出的多径自适应融合模块以各种尺度和感受野的特征重新校准解码器输入，确保关键信息的传输。大量实验表明，我们的方法在多种聚变任务中表现出最先进的性能，并显著改善了下游应用。该代码可在 https://github.com/HeDan-11/LKC-FUNet 获取。</p></details> |  | [Code Link](https://github.com/HeDan-11/LKC-FUNet) |
| **[红外-可见光图像的指令驱动融合：为不同的下游任务量身定制(Arxiv)](http://arxiv.org/abs/2411.09387v1)** | 2024-11-14 | <details><summary>Show</summary><p>红外和可见光图像融合技术的主要价值在于将融合结果应用于下游任务。然而，现有方法面临着挑战，例如在同时处理多个下游任务时，训练复杂性增加和单个任务的性能严重受损。为了解决这个问题，我们提出了面向任务的自适应调节 （T-OAR），这是一种专为多任务环境设计的自适应机制。此外，我们还介绍了与任务相关的动态提示注入 （T-DPI） 模块，该模块从用户输入的文本指令中生成特定于任务的动态提示，并将其集成到目标表示中。这将指导特征提取模块生成与下游任务的特定要求更紧密一致的表示。通过将 T-DPI 模块整合到 T-OAR 框架中，我们的方法可以生成根据特定任务要求量身定制的融合图像，而无需单独的训练或特定任务权重。这不仅降低了计算成本，还增强了多个任务的适应性和性能。实验结果表明，该方法在目标检测、语义分割和显著目标检测方面表现出色，表现出较强的适应性、灵活性和任务特异性。这为多任务环境中的图像融合提供了有效的解决方案，突出了该技术在各种应用中的潜力。</p></details> | 10 pages | None | Huafeng Li |
| **[高光谱图像分类的全面调查：从传统模型到 Transformers 和 Mamba 模型的演变(Arxiv)](http://arxiv.org/abs/2404.14955v4)** | 2024-11-14 | <details><summary>Show</summary><p>由于高光谱 （HS） 数据的高维性和复杂性，高光谱图像分类 （HSC） 带来了重大挑战。虽然传统的机器学习 （TML） 方法已证明有效，但它们在实际应用中经常遇到重大障碍，包括最佳特征集的可变性、人工驱动设计中的主观性、固有偏见和方法限制。具体来说，TML 遭受了维度的诅咒、特征选择和提取困难、对空间信息的考虑不足、对噪声的鲁棒性有限、可扩展性问题以及对复杂数据分布的适应性不足。近年来，深度学习 （DL） 技术已成为应对这些挑战的强大解决方案。该调查全面概述了 HSC 的当前趋势和未来前景，强调了从 DL 模型到 Transformer 和 Mamba 模型架构的日益采用的进步。我们系统地回顾了 HSC DL 中的关键概念、方法和最先进的方法。此外，我们研究了基于 Transformer 的模型和 Mamba 模型在 HSC 中的潜力，详细介绍了它们的优势和挑战。探讨了 HSC 的新兴趋势，包括对可解释的 AI 和互作性概念的深入讨论，以及用于图像去噪、特征提取和图像融合的扩散模型。对三个 HS 数据集进行了全面的实验结果，以证实各种常规 DL 模型和 Transformer 的有效性。此外，我们还确定了 HSC 领域的几个开放挑战和相关研究问题。最后，我们概述了旨在提高 HSC 准确性和效率的未来研究方向和潜在应用。</p></details> |  | None |
| **[全天候多模态图像融合：统一框架和 100k 基准测试(Arxiv)](http://arxiv.org/abs/2402.02090v2)** | 2024-11-11 | <details><summary>Show</summary><p>多模态图像融合 （MMIF） 结合了来自不同图像模态的互补信息，以提供对场景的更全面、更客观的解读。然而，现有的 MMIF 方法缺乏抵抗现实世界场景中不同天气干扰的能力，因此无法在自动驾驶等实际应用中发挥作用。为了弥合这一研究差距，我们提出了一个全天候 MMIF 模型。由于天气条件的复杂性和多样性，在这种情况下实现有效的多任务处理尤其具有挑战性。一个关键障碍在于当前深度学习架构的“黑匣子”性质，这限制了它们的多任务处理能力。为了克服这个问题，我们将网络分解为两个模块：融合模块和恢复模块。对于融合模块，我们引入了一个可学习的低秩表示模型，将图像分解为低秩和稀疏分量。这种可解释的特征分离使我们能够更好地观察和理解图像。对于恢复模块，我们提出了一个基于大气散射模型的物理感知清晰特征预测模块，该模块可以从场景照明和反射率中推断出透光率的变化。我们还构建了一个大规模多模态数据集，其中包含 100,000 个图像对，涵盖雨、霾和雪况，涵盖各种退化程度和不同场景，以全面评估恶劣天气下的图像融合方法。在真实世界和合成场景中的实验结果表明，所提算法在细节恢复和多模态特征提取方面表现出色。该代码可在 https://github.com/ixilai/AWFusion 获取。</p></details> |  | [Code Link](https://github.com/ixilai/AWFusion) | Huafeng Li |
| **[基于l0-正则化稀疏编码的多模态图像融合可解释网络(Arxiv)](http://arxiv.org/abs/2411.04519v1)** | 2024-11-07 | <details><summary>Show</summary><p>多模态图像融合 （MMIF） 通过结合从不同模态传感器图像获得的独特和通用特征来增强融合图像的信息内容，从而改进可视化、对象检测和更多任务。在这项工作中，我们为MMIF任务引入了一个名为FNet的可解释网络，该网络基于l0正则化多模态卷积稀疏编码（MCSC）模型。具体来说，为了解决 l0 正则化 CSC 问题，我们开发了一种基于算法展开的 l0 正则化稀疏编码 （LZSC） 块。给定不同的模态源图像，FNet 首先使用 LZSC 块从中分离出独特和常见的特征，然后将这些特征组合起来生成最终的融合图像。此外，我们提出了一个用于逆融合过程的 l0 正则化 MCSC 模型。基于这个模型，我们引入了一个名为 IFNet 的可解释逆融合网络，该网络在 FNet 的训练中使用。大量实验表明，FNet 在五个不同的 MMIF 任务中实现了高质量的融合结果。此外，我们表明 FNet 增强了可见光-热图像对中的下游目标检测。我们还可视化了 FNet 的中间结果，这证明了我们网络的良好可解释性。</p></details> |  | None |
| **[DDF： 一种基于无监督域自适应的遥感图像语义分割的新型双域图像融合策略(Arxiv)](http://arxiv.org/abs/2403.02784v2)** | 2024-10-24 | <details><summary>Show</summary><p>由于存在大量未标注的数据，遥感图像的语义分割是一个具有挑战性的热点问题。事实证明，无监督域适应 （UDA） 在整合来自目标域的非机密信息方面具有优势。但是，在源域和目标域上独立微调 UDA 模型对结果的影响有限。该文提出了一种混合训练策略，以及一种新颖的双域图像融合策略，该策略有效利用了原始图像、转换图像和中间域信息。此外，为了提高伪标签的精度，我们提出了一种伪标签区域特异性权重策略。在 ISPRS Vaihingen 和 Potsdam 数据集上进行的广泛的基准实验和消融研究证实了我们方法的有效性。</p></details> |  | None |
| **[重新思考可见光和红外图像融合的评估(Arxiv)](http://arxiv.org/abs/2410.06811v1)** | 2024-10-09 | <details><summary>Show</summary><p>可见光和红外图像融合 （VIF） 在广泛的高级视觉任务中引起了极大的兴趣，例如对象检测和语义分割。然而，由于缺乏基本实况，对 VIF 方法的评估仍然具有挑战性。本文提出了一种面向分割的评估方法 （SEA），通过结合语义分割任务并利用最新 VIF 数据集中可用的分割标签来评估 VIF 方法。具体来说， SEA 利用能够处理各种图像和类别的通用分割模型来预测融合图像的分割输出，并将这些输出与分割标签进行比较。我们对最近使用 SEA 的 VIF 方法的评估表明，尽管近一半的红外图像表现出比可见光图像更好的性能，但它们的性能与仅使用可见光图像相当甚至不如。进一步的分析表明，与我们的 SEA 最相关的两个指标是基于梯度的融合指标QABF和 Visual Information Fidelity （视觉信息保真度）。在传统的 VIF 评估指标中，当分段标签不可用时，这些指标可以用作代理。我们希望我们的评估将指导新颖实用的 VIF 方法的开发。代码已在 \url{https://github.com/Yixuan-2002/SEA/} 中发布。/p></details> |  | [Code Link](https://github.com/Yixuan-2002/SEA) |
| **[用于非小细胞肺癌分类的多模态医学图像融合(Arxiv)](http://arxiv.org/abs/2409.19220v1)** | 2024-09-28 | <details><summary>Show</summary><p>非小细胞肺癌 （NSCLC） 是全球癌症死亡的主要原因，其早期检测和细微的亚型分类是一个关键而复杂的问题。在本文中，我们介绍了一种创新的多模态数据集成，将融合医学成像（CT 和 PET 扫描）与临床健康记录和基因组数据相结合。这种独特的融合方法利用先进的机器学习模型（尤其是 MedClip 和 BEiT）进行复杂的图像特征提取，为计算肿瘤学树立了新标准。我们的研究超越了现有方法，NSCLC 检测和分类精度的显着提高证明了这一点。结果显示了关键性能指标的显著改进，包括准确率、精度、召回率和 F1 分数。具体来说，我们领先的多模态分类器模型记录了令人印象深刻的 94.04% 的准确率。我们相信，我们的方法有可能改变 NSCLC 诊断，促进早期检测和更有效的治疗计划，并最终在肺癌护理中为患者带来卓越的结果。</p></details> |  | None |
| **[DAF-Net：用于红外和可见光图像融合的具有域自适应的双分支特征分解融合网络(Arxiv)](http://arxiv.org/abs/2409.11642v1)** | 2024-09-18 | <details><summary>Show</summary><p>红外和可见光图像融合旨在结合来自两种模式的互补信息，以提供更全面的场景理解。然而，由于两种模式之间的显著差异，在熔融过程中保留关键特征仍然是一个挑战。针对这一问题，我们提出了一种具有域自适应的双分支特征分解融合网络（DAF-Net），在基编码器中引入多核最大均值差异（MK-MMD），设计了一种适用于红外和可见光图像融合的混合核函数。基于 Restormer 网络构建的基本编码器捕获全局结构信息，而基于可逆神经网络 （INN） 的细节编码器专注于提取细节纹理信息。通过结合 MK-MMD，DAF-Net 有效地对齐可见光和红外图像的潜在特征空间，从而提高融合图像的质量。实验结果表明，所提出的方法在多个数据集上优于现有技术，显著提高了视觉质量和融合性能。相关的 Python 代码可在 https://github.com/xujian000/DAF-Net 中找到。</p></details> | 5pages | [Code Link](https://github.com/xujian000/DAF-Net) |
| **[具有分层人类感知的红外和可见光图像融合(Arxiv)](http://arxiv.org/abs/2409.09291v1)** | 2024-09-14 | <details><summary>Show</summary><p>图像融合将来自多个域的图像合并到一个图像中，其中包含来自源域的互补信息。现有方法以像素强度、纹理和高级视觉任务信息作为确定信息保存的标准，缺乏对人类感知的增强。我们介绍了一种图像融合方法，即分层感知融合 （HPFusion），它利用大型视觉语言模型来整合分层人类语义先验，保留满足人类视觉系统的互补信息。我们提出了人类在查看图像对时关注的多个问题，并通过大型视觉语言模型根据图像生成答案。答案的文本被编码到融合网络中，优化还旨在引导融合图像的人类语义分布更类似于源图像，探索人类感知域内的互补信息。大量实验表明，我们的 HPFusoin 可以在信息保存和人类视觉增强方面获得高质量的融合结果。</p></details> |  | None | Xinbo Gao |
| **[用于基于模型的全色锐化的多头注意力残差展开网络(Arxiv)](http://arxiv.org/abs/2409.02675v1)** | 2024-09-04 | <details><summary>Show</summary><p>全色锐化和超锐化的目标是分别将高分辨率全色 （PAN） 图像与低分辨率多光谱 （MS） 或高光谱 （HS） 图像准确组合。展开融合方法将深度学习的强大表示功能与基于模型的方法的稳健性相结合。这些技术涉及将优化方案的步骤展开到深度学习框架中，从而产生高效且高度可解释的架构。在本文中，我们提出了一种基于模型的卫星图像融合深度展开方法。我们的方法基于变分公式，该公式结合了 MS/HS 数据的经典观察模型、基于 PAN 图像的高频注入约束和任意凸先验。对于展开阶段，我们引入了上采样和下采样层，它们通过残差网络使用 PAN 图像中编码的几何信息。我们方法的支柱是多头注意力残差网络 （MARNet），它取代了优化方案中的邻近运算符，并将多个头部注意力与残差学习相结合，通过根据补丁定义的非局部运算符来利用图像自相似性。此外，我们还集成了基于 MARNet 架构的后处理模块，以进一步提高融合图像的质量。在 PRISMA、Quickbird 和 WorldView2 数据集上的实验结果表明，我们的方法具有卓越的性能，并且能够在不同的传感器配置和不同的空间和光谱分辨率上进行泛化。源代码将在 https://github.com/TAMI-UIB/MARNet 上提供。</p></details> |  | [Code Link](https://github.com/TAMI-UIB/MARNet) |
| **[Shuffle Mamba：用于多模态图像融合的 Random Shuffle 状态空间模型(Arxiv)](http://arxiv.org/abs/2409.01728v1)** | 2024-09-03 | <details><summary>Show</summary><p>多模态图像融合集成了来自不同模态的互补信息，以生成增强且信息丰富的图像。尽管状态空间模型（如 Mamba）精通线性复杂度的长距离建模，但大多数基于 Mamba 的方法使用固定的扫描策略，这可能会引入有偏差的先验信息。为了缓解这个问题，我们提出了一种新颖的贝叶斯启发扫描策略，称为 Random Shuffle，并辅以理论上可行的逆向洗牌以保持信息协调不变性，旨在消除与固定序列扫描相关的偏差。基于这个转换对，我们定制了 Shuffle Mamba 框架，在空间和通道轴上渗透模态感知信息表示和跨模态信息交互，以确保多模态图像融合的稳健交互和无偏全局感受野。此外，我们开发了一种基于蒙特卡洛平均的测试方法，以确保模型的输出与预期结果更紧密地保持一致。在多个多模态图像融合任务中的广泛实验证明了我们提出的方法的有效性，与最先进的替代方案相比，产生了出色的融合质量。代码将在接受后可用。</p></details> |  | None |
| **[FusionSAM：用于多模态融合和分割的潜在空间驱动分割任何东西模型(Arxiv)](http://arxiv.org/abs/2408.13980v1)** | 2024-08-26 | <details><summary>Show</summary><p>多模态图像融合和分割通过集成来自各种传感器的数据来增强对自动驾驶中的场景理解。然而，由于缺乏全面的融合功能来指导中期微调并将注意力集中在相关区域，因此当前的模型难以在此类场景中有效地分割密集的元素。Segment Anything Model （SAM） 已成为一种变革性的细分方法。与缺乏微调控制的变压器相比，它通过其灵活的提示编码器提供更有效的提示。然而，SAM 尚未在自然图像的多模态融合领域得到广泛研究。在本文中，我们首次将 SAM 引入多模态图像分割，提出了一个结合了潜在空间令牌生成 （LSTG） 和融合掩码提示 （FMP） 模块的新框架，以增强 SAM 的多模态融合和分割能力。具体来说，我们首先通过向量量化获得两种模态的潜在空间特征，并将其嵌入到基于交叉注意力的域间融合模块中，以建立模态之间的长期依赖关系。然后，我们使用这些全面的融合功能作为提示来指导精确的像素级分割。在多个公共数据集上的广泛实验表明，所提出的方法在多模态自动驾驶场景中明显优于 SAM 和 SAM2，比最先进的方法实现了至少 3.9$%$ 的分割 mIoU。</p></details> |  | None |
| **[用于图像融合和曝光校正的整体动态频率转换器(Arxiv)](http://arxiv.org/abs/2309.01183v2)** | 2024-08-03 | <details><summary>Show</summary><p>校正曝光相关问题是提高图像质量的关键组成部分，为各种计算机视觉任务提供重大影响。从历史上看，大多数方法主要使用空间域恢复，对频域的潜力考虑有限。此外，对于低光增强、曝光校正和多重曝光融合，缺乏统一的视角，使图像处理的优化复杂化和阻碍。为了应对这些挑战，本文提出了一种新的方法，该方法利用频域来改进和统一曝光校正任务的处理。我们的方法引入了整体频率注意力和动态频率前馈网络，它们取代了空间域中的传统相关计算。它们构成了一个基础构建块，有助于将 U 形整体动态频率转换器作为滤波器来提取全局信息并动态选择重要的频段进行图像修复。作为补充，我们采用拉普拉斯金字塔将图像分解为不同的频带，然后是多个恢复器，每个恢复器都经过调整以恢复特定的频段信息。金字塔融合允许更详细和细致的图像恢复过程。最终，我们的结构统一了低光增强、曝光校正和多重曝光融合这三项任务，从而能够全面处理所有经典曝光误差。通过对这些任务的主流数据集进行基准测试，我们提出的方法取得了最先进的结果，为更复杂和统一的曝光校正解决方案铺平了道路。</p></details> |  | None | Jinyuan Liu |
| **[用于红外-可见光图像融合的语义感知和多导向网络(Arxiv)](http://arxiv.org/abs/2407.06159v2)** | 2024-08-03 | <details><summary>Show</summary><p>多模态图像融合旨在融合来自两个源图像的特定模态和共享模态信息。针对复杂场景特征提取不足和语义感知不足的问题，该文重点介绍了如何通过高效提取互补特征和多导向特征聚合来对关联驱动的分解特征进行建模并推理高级图表示。我们提出了一种三分支编码器-解码器架构以及相应的融合层作为融合策略。使用具有 Multi-Dconv Transposed Attention 和 Local-enhanced Feed Forward 网络的 transformer 在深度卷积后提取浅层特征。在三个并行分支编码器中，交叉注意和可逆块 （CAI） 可以提取局部特征并保留高频纹理细节。具有残差连接的基本特征提取模块 （BFE） 可以捕获长程依赖性并增强共享模态表达能力。引入图推理模块 （GR） 来推理高级跨模态关系，同时提取低级细节特征作为 CAI 的特定模态互补信息。实验表明，与最先进的方法相比，我们的方法在可见光/红外图像融合和医学图像融合任务中获得了有竞争力的结果。此外，我们在后续任务方面超过了其他融合方法，在目标检测方面平均得分高出 8.27% mAP@0.5，在语义分割方面高出 5.85% mIoU。</p></details> |  | None |
| **[HSFusion：通过语义和几何域转换的高级视觉任务驱动的红外和可见光图像融合网络(Arxiv)](http://arxiv.org/abs/2407.10047v1)** | 2024-07-14 | <details><summary>Show</summary><p>红外和可见光图像融合已经从面向视觉感知的融合方法发展到既考虑视觉感知又考虑高级视觉任务的策略。然而，现有的任务驱动方法无法解决语义和几何表示之间的领域差距。为了克服这些问题，我们提出了一种通过语义和几何域转换（称为 HSFusion）的高级视觉任务驱动的红外和可见光图像融合网络。具体来说，为了最大限度地减少语义和几何表示之间的差距，我们通过 CycleGAN 框架设计了两个独立的域转换分支，每个分支包括两个过程：正向分割过程和反向重建过程。CycleGAN 能够学习域转换模式，CycleGAN 的重建过程就是在这些模式的约束下进行的。因此，我们的方法可以显著促进语义和几何信息的整合，并进一步减小域差距。在融合阶段，我们将从两个独立 CycleGAN 的重建过程中提取的红外和可见光特征进行整合，以获得融合结果。这些特征包含不同比例的语义和几何信息，可以显着增强高级视觉任务。此外，我们根据分割结果生成掩码，以指导融合任务。这些掩码可以提供语义先验，我们为掩码中的两个不同区域设计了自适应权重，以促进图像融合。最后，我们对我们的方法与其他 11 种最先进的方法进行了比较实验，证明我们的方法在视觉吸引力和语义分割任务方面都优于其他方法。</p></details> |  | None |
| **[MMA-UNet：一种用于红外和可见光图像融合的多模态非对称 UNet 架构(Arxiv)](http://arxiv.org/abs/2404.17747v2)** | 2024-07-11 | <details><summary>Show</summary><p>多模态图像融合 （MMIF） 将来自各种模态的有用信息映射到相同的表示空间中，从而生成信息丰富的融合图像。然而，现有的融合算法倾向于对称地融合多模态图像，导致融合结果的某些区域丢失浅层信息或偏向单一模态。在这项研究中，我们分析了不同模态中信息的空间分布差异，并证明在同一网络内编码特征不利于实现多模态图像的同步深度特征空间对齐。为了克服这个问题，提出了一种多模态非对称 UNet （MMA-UNet）。我们分别为不同的模态训练了专门的特征编码器，并实施了跨尺度融合策略，以将来自不同模态的特征保持在同一表示空间内，从而确保平衡的信息融合过程。此外，还进行了广泛的融合和下游任务实验，以证明 MMA-UNet 在融合红外和可见光图像信息方面的效率，从而产生视觉自然且语义丰富的融合结果。它的性能超过了最先进的比较融合方法。</p></details> |  | [Code Link](https://github.com/JasonWong30/MMA-UNet) |
| **[S4Fusion： 用于红外可见光图像融合的显著性感知选择性状态空间模型(Arxiv)](http://arxiv.org/abs/2405.20881v2)** | 2024-06-03 | <details><summary>Show</summary><p>作为 Image Fusion 的任务之一，Infrared 和 Visible Image Fusion 旨在将不同模态的传感器捕获的互补信息集成到单个图像中。选择性状态空间模型 （SSSM） 以其捕获远程依赖关系的能力而闻名，已在计算机视觉领域展示了其潜力。然而，在图像融合中，目前的方法低估了 SSSM 在捕获两种模式的全局空间信息方面的潜力。这种限制阻止了在交互过程中同时考虑来自两种模式的全局空间信息，导致缺乏对突出目标的全面感知。因此，融合结果倾向于偏向于一种模式，而不是适应性地保留突出的目标。为了解决这个问题，我们提出了 Saliency-aware 选择性状态空间融合模型 （S4Fusion）。在我们的 S4Fusion 中，设计的跨模态空间感知模块 （CMSA） 可以同时关注来自两种模态的全球空间信息，同时促进它们的交互，从而全面捕获互补信息。此外，S4Fusion 利用预先训练的网络来感知融合图像中的不确定性。通过最大限度地减少这种不确定性，S4Fusion 自适应地突出显示了两张图像中的突出目标。大量实验表明，我们的方法可以生成高质量的图像并提高下游任务的性能。</p></details> |  | [Code Link](https://github.com/zipper112/S4Fusion/tree/main) | Hui Li |
| **[FuseFormer: A Transformer for Visual and Thermal Image Fusion(Arxiv)](http://arxiv.org/abs/2402.00971v2)** | 2024-04-24 | <details><summary>Show</summary><p>由于图像融合问题缺乏明确的地面实况，损失函数是根据评估指标构建的，例如结构相似性指数度量 （SSIM）。但是，在这样做的过程中，会向 SSIM 引入偏差，从而引入输入视觉波段图像。本研究的目的是为图像融合问题提出一种新的方法，以减轻与使用经典评估指标作为损失函数相关的局限性。我们的方法集成了基于 transformer 的多尺度融合策略，该策略巧妙地处理本地和全局上下文信息。这种集成不仅改进了图像融合过程的各个组成部分，还显著提高了该方法的整体效率。我们提出的方法遵循一个两阶段的训练方法，其中自动编码器最初被训练为在第一阶段提取多个尺度的深度特征。对于第二阶段，我们集成我们的 fusion block 并更改如前所述的损失函数。多尺度特征使用卷积神经网络 （CNN） 和 Transformer 的组合进行融合。CNN 用于捕获本地特征，而 Transformer 处理一般上下文特征的集成。通过对各种基准数据集的广泛实验，我们提出的方法以及新颖的损失函数定义，与其他竞争性融合算法相比，表现出卓越的性能。</p></details> | 8 pages | None |
| **[MambaDFuse：基于 Manba 的多模态图像融合双相模型(Arxiv)](http://arxiv.org/abs/2404.08406v1)** | 2024-04-12 | <details><summary>Show</summary><p>多模态图像融合 （MMIF） 旨在将来自不同模态的互补信息整合到单个融合图像中，以表示成像场景并全面促进下游视觉任务。近年来，由于深度神经网络的进步，MMIF 任务取得了重大进展。然而，现有方法无法有效且高效地提取受固有局部还原偏差 （CNN） 或二次计算复杂性 （Transformers） 约束的模态特异性和模态融合特征。为了克服这个问题，我们提出了一种基于 Manba 的双相融合 （MambaDFuse） 模型。首先，设计了一种双级特征提取器，通过从 CNN 和 Mamba 块中提取低级和高级特征，从单模态图像中捕获远程特征。然后，提出了一种双相特征融合模块，以获得结合来自不同模态的互补信息的融合特征。它使用通道交换方法进行浅层融合，并使用增强的多模态 Mamba （M3） 模块进行深度融合。最后，融合图像重建模块利用特征提取的逆变换来生成融合结果。通过广泛的实验，我们的方法在红外-可见光图像融合和医学图像融合方面取得了有希望的融合结果。此外，在统一的基准测试中，MambaDFuse 还展示了下游任务（如对象检测）的改进性能。带有检查点的代码将在同行评审过程后可用。</p></details> |  | [Code Link](https://github.com/Lizhe1228/MambaDFuse) |
| **[TSJNet： 一种多模态目标和语义感知联合驱动的图像融合网络(Arxiv)](http://arxiv.org/abs/2402.01212v1)** | 2024-02-02 | <details><summary>Show</summary><p>多模态图像融合涉及将来自不同模态的互补信息集成到单个图像中。当前方法主要侧重于通过单个高级任务来增强图像融合，例如将语义或对象相关信息纳入融合过程。这种方法为同时实现多个目标带来了挑战。我们介绍了一种称为 TSJNet 的目标和语义感知联合驱动融合网络。TSJNet 由以系列结构排列的融合、检测和分割子网组成。它利用从双重高级任务中得出的对象和语义相关信息来指导融合网络。此外，我们提出了一个具有双并行分支结构的局部显著特征提取模块，以充分捕获跨模态图像的细粒度特征，并促进模态、目标和分割信息之间的交互。我们对四个公开可用的数据集（MSRS、M3FD、RoadScene 和 LLVIP）进行了广泛的实验。结果表明，TSJNet 可以产生视觉上令人愉悦的融合结果，与最先进的方法相比，目标检测和分割 mAP @0.5 和 mIoU 分别平均增加 2.84% 和 7.47%。</p></details> |  | None | Xiaosong Li |
| **[遥感中的图像融合：概述和元分析(Arxiv)](http://arxiv.org/abs/2401.08837v1)** | 2024-01-16 | <details><summary>Show</summary><p>遥感 （RS） 中的影像融合一直是一个持续的需求，因为它能够将不同分辨率、来源和模态的原始影像转换为准确、完整且时空相干的图像。它极大地促进了下游应用，例如全色锐化、变化检测、土地覆被分类等。然而，图像融合解决方案与各种遥感问题高度不同，因此在现有综述中通常被狭隘地定义为局部应用，例如全色锐化和时空图像融合。考虑到图像融合理论上可以通过像素级作应用于任何网格化数据，在本文中，我们通过简单的分类法对相关工作进行了全面调查，扩大了其范围：1）多对一图像融合;2） 多对多图像融合。这种简单的分类法将图像融合定义为一个映射问题，该问题根据所需的相干性（例如光谱、空间/分辨率相干性等）将单个或一组图像转换为另一个或一组图像。我们表明，尽管这种简单的分类法涵盖了显着的模态差异，但可以通过概念上简单的框架来表示。此外，我们还提供了一项荟萃分析，以回顾多年来（从 1980 年代至今）研究各种类型图像融合及其应用的主要论文，涵盖 5,926 篇同行评审论文。最后，我们讨论了主要好处和新出现的挑战，以提供开放的研究方向和潜在的未来工作。</p></details> | 21pages, 10 figures | None |
| **[从文本到像素：用于红外和可见光图像融合的上下文感知语义协同解决方案(Arxiv)](http://arxiv.org/abs/2401.00421v1)** | 2023-12-31 | <details><summary>Show</summary><p>随着深度学习技术的快速发展，多模态图像融合在目标检测任务中越来越普遍。尽管融合很受欢迎，但不同来源描述场景内容的方式存在固有差异，这使得融合成为一个具有挑战性的问题。当前的融合方法确定了两种模态之间的共同特征，并使用迭代优化或深度学习架构将它们集成到这个共享域中，这往往忽略了模态之间错综复杂的语义关系，导致对模态间连接的理解肤浅，因此，融合结果欠佳。为了解决这个问题，我们引入了一种文本引导的多模态图像融合方法，该方法利用文本描述中的高级语义来集成来自红外和可见光图像的语义。这种方法利用了不同模式的互补特性，提高了对象检测的准确性和稳健性。该码本用于增强对融合域内和域间动力学的简化和简洁描述，并进行了微调以在检测任务中实现最佳性能。我们提出了一种双层优化策略，该策略在融合和检测的联合问题之间建立了联系，同时优化了这两个过程。此外，我们引入了第一个配对的红外和可见光图像数据集，并附有文本提示，为未来的研究铺平了道路。在多个数据集上的广泛实验表明，我们的方法不仅产生了视觉上优越的融合结果，而且比现有方法实现了更高的检测 mAP，获得了最先进的结果。</p></details> | 10 pages, 12 figures | None | Jinyuan Liu |
| **[SSPFusion：一种用于红外和可见光图像融合的语义结构保留方法(Arxiv)](http://arxiv.org/abs/2309.14745v2)** | 2023-12-26 | <details><summary>Show</summary><p>现有的基于学习的红外和可见光图像融合 （IVIF） 方法大多在融合图像中表现出大量的冗余信息，即产生边缘模糊效果或无法被目标检测器识别。为了缓解这些问题，我们提出了一种 IVIF 的语义结构保留方法，即 SSPFusion。首先，我们设计了一个结构特征提取器 （SFE） 来提取红外和可见光图像的结构特征。然后，我们引入了一个多尺度结构保留融合 （SPF） 模块来融合红外和可见光图像的结构特征，同时保持融合图像和源图像之间语义结构的一致性。由于这两个有效的模块，我们的方法能够从红外和可见光图像对中生成高质量的融合图像，这可以提高下游计算机视觉任务的性能。三个基准的实验结果表明，我们的方法在定性和定量评估方面都优于八种最先进的图像融合方法。我们方法的代码以及其他比较结果将在以下位置提供：https://github.com/QiaoYang-CV/SSPFUSION。</p></details> |  | [Code Link](https://github.com/QiaoYang-CV/SSPFUSION) |
| **[一种基于空间频率集成的双域多曝光图像融合网络(Arxiv)](http://arxiv.org/abs/2312.10604v1)** | 2023-12-17 | <details><summary>Show</summary><p>多重曝光图像融合旨在通过集成具有不同曝光度的图像来生成单个高动态图像。现有的基于深度学习的多重曝光图像融合方法主要集中在空间域融合上，忽视了频域的全局建模能力。为了有效地利用频域的全局照明建模能力，我们提出了一种通过空间频率集成框架（名为 MEF-SFI）进行多曝光图像融合的新视角。最初，我们重新审视了 2D 图像上傅里叶变换的特性，并验证了在频域上进行多次曝光图像融合的可行性，其中振幅和相位分量能够指导照明信息的整合。随后，我们提出了基于深度傅里叶的多重曝光图像融合框架，该框架由空间路径和频率路径组成，分别用于局部和全局建模。具体来说，我们引入了一个空间频率融合模块，以促进双域之间的有效交互，并从具有不同曝光的输入图像中捕获互补信息。最后，我们结合了双域损失函数，以确保在空间域和频域中保留互补信息。对 PQA-MEF 数据集的广泛实验表明，与最先进的多曝光图像融合方法相比，我们的方法获得了视觉上吸引人的融合结果。我们的代码可在 https://github.com/SSyangguang/MEF-freq 获取。</p></details> |  | [Code Link](https://github.com/SSyangguang/MEF-freq) | Xinbo Gao |
| **[用于红外和可见光图像融合的图表示学习(Arxiv)](http://arxiv.org/abs/2311.00291v1)** | 2023-11-01 | <details><summary>Show</summary><p>红外和可见光图像融合旨在提取互补特征以合成单个融合图像。由于卷积神经网络 （CNN） 的平移不变性和局部性，许多方法采用卷积神经网络 （CNN） 来提取局部特征。然而，CNN 没有考虑图像的非局部自相似性 （NLss），尽管它可以通过池化作来扩大感受野，但它仍然不可避免地导致信息丢失。此外，transformer 结构通过考虑所有图像块之间的相关性来提取长距离依赖性，导致这种基于 transformer 的方法存在信息冗余。然而，图形表示比网格 （CNN） 或序列 （transformer 结构） 表示更灵活，可以处理不规则对象，并且图形还可以构建空间可重复细节或具有远空间距离的纹理之间的关系。因此，为了解决上述问题，将图像转换为图空间，从而采用图卷积网络 （GCN） 来提取 NLss 具有重要意义。这是因为该图可以提供精细的结构来聚合特征并在最近的顶点之间传播信息，而不会引入冗余信息。具体来说，我们实现了一种级联 NLss 提取模式，通过探索不同图像像素在图像内和图像间位置距离中的相互作用来提取模态内和模态间的 NLss。我们首先在每个模态内构建 GCN，以聚合特征并传播信息以提取独立的模态内 NLss。然后，对红外和可见光图像的串联模态内NLss特征进行GCN，探索模态间的跨域NLss，重建融合后的图像;消融研究和广泛的实验说明了所提出的方法在三个数据集上的有效性和优越性。</p></details> |  | None |
| **[使用 Wavelet Pooled Edge Preserving 自动编码器的多模态医学神经图像融合(Arxiv)](http://arxiv.org/abs/2310.11910v1)** | 2023-10-18 | <details><summary>Show</summary><p>医学图像融合集成了源图像模态的补充诊断信息，以改进对潜在异常的可视化和分析。最近，基于深度学习的模型通过同时执行特征提取、特征选择和特征融合任务，超越了传统的融合方法。然而，大多数现有的卷积神经网络 （CNN） 架构使用传统的池化或跨步卷积策略来对特征图进行下采样。它会导致源图像中可用的重要诊断信息和边缘细节模糊或丢失，并稀释特征提取过程的有效性。因此，该文提出了一种基于边缘保持密集自编码器网络的多模态医学图像端到端无监督融合模型。在所提出的模型中，通过使用基于小波分解的特征图注意力池来改进特征提取。这有助于保留源图像中存在的精细边缘细节信息，并增强融合图像的视觉感知。此外，所提出的模型在各种医学图像对上进行了训练，这有助于捕捉源图像的强度分布并有效地保留诊断信息。进行的大量实验表明，与其他最先进的融合方法相比，所提出的方法提供了改进的视觉和定量结果。</p></details> | 8 pages | None |
| **[一种基于通道注意力的 Laplacian 自动编码器的新型多模态医学图像融合(Arxiv)](http://arxiv.org/abs/2310.11896v1)** | 2023-10-18 | <details><summary>Show</summary><p>医学影像融合结合了多模态医学影像的互补信息，协助医护人员临床诊断患者的疾病，并在术前和术中过程中提供指导。深度学习 （DL） 模型已经实现了端到端的图像融合，具有高度稳健和准确的融合性能。然而，大多数基于 DL 的融合模型对输入图像执行下采样，以最大限度地减少可学习参数和计算的数量。在此过程中，源图像的显着特征变得无法挽回，导致关键诊断边缘细节和各种脑组织的对比度丢失。在本文中，我们提出了一种新的多模态医学图像融合模型，该模型基于集成拉普拉斯-高斯连接与注意力池 （LGCA）。我们证明我们的模型有效地保留了互补信息和重要的组织结构。</p></details> | 10 pages | None |
| **[具有可变形交叉注意力的三维医学图像融合(Arxiv)](http://arxiv.org/abs/2310.06291v1)** | 2023-10-10 | <details><summary>Show</summary><p>多模态医学图像融合在医学图像处理的多个领域发挥着重要作用，特别是在疾病识别和肿瘤检测方面。传统的融合方法往往在组合特征和重建融合图像之前独立处理每种模态。然而，这种方法往往忽视了多模态信息之间的基本共性和差异。此外，流行的方法主要局限于融合二维 （2D） 医学图像切片，导致融合图像中缺乏上下文监督，随后，相对于三维 （3D） 图像，医生的信息产量降低。在这项研究中，我们引入了一种创新的无监督特征互学融合网络，旨在纠正这些限制。我们的方法包含一个可变形交叉特征混合 （DCFB） 模块，该模块有助于双模态辨别它们各自的相似性和差异性。我们已将我们的模型应用于从阿尔茨海默病神经影像学计划 （ADNI） 数据集中的 660 名患者获得的 3D MRI 和 PET 图像的融合。通过应用 DCFB 模块，我们的网络生成高质量的 MRI-PET 融合图像。实验结果表明，我们的方法在峰值信噪比 （PSNR） 和结构相似性指数测量 （SSIM） 等性能指标上超越了传统的 2D 图像融合方法。重要的是，我们的方法融合 3D 图像的能力增强了医生和研究人员可获得的信息，从而标志着该领域向前迈出了重要一步。该代码将很快在网上提供。</p></details> |  | None |
| **[基于信息互补性的可见光与近红外图像融合算法(Arxiv)](http://arxiv.org/abs/2309.10522v1)** | 2023-09-19 | <details><summary>Show</summary><p>可见光和近红外 （NIR） 波段传感器提供的图像，可捕获来自场景的互补光谱辐射。可见光和 NIR 图像的融合旨在利用它们的光谱特性来提高图像质量。然而，目前的可见光和 NIR 融合算法不能很好地利用光谱特性，并且缺乏信息互补性，这会导致颜色失真和伪影。因此，本文从物理信号层面设计了一个互补融合模型。首先，为了区分噪声和有用信息，我们使用权重导向滤波器和导向滤波器两层，分别获得纹理层和边缘层。其次，为了生成初始可见光-NIR 互补权重图，通过扩展 DoG 滤波器过滤可见光和 NIR 的差异图。之后，NIR 夜间补偿的重要区域通过 arctanI 函数引导初始互补权重图。最后，融合图像可以分别由可见光和 NIR 图像的互补权重图生成。实验结果表明，所提算法不仅能很好地利用光谱特性和信息互补性，而且在保持自然性的同时避免了色彩不自然，优于现有算法。</p></details> |  | None |
| **[使用条件去噪扩散概率模型的高光谱和多光谱图像融合(Arxiv)](http://arxiv.org/abs/2307.03423v1)** | 2023-07-07 | <details><summary>Show</summary><p>高光谱图像 （HSI） 具有大量反映物质特性的光谱信息，但由于成像技术的限制，其空间分辨率较低。与此相辅相成的是多光谱图像 （MSI），例如 RGB 图像，具有高空间分辨率但光谱波段不足。高光谱和多光谱图像融合是一种经济高效地获取具有高空间和高光谱分辨率的理想图像的技术。许多现有的 HSI 和 MSI 融合算法依赖于已知的成像退化模型，而这些模型在实践中通常不可用。在本文中，我们提出了一种基于条件去噪扩散概率模型的深度融合方法，称为 DDPM-Fus。具体来说，DDPM-Fus 包含正向扩散过程，该过程逐渐将高斯噪声添加到高空间分辨率 HSI （HrHSI） 中，以及另一个反向去噪过程，该过程从其噪声版本对相应的高空间分辨率 MSI （HrMSI） 和低空间分辨率 HSI （LrHSI） 的调节中学习预测所需的 HrHSI。训练完成后，建议的 DDPM-Fus 在测试 HrMSI 和 LrHSI 上实施相反的过程，以生成融合的 HrHSI。在一个室内数据集和两个遥感数据集上进行的实验表明，与其他基于深度学习的先进融合方法相比，所提出的模型具有优越性。这项工作的代码将在以下地址开源：https://github.com/shuaikaishi/DDPMFus for reproduucibility。</p></details> |  | [Code Link](https://github.com/shuaikaishi/DDPMFus) |
| **[DePF：一种基于红外和可见光图像分解池的新型融合方法(Arxiv)](http://arxiv.org/abs/2305.17376v2)** | 2023-07-04 | <details><summary>Show</summary><p>红外和可见光图像融合旨在同时生成包含显着特征和丰富纹理细节的合成图像，可用于促进下游任务。然而，现有的融合方法存在纹理丢失和边缘信息不足的问题，导致融合结果欠佳。同时，直接的上采样算子不能很好地保留来自多尺度特征的源信息。为了解决这些问题，提出了一种基于分解池化 （de-pooling） 方式的新型融合网络，称为 DePF。具体来说，基于解池的编码器旨在同时提取源图像的多尺度图像和细节特征。此外，空间注意力模型用于聚合这些显著特征。之后，解码器将重建融合的特征，其中 up-sampling 运算符被 de-pooling 反向作取代。与常见的 max-pooling 技术不同，de-pooling 层后的图像特征可以保留丰富的细节信息，有利于融合过程。在这种情况下，在重建阶段将保留丰富的纹理信息和多尺度信息。实验结果表明，所提方法在多个图像融合基准上表现出优于现有方法的融合性能。</p></details> |  | None | Hui Li |
| **[LE2Fusion：一种用于红外和可见光图像融合的新型局部边缘增强模块(Arxiv)](http://arxiv.org/abs/2305.17374v1)** | 2023-05-27 | <details><summary>Show</summary><p>红外可见光图像融合任务旨在从多源图像中生成包含显著特征和丰富纹理细节的融合图像。然而，在复杂的光照条件下，很少有算法关注局部区域的边缘信息，这对下游任务至关重要。为此，我们提出了一个基于本地边缘增强的融合网络，命名为 LE2Fusion。具体来说，提出了一种局部边缘增强 （LE2） 模块，以改善复杂照明条件下的边缘信息并保留图像的基本特征。对于特征提取，应用多尺度残差注意力 （MRA） 模块来提取丰富的特征。然后，使用 LE2 生成一组增强权重，这些权重用于特征融合策略并用于指导图像重建。为了更好地保留局部细节信息和结构信息，文中还提出了基于局部区域的像素强度损失函数。实验表明，在公共数据集上，所提出的方法比最先进的融合方法表现出更好的融合性能。</p></details> |  | None |  Hui Li |
| **[DCT 域中基于空间频率SF和一致性验证CV的多焦点图像融合(Arxiv)](http://arxiv.org/abs/2305.11265v1)** | 2023-05-18 | <details><summary>Show</summary><p>多焦点是一种专注于特定对象或场景的不同方面的技术。无线视觉传感器网络 （WVSN） 使用多焦点图像融合，将两个或多个图像组合在一起，以创建比任何单个输入图像更准确的输出图像，更好地描述场景。WVSN 具有多种应用，包括视频监控、监控和跟踪。因此，对这些网络进行高级分析可以使 Biometrics 受益。本文介绍了一种算法，该算法利用离散余弦变换 （DCT） 标准在 WVSN 中融合多焦点图像。源图像中相应块的空间频率 （SF） 决定了融合标准。具有较高空间频率的块构成了融合图像的 DCT 表示，并使用一致性验证 （CV） 程序来提高输出图像质量。在基于 JPEG 标准编码的多对多焦点图像上测试了所提出的融合方法，以评估融合性能，结果表明它提高了输出图像的视觉质量，并优于其他基于 DCT 的技术。</p></details> |  | None |


## 期刊
| **题目** | **日期** | **摘要** | **录用信息** | **代码** | **备注**
| --- | --- | --- | --- | --- | --- |
| **[ReFusion：通过元学习从具有可学习损失的重建中学习图像融合(Arxiv)](http://arxiv.org/abs/2312.07943v3)** | 2025-02-03 | <details><summary>Show</summary><p>Image fusion 旨在将来自多个源图像的信息合并为一个具有更全面信息内容的源图像。基于深度学习的图像融合算法面临重大挑战，包括缺乏明确的地面实况和相应的距离测量。此外，当前手动定义的损失函数限制了模型在各种融合任务中的灵活性和泛化性。为了解决这些限制，我们提出了 ReFusion，这是一个基于元学习的统一图像融合框架，它通过源图像重建动态优化各种任务的融合损失。与现有方法相比，ReFusion 采用参数化损失函数，允许根据特定的融合场景和任务动态调整训练框架。ReFusion 由三个关键组件组成：融合模块、源重构模块和损失建议模块。我们采用元学习策略来使用重建损失来训练损失提案模块。这种策略迫使融合图像更有利于重建源图像，允许损失建议模块生成自适应融合损失，从而保留源图像的最佳信息。fusion 模块的更新依赖于 loss proposal 模块提出的可学习的 fusion loss。这三个模块交替更新，相互增强，以优化不同任务的熔合损失，并始终如一地获得令人满意的结果。大量实验表明，ReFusion 能够适应各种任务，包括红外可见光、医疗、多焦点和多曝光图像融合。</p></details> | IJCV2024 | [Code Link](https://github.com/HaowenBai/ReFusion) | Zixiang Zhao |
| **[基于归因分析的深度展开多模态图像融合网络(Arxiv)](http://arxiv.org/abs/2502.01467v1)** | 2025-02-03 | <details><summary>Show</summary><p>多模态图像融合将来自多个来源的信息合成到单个图像中，从而促进语义分割等下游任务。目前的方法主要侧重于通过复杂的映射在视觉显示层获取信息丰富的融合图像。尽管一些方法试图共同优化图像融合和下游任务，但这些工作通常缺乏直接的指导或交互，仅用于帮助解决预定义的融合损失。为了解决这个问题，我们提出了一个 “展开归因分析融合网络”（UAAFusion），使用归因分析更有效地定制融合图像以进行语义分割，增强融合和分割之间的交互。具体来说，我们利用归因分析技术来探索源图像中语义区域对任务辨别的贡献。同时，我们的融合算法整合了源图像中更多有益的特征，从而允许分割指导融合过程。我们的方法构建了一个模型驱动的展开网络，该网络使用从归因分析中得出的优化目标，并根据细分网络的当前状态计算归因融合损失。我们还开发了一种新的归因分析路径函数，专门针对我们展开网络中的融合任务量身定制。在每个网络阶段都集成了归因注意力机制，使融合网络能够优先考虑对高级识别任务至关重要的区域和像素。此外，为了减轻传统展开网络中的信息丢失，我们的网络中集成了内存增强模块，以改善各个网络层之间的信息流。广泛的实验证明了我们的方法在图像融合和语义分割方面的优越性。</p></details> | TCSVT2024 | [Code Link](https://github.com/HaowenBai/UAAFusion) | Zixiang Zhao |
| **[FusionMamba：使用 Mamba 进行多模态图像融合的动态功能增强(Arxiv)](http://arxiv.org/abs/2404.09498v3)** | 2025-02-02 | <details><summary>Show</summary><p>多模态图像融合旨在整合来自不同成像技术的信息，为下游视觉任务生成全面、细节丰富的单张图像。基于局部卷积神经网络 （CNN） 的现有方法难以有效地捕获全局特征，而基于 Transformer 的模型尽管擅长全局建模，但计算成本很高。Mamba 通过利用选择性结构化状态空间模型 （S4） 来有效处理长期依赖关系，同时保持线性复杂性，从而解决了这些限制。在本文中，我们提出了 FusionMamba，这是一种新颖的动态特征增强框架，旨在克服 CNN 和视觉转换器 （ViT） 在计算机视觉任务中面临的挑战。该框架通过集成动态卷积和通道注意力机制，对视觉状态空间模型 Mamba 进行了改进，不仅保留了其强大的全局特征建模能力，还大大减少了冗余，增强了局部特征的表现力。此外，我们还开发了一个名为动态特征融合模块 （DFFM） 的新模块。它将用于纹理增强和视差感知的动态特征增强模块 （DFEM） 与跨模态融合 Mamba 模块 （CMFM） 相结合，后者专注于增强模态间相关性，同时抑制冗余信息。实验表明，FusionMamba 在各种多模态图像融合任务以及下游实验中取得了最先进的性能，证明了其广泛的适用性和优越性。</p></details> | Visual Intelligence | [Code Link](https://github.com/millieXie/FusionMamba) |
| **[CoCoNet：用于多模态图像融合的多级特征集成耦合对比学习网络(Arxiv)](http://arxiv.org/abs/2211.10960v3)** | 2024-12-14 | <details><summary>Show</summary><p>红外和可见光图像融合目标，通过组合来自不同传感器的互补信息来提供信息丰富的图像。现有的基于学习的融合方法试图构建各种损失函数来保留互补特征，而忽视了发现两种模态之间的相互关系，导致融合结果的信息冗余甚至无效。此外，大多数方法都侧重于通过增加深度来加强网络，而忽视了特征传输的重要性，导致重要信息退化。为了缓解这些问题，我们提出了一种耦合的对比学习网络，称为 CoCoNet，以实现端到端的红外和可见光图像融合。具体来说，为了同时保留两种模态的典型特征并避免在融合结果上出现伪影，我们在损失函数中开发了一个耦合对比约束。在融合图像中，其前景目标/背景细节部分被拉近红外/可见光源，并被推离表示空间中的可见光/红外光源。我们进一步利用图像特征来提供数据敏感的权重，使我们的损失函数能够与源图像建立更可靠的关系。建立多层次注意力模块，学习丰富的层次特征表示，并在融合过程中全面传递特征。我们还将所提出的 CoCoNet 应用于不同类型的医学图像融合，例如磁共振图像、正电子发射断层扫描图像和单光子发射计算机断层扫描图像。广泛的实验表明，我们的方法在主观和客观评估下都实现了最先进的 （SOTA） 性能，尤其是在保留突出目标和恢复重要纹理细节方面。</p></details> | IJCV2024 | [Code Link](https://github.com/runjia0124/CoCoNet) | Jinyuan Liu |
| **[Conti-Fuse：一种用于红外和可见光图像的新型基于连续分解的融合框架(Arxiv)](http://arxiv.org/abs/2406.04689v3)** | 2024-12-03 | <details><summary>Show</summary><p>为了更好地探索模态间和内模态的关系，即使在深度学习融合框架中，分解的概念也起着至关重要的作用。然而，以前的分解策略（基础和细节或低频和高频）过于粗糙，无法呈现源模态的共同特征和独特特征，这导致了融合图像的质量下降。现有的策略将这些关系视为一个二进制系统，这可能不适合复杂的生成任务（例如图像融合）。针对这一问题，提出了一种基于连续分解的融合框架（Conti-Fuse）。Conti-Fuse 将分解结果视为沿源图像特征变化轨迹的少量样本，并将此概念扩展到更通用的状态，以实现连续分解。这种新颖的连续分解策略通过增加分解样本的数量来增强模态间互补信息的表示，从而减少关键信息的损失。为了促进这一过程，引入了连续分解模块 （CDM） 将输入分解为一系列连续分量。CDM 的核心模块 State Transformer （ST） 用于有效地捕获来自源模态的互补信息。此外，还设计了一种新的分解损失函数，该函数保证了分解过程的顺利进行，同时保持了时间复杂度相对于分解样本数量的线性增长。大量实验表明，与最先进的熔融方法相比，我们提出的 Conti-Fuse 实现了卓越的性能。</p></details> | Inf. Fusion2025 | None | Hui Li |
| **[FusionMamba：基于状态空间模型的高效遥感图像融合(Arxiv)](http://arxiv.org/abs/2404.07932v3)** | 2024-11-17 | <details><summary>Show</summary><p>遥感影像融合旨在通过将具有有限光谱数据的高分辨率图像与富含光谱信息的低分辨率图像相结合，生成高分辨率多/高光谱图像。当前的深度学习 （DL） 方法通常采用卷积神经网络 （CNN） 或 Transformer 进行特征提取和信息集成。虽然 CNN 很有效，但它们有限的感受野限制了它们捕获全局上下文的能力。Transformer 擅长学习全局信息，但计算成本高昂。状态空间模型 （SSM） 的最新进展，尤其是 Mamba，通过实现低复杂度的全局感知，提供了一种很有前途的替代方案。然而，SSM 在信息集成方面的潜力在很大程度上仍未得到开发。因此，我们提出了 FusionMamba，一种高效的遥感图像融合创新方法。我们的贡献是双重的。首先，为了有效地融合空间和光谱特征，我们扩展了单输入 Mamba 模块以适应双输入，创建了 FusionMamba 模块，作为信息集成的即插即用解决方案。其次，我们将 Mamba 和 FusionMamba 块整合到一个为遥感图像融合量身定制的可解释网络架构中。我们的设计利用两个 U 形网络分支，每个分支主要由四个方向的 Mamba 块组成，以分别和分层地提取空间和光谱特征。生成的特征图被充分合并到一个用 FusionMamba 块构建的辅助网络分支中。此外，我们通过增强的通道注意力模块改进了光谱信息的表示。六个数据集的定量和定性估值结果表明，我们的方法实现了 SOTA 性能。该代码可在 https://github.com/PSRben/FusionMamba 获取。</p></details> | TGRS | [Code Link](https://github.com/PSRben/FusionMamba) |
| **[CHITNet：红外和可见光图像融合的和谐信息传输网络的补充(Arxiv)](http://arxiv.org/abs/2309.06118v6)** | 2024-10-22 | <details><summary>Show</summary><p>目前的红外和可见光图像融合 （IVIF） 方法不遗余力地挖掘互补特征并设计复杂的融合策略，极具挑战性。为此，我们跳出框框重新思考 IVIF，提出了与和谐信息传递网络 （CHITNet） 相辅相成的方法。它合理地将互补信息转化为和谐信息，将两种模态的共享特征和互补特征融为一体。具体来说，为了巧妙地回避 IVIF 中的互补信息聚合，我们设计了一个互信息传递 （MIT） 模块来相互表示两种模态的特征，将互补信息大致转化为和谐的信息。然后，设计了源图像监督的和谐信息采集 （HIASSI） 模块，以进一步保证 MIT 后和谐信息传递的互补性。同时，我们还提出了结构信息保存 （SIP） 模块，以保证源图像的边缘结构信息能够传递到融合结果中。此外，采用交互损失的互促培训范式，以促进 MIT、HIASSI 和 SIP 之间更好的协作。通过这种方式，所提出的方法能够生成具有更高质量 的融合图像。广泛的实验结果表明，CHITNet 在视觉质量和定量评估方面优于最先进的算法。</p></details> | TIM2025 | [Code Link](https://github.com/lhf12278/CHITNet) | Huafeng Li |
| **[寻找紧凑的架构以实现强大的多重曝光图像融合(Arxiv)](http://arxiv.org/abs/2305.12236v2)** | 2024-08-26 | <details><summary>Show</summary><p>近年来，基于学习的方法在多重曝光图像融合方面取得了重大进展。然而，有两个主要的绊脚石阻碍了开发，包括像素错位和低效推理。在现有方法中依赖对齐的图像对会导致设备运动导致容易出现伪影。此外，现有技术通常依赖于具有庞大网络工程的手工架构，导致参数冗余，对推理效率和灵活性产生不利影响。为了减轻这些限制，本研究引入了一种基于架构搜索的范式，该范式结合了自对准和细节填充模块，以实现强大的多重曝光图像融合。具体来说，针对曝光的极端差异，我们提出了自对齐模块，利用场景重新照明来限制照明度，以便进行后续对齐和特征提取。建议进行细节填充以增强场景的纹理细节。此外，结合硬件敏感约束，我们提出了面向融合的架构搜索，以探索紧凑高效的融合网络。所提出的方法优于各种竞争方案，在一般情况下实现了 3.19% 的 PSNR 显着提高，在错位场景中实现了令人印象深刻的 23.5% 的提高。此外，它还将推理时间显著缩短了 69.1%。该代码将在 https://github.com/LiuZhu-CV/CRMEF 上提供。</p></details> | TCSVT2024 | [Code Link](https://github.com/LiuZhu-CV/CRMEF) | Jinyuan Liu |
| **[FCDFusion：一种用于融合可见光和红外图像对的快速、低颜色偏差方法(Arxiv)](http://arxiv.org/abs/2408.01080v1)** | 2024-08-02 | <details><summary>Show</summary><p>可见光和红外图像融合 （VIF） 旨在将可见光和红外图像的信息组合成单个融合图像。以前的 VIF 方法通常采用色彩空间变换来保持原始可见图像的色相和饱和度。但是，对于快速 VIF 方法，此作占计算的大部分，并且是阻止更快处理的瓶颈。在本文中，我们提出了一种颜色偏差很小的快速融合方法FCDFusion。它通过在 RGB 色彩空间中直接作来保留颜色信息，而无需进行色彩空间转换。它以很少的额外成本集成了 Gamma 校正，从而可以快速改善色彩和对比度。我们将融合过程视为对 3D 颜色向量的缩放作，从而大大简化了计算。理论分析和实验表明，我们的方法每像素只需 7 FLOPs 就可以获得令人满意的结果。与使用 HSV 色彩空间的最先进的快速、保色方法相比，我们的方法以一半的计算成本提供了更高的对比度。我们进一步提出了一个新的指标 Color deviation，用于测量 VIF 方法保留颜色的能力。它专为具有彩色可见光图像的 VIF 任务而设计，并克服了用于此目的的现有 VIF 指标的缺陷。我们的代码可在 https://github.com/HeasonLee/FCDFusion 获取。</p></details> | Computational Visual Media | [Code Link](https://github.com/HeasonLee/FCDFusion) |
| **[CSAKD：用于高光谱和多光谱图像融合的交叉自注意力知识蒸馏(Arxiv)](http://arxiv.org/abs/2406.19666v1)** | 2024-06-28 | <details><summary>Show</summary><p>高光谱成像可捕获每个像素的详细光谱信息，在各种科学和工业应用中至关重要。然而，由于现有成像系统的硬件限制，通常需要解决高分辨率 （HR） 高光谱图像 （HSI） 的采集问题。一种流行的解决方法包括捕获高分辨率多光谱图像 （HR-MSI） 和低分辨率 （LR） HSI，然后将它们融合以产生所需的 HR-HSI。尽管基于深度学习的方法在 HR-MSI/LR-HSI 融合和 LR-HSI 超分辨率 （SR） 方面显示出前景，但它们巨大的模型复杂性阻碍了在资源受限的成像设备上的部署。本文介绍了一种新的知识蒸馏 （KD） 框架，用于 HR-MSI/LR-HSI 融合以实现 LR-HSI 的 SR。我们的 KD 框架集成了所提出的跨层残差聚合 （CLRA） 块，以提高构建双双流 （DTS） 网络结构的效率，旨在同时从 LR-HSI 和 HR-MSI 中提取联合和不同的特征。为了充分利用 LR-HSI 和 HR-MSI 的空间和光谱特征表示，我们提出了一种新的交叉自我注意力 （CSA） 融合模块来自适应融合这些特征，以提高重建 HR-HSI 的空间和光谱质量。最后，采用所提出的基于 KD 的联合损失函数对教师和学生网络进行共训练。我们的实验结果表明，学生模型不仅实现了相当或更好的 LR-HSI SR 性能，而且还显着降低了模型大小和计算要求。这标志着对现有最先进方法的重大进步。源代码可在 https://github.com/ming053l/CSAKD 获取。</p></details> | TIP2024 | [Code Link](https://github.com/ming053l/CSAKD) |
| **[IAIFNet：照明感知红外和可见光图像融合网络(Arxiv)](http://arxiv.org/abs/2309.14997v3)** | 2024-05-26 | <details><summary>Show</summary><p>红外可见光图像融合 （IVIF） 用于生成具有两种图像综合特征的融合图像，有利于下游视觉任务。然而，目前的方法很少考虑弱光环境下的照度条件，融合图像中的目标往往不突出。为了解决上述问题，我们提出了一种照明感知红外和可见光图像融合网络，命名为 IAIFNet。在我们的框架中，照明增强网络首先估计输入图像的入射照明图。随后，借助所提出的自适应差分融合模块（ADFM）和显著目标感知模块（STAM），图像融合网络将照明增强红外和可见光图像的显著特征有效地融合成高视觉质量的融合图像。大量的实验结果验证了我们的方法优于五种最先进的红外和可见光图像融合方法。</p></details> | IEEE Signal Processing Letters| None |
| **[TextFusion：揭示文本语义在可控图像融合中的力量(Arxiv)](http://arxiv.org/abs/2312.14209v2)** | 2024-02-08 | <details><summary>Show</summary><p>先进的图像融合方法致力于通过聚合源图像传达的互补信息来生成融合结果。然而，成像场景内容的源特定表现形式存在差异，因此很难设计出健壮且可控的融合过程。我们认为，这个问题可以在文本模态传达的更高级别语义的帮助下得到缓解，这应该使我们能够以可控的方式生成用于不同目的的融合图像，例如可视化和下游任务。这是通过利用视觉和语言模型在文本和图像信号之间构建粗到细的关联机制来实现的。在关联图的引导下，在 transformer 网络中嵌入了一个仿射融合单元，以在特征级别融合文本和视觉模态。作为这项工作的另一个组成部分，我们建议使用文本注意来使图像质量评估适应融合任务。为了促进所提出的文本引导融合范式的实施，并被更广泛的研究界采用，我们发布了一个文本注释的图像融合数据集 IVT。广泛的实验表明，我们的方法 （TextFusion） 始终优于传统的基于外观的融合方法。我们的代码和数据集将在 https://github.com/AWCXV/TextFusion 上公开提供。</p></details> | Inf. Fusion |None | Hui Li | Hui Li |
| **[AdaFuse：基于空间频率交叉注意的自适应医学图像融合(Arxiv)](http://arxiv.org/abs/2310.05462v2)** | 2023-10-24 | <details><summary>Show</summary><p>多模态医学图像融合可以将多模态中的互补信息合并为一张图像，这对于临床精准诊断和手术导航至关重要。融合图像的质量取决于提取的单模态特征以及多模态信息的融合规则。现有的基于深度学习的融合方法可以充分利用每种模态的语义特征，它们无法区分每种模态的有效低频和高频信息并对其进行自适应融合。为了解决这个问题，我们提出了 AdaFuse，其中多模态图像信息通过基于傅里叶变换的频率引导注意力机制自适应融合。具体来说，我们提出了交叉注意力融合 （CAF） 块，它通过交换键和查询值来自适应地融合空间域和频域两种模态的特征，然后计算空间和频率特征之间的交叉注意力分数，以进一步指导空间频率信息融合。CAF 块增强了不同模态的高频特征，以便可以保留融合图像中的细节。此外，我们设计了一种由结构损失和内容损失组成的新颖损失函数，以保留低频和高频信息。对几个数据集的广泛比较实验表明，所提出的方法在视觉质量和定量指标方面都优于最先进的方法。消融实验还验证了所提出的损失和融合策略的有效性。</p></details> | BSPC2024 | [Code Link](https://github.com/xianming-gu/AdaFuse) |  |
| **[通过一步式渐进密集配准改进错位多模态图像融合(Arxiv)](http://arxiv.org/abs/2308.11165v1)** | 2023-08-22 | <details><summary>Show</summary><p>多模态图像之间的错位给图像融合带来了挑战，表现为结构失真和边缘重影。现有的工作通常采用先注册后融合的方式，通常采用两个级联注册阶段，即粗注册和精细注册。这两个阶段都直接估计相应的目标变形场。在本文中，我们认为分离的两阶段配准不紧凑，目标变形场的直接估计不够准确。为了应对这些挑战，我们提出了一种跨模态多尺度渐进式密集配准 （C-MPDR） 方案，该方案仅使用一阶段优化完成从粗到精的配准，从而提高了未对准的多模态图像的融合性能。具体来说，涉及两个关键组件，一个密集变形场融合 （DFF） 模块和一个渐进特征精细 （PFF） 模块。DFF 在当前尺度上聚合预测的多尺度变形子场，而 PFF 逐步细化剩余的未对准特征。两者协同工作以准确估计最终变形场。此外，我们还开发了一个基于 Transformer-Conv 的融合 （TCF） 子网络，该子网络考虑了局部和远程特征依赖性，使我们能够从注册的红外和可见光图像中捕获更多信息丰富的特征，以生成高质量的融合图像。广泛的实验分析证明了所提出的方法在未对准的跨模态图像融合方面的优越性。</p></details> | TCSVT2024 | [Code Link](https://github.com/wdhudiekou/IMF) | Jinyuan Liu |
| **[用于图像融合的任务导向、隐式搜索和元初始化的深度模型(Arxiv)](http://arxiv.org/abs/2305.15862v1)** | 2023-05-25 | <details><summary>Show</summary><p>图像融合在各种基于多传感器的视觉系统中发挥着关键作用，特别是对于提高视觉质量和/或提取聚合特征以进行感知。然而，大多数现有的方法只是将图像融合视为一项单独的任务，从而忽略了它与这些下游视觉问题的潜在关系。此外，设计适当的融合架构通常需要大量的工程劳动力。它还缺乏提高当前融合方法的灵活性和泛化能力的机制。为了缓解这些问题，我们建立了一个任务导向、隐式搜索和元初始化 （TIM） 深度模型，以解决具有挑战性的真实场景中的图像融合问题。具体来说，我们首先提出了一种约束策略，以整合来自下游任务的信息，以指导图像融合的无监督学习过程。然后，在这个框架内，我们设计了一个隐式搜索方案，以高效地自动发现融合模型的紧凑架构。此外，还引入了一种前置元初始化技术，以利用发散融合数据来支持对不同类型图像融合任务的快速适应。不同类别的图像融合问题和相关下游任务（例如，视觉增强和语义理解）的定性和定量实验结果证实了我们 TIM 的灵活性和有效性。源代码将在 https://github.com/LiuZhu-CV/TIMFusion 上提供。</p></details> | TPAMI2024 | [Code Link](https://github.com/LiuZhu-CV/TIMFusion) | Jinyuan Liu |
| **[用于联合红外-可见光图像融合和显著性目标检测的交互式增强范式(Arxiv)](http://arxiv.org/abs/2305.09999v1)** | 2023-05-17 | <details><summary>Show</summary><p>这项研究的重点是在野外发现和定位隐藏的物体，并为无人系统服务。通过实证分析，红外和可见光图像融合 （IVIF） 使难以找到的物体变得明显，而多模态显著物体检测 （SOD） 则准确描绘了图片中物体的精确空间位置。它们从不同源图像中寻找互补线索的共同特征促使我们首次通过交互式增强多任务范式（称为 IRFS）探索红外和可见光图像上的 Fusion 和 Salient 对象检测任务之间的协作关系。为了实现多模态图像融合和 SOD 任务的无缝桥梁，我们专门开发了一种基于特征筛选的融合子网 （FSFNet），以从源图像中筛选出干扰特征，从而保留与显著性相关的特征。通过 FSFNet 生成融合图像后，将其输入到随后的融合引导交叉互补 SOD 子网络 （FC$^2$Net） 中，作为第三种模式，通过利用从融合图像得出的互补信息来驱动显著图的精确预测。此外，我们开发了一种交互式循环学习策略，以更短的训练周期和更少的网络参数实现 IVIF 和 SOD 任务的相互强化。综合实验结果表明，IVIF 和 SOD 的无缝桥接相互增强其性能，凸显其优越性。</p></details> | Inf. Fusion2023 | [Code Link](https://github.com/wdhudiekou/IRFS) | Jinyuan Liu |
| **[DCINN: 用于图像融合的细节保持条件可逆网络的一般范式](https://link.springer.com/article/10.1007/s11263-023-01924-5)** | 2023-04-03 | <details><summary>Show</summary><p> 现有的图像融合深度学习技术要么直接学习图像映射(LIM)，由于每个像素的考虑相等，这使得它们在保留细节方面无效，要么学习细节映射(LDM)，它只获得有限的性能水平，因为只有细节用于推理。最近的无损可逆网络(INN)已经证明了其细节保持能力。然而，INN对图像融合任务的直接适用性受到保体积约束的限制。此外，缺乏一致的保细节图像融合框架来产生令人满意的结果。为此，我们提出了一种基于新条件 INN（称为 DCINN）的图像融合通用范式。DCINN范式有三个核心组件:将图像映射转换为细节映射的分解模块;一个直接从源图像中提取辅助特征的辅助网络(ANet);以及一个条件INN (CINN)，它学习基于辅助特征的细节映射。新颖的设计得益于 INN、LIM 和 LDM 方法的优点，同时避免它们的缺点。特别是，使用 INN 到 LDM 可以轻松满足保体积约束，同时仍然保留细节。此外，由于辅助特征作为条件特征，ANet 允许在不影响细节映射的情况下使用不仅仅是推理的细节。在三个基准融合问题上的广泛实验，即全色锐化、高光谱和多光谱图像融合，以及红外和可见光图像融合，证明了我们的方法与最近的最先进方法相比的优越性。 </p></details> | IJCV2024 |  [Code Link](https://github.com/wwhappylife/DCINN) |
| **[CRMEF: 一种结构紧凑的鲁棒多曝光图像融合算法]()** | 2024-01 | <details><summary>Show</summary><p>近年来，基于学习的图像融合方法在多曝光图像融合方面取得了显著的进展。然而，两个主要的障碍阻碍发展，包括像素错位和低效的推理。在现有方法中依赖对齐的图像对会导致由于设备运动而产生伪影。此外，现有技术通常依赖于具有大型网络工程的手工架构，导致参数冗余，从而对推理效率和灵活性产生不利影响。为了减轻这些局限性，本研究引入了一种基于体系结构搜索的范例，该范例结合了自对齐和细节填充模块，用于鲁棒的多曝光图像融合。针对曝光差异较大的特点，提出了自对准模块，利用场景重光度约束对准和特征提取后的光照度。为了增强场景的纹理细节，提出了细节填充算法。此外，结合硬件敏感的约束，我们提出了面向融合的体系结构搜索，以探索紧凑和有效的网络融合。该方法优于各种竞争方案，在一般情况下 PSNR 提高了 3.19% ，在失调情况下提高了 23.5% 。此外，该方法显著缩短了推理时间 69.1%。</p></details> | TCSVT2024 |  [Code Link](https://github.com/LiuZhu-CV/CRMEF) | Jinyuan Liu |
| **[CCAFusion: 红外与可见光图像融合的交叉模态坐标注意网络](https://ieeexplore.ieee.org/document/10175595)** | 2023 | <details><summary>Show</summary><p> 红外与可见光图像融合的目的是生成一幅具有综合信息的图像。它可以保持丰富的纹理特征和热信息。然而，对于现有的图像融合方法，融合后的图像要么牺牲热目标的显著性和纹理的丰富性，要么引入伪影等无用信息的干扰。为了解决这些问题，本文提出了一种有效的红外与可见光图像融合的跨模态坐标注意网络 CCAFusion。为了充分集成互补特征，设计了基于坐标注意的跨模态图像融合策略，该策略由特征感知融合模块和特征增强融合模块组成。此外，采用基于跳跃连接的多尺度网络获取红外图像和可见光图像的多尺度特征，充分利用融合过程中的多尺度信息。为了减少融合图像与输入图像之间的差异，提出了一种包含基损失和辅助损失的多约束损失函数，以调整图像的灰度分布，保证融合图像结构和强度的和谐共存，从而防止伪影等无用信息的污染。在广泛使用的数据集上进行的大量实验表明，我们的 CCAFusion 在定性评价和定量测量方面都比最先进的图像融合方法有更好的性能。此外，应用于显著目标检测显示我们的 CCAFusion 潜在的高水平视觉任务，可以有效地提高检测性能。 </p></details> | TCSVT2023 |  [Code(NO)](https://github.com/Li-Xiaoling/CCAFusion) | Pan Pan |
| **[MLFuse: 多场景特征联合学习在多模态图像融合中的应用](https://ieeexplore.ieee.org/document/10856398)** | 2025 | <details><summary>Show</summary><p> 多模态图像融合 (MMIF) 需要合成具有详细纹理和突出对象的图像。现有方法倾向于使用一般特征提取来处理不同的融合任务。然而，由于缺乏有针对性的学习途径，这些方法难以跨越各种模式的融合障碍。在这项工作中，我们提出了一个多场景特征联合学习架构 MLFuse，它利用多模态图像的共性来解构融合过程。具体来说，我们构建了一个跨模态知识增强网络，该网络采用多路径标定策略来促进不同图像之间的信息交流。此外，两个专业网络的发展，以维护显着和纹理信息的融合结果。空间 - 光谱域优化网络可以借助空间注意和光谱注意来学习源图像上下文的重要关系。该边缘引导学习网络利用各种接收域的卷积运算来获取图像纹理信息。通过聚合三个网络的输出，得到了期望的融合结果。大量的实验证明了 MLFuse 在红外-可见光图像融合和医学图像融合中的优越性。下游任务 (即目标检测和语义分割) 的出色结果进一步验证了我们方法的高质量融合性能。 </p></details> | TMM2025 |  [Code Link](https://github.com/jialei-sc/MLFuse) | Jinyuan Liu |
| **[ITFuse: 用于红外和可见光图像融合的交互式Transformer]()** | 2024 | <details><summary>Show</summary><p> 红外与可见光图像融合技术 (IVIF) 因其在下游应用中取得了令人满意的效果而越来越受到人们的关注。然而，现有的深度融合模型大多是特征级融合或图像级融合，导致信息丢失。在本文中，我们提出了一个交互式转换器的 IVIF，称为 ITFuse。与以往的算法相比，ITFuse 由特征交互模块 (FIM) 和特征重构模块 (FRM) 组成，可以交替提取和集成重要特征。具体来说，为了充分利用不同源图像的共同特征，我们设计了一个剩余注意块 (RAB) 来实现互特征表示。为了聚合相应输入图像中存在的独特特征，我们利用交互注意 (ITA) 来整合互补信息，以实现全面的特征保存和交互。此外，提出了交叉模式注意 (CMA) 和变压器块 (TRB) ，以充分融合大写特征和构建长程关系。此外，我们设计了像素损失和结构损失来训练提出的深度融合模型在无监督的方式超额性能改善。在流行数据库上的大量实验表明，我们的 ITFuse 在定性和定量评估方面都比其他代表性的最先进的方法表现得更好。 </p></details> | PR2024 |  [Code Link](https://github.com/tthinking/ITFuse) | Wei Tang |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |

## 会议
| **题目** | **日期** | **摘要** | **录用信息** | **代码** | **备注**
| --- | --- | --- | --- | --- | --- |
| **[VIIS：用于严重低光图像增强的可见光和红外信息合成(Arxiv)](http://arxiv.org/abs/2412.13655v2)** | 2025-02-13 | <details><summary>Show</summary><p>在严重的低光环境下拍摄的图像通常会出现大量信息缺失的情况。现有的奇异模态图像增强方法难以恢复缺乏有效信息的图像区域。通过利用不透光的红外图像，可见光和红外图像融合方法有可能揭示隐藏在黑暗中的信息。然而，它们主要强调模态间互补，而忽视了模态内增强，限制了输出图像的感知质量。为了解决这些限制，我们提出了一项称为可见光和红外信息合成 （VIIS） 的新任务，旨在实现两种模式的信息增强和融合。鉴于在 VIIS 任务中难以获得地面实况，我们设计了一个基于图像增强的信息合成前置任务 （ISPT）。我们采用扩散模型作为框架，设计了一种稀疏基于注意力的双模态残差 （SADMR） 条件反射机制，以增强两种模态之间的信息交互。这种机制使具有两种模态先验知识的特征能够在去噪过程中自适应和迭代地关注每个模态的信息。我们广泛的实验表明，我们的模型不仅在定性和定量上优于相关领域最先进的方法，而且优于能够进行信息增强和融合的新设计的基线。该代码可在 https://github.com/Chenz418/VIIS 获取。</p></details> | WACV2025 | [Code Link](https://github.com/Chenz418/VIIS) |
| **[重新思考改进多模态图像分割的早期融合策略(Arxiv)](http://arxiv.org/abs/2501.10958v1)** | 2025-01-19 | <details><summary>Show</summary><p>RGB和热成像图像融合在低光照条件下具有显著的潜力，能够提高语义分割的效果。现有方法通常采用双分支编码器框架进行多模态特征提取，并设计复杂的特征融合策略以实现多模态语义分割的特征提取和融合。然而，这些方法在特征提取和融合过程中需要大量的参数更新和计算工作。为了解决这一问题，我们提出了一种新颖的多模态融合网络（EFNet），基于早期融合策略和简单但有效的特征聚类方法，用于高效训练RGB-T语义分割。此外，我们还提出了一种基于欧几里得距离的轻量级高效多尺度特征聚合解码器。我们在不同数据集上验证了我们方法的有效性，并在参数和计算量更少的情况下超越了先前的最先进方法。</p></details> | ICASSP2025 | None |
| **[A2RNet：用于强大红外和可见光图像融合的对抗性攻击弹性网络(Arxiv)](http://arxiv.org/abs/2412.09954v2)** | 2024-12-18 | <details><summary>Show</summary><p>红外和可见光图像融合 （IVIF） 是一种通过将来自不同模态的独特信息集成到一个融合图像中来提高视觉性能的关键技术。现有的方法更注重使用未受干扰的数据进行融合，而忽视了故意干扰对融合结果有效性的影响。为了研究融合模型的鲁棒性，在本文中，我们提出了一种新的对抗性攻击弹性网络，称为 $\textrm{A}^{\textrm{2}}$RNet。具体来说，我们开发了一个具有抗攻击损失函数的对抗范式来实现对抗性攻击和训练。它是基于 IVIF 的内在本质构建的，为未来的研究进展提供了坚实的基础。在这种范式下，我们采用 Unet 作为管道，并使用基于 transformer 的防御优化模块 （DRM），以稳健的粗到精方式保证融合图像质量。与以前的工作相比，我们的方法减轻了对抗性扰动的不利影响，始终保持高保真融合结果。此外，下游任务的性能也可以在对抗性攻击下得到很好的维护。代码可在 https://github.com/lok-18/A2RNet 获取。</p></details> | AAAI2025 | [Code Link](https://github.com/lok-18/A2RNet) | Jinyuan Liu |
| **[BSAFusion：用于未对齐医学图像融合的双向逐步特征对齐网络(Arxiv)](http://arxiv.org/abs/2412.08050v2)** | 2024-12-13 | <details><summary>Show</summary><p>如果未对齐的多模态医学图像可以在统一的处理框架内使用单阶段方法同时对齐和融合，不仅可以实现双重任务的相互促进，还有助于降低模型的复杂性。然而，该模型的设计面临着特征融合和对齐要求不兼容的挑战;具体来说，特征对齐需要相应特征之间的一致性，而特征融合需要特征之间相互补充。为了应对这一挑战，本文提出了一种称为双向逐步特征对齐和融合 （BSFA-F） 策略的未对齐医学图像融合方法。为了减少模态差异对跨模态特征匹配的负面影响，我们将模态无差异特征表示 （MDF-FR） 方法纳入 BSFA-F 中。MDF-FR 利用模态特征表示头 （MFRH） 来集成输入图像的全局信息。通过将当前图像的 MFRH 中包含的信息注入到其他模态图像中，在保留不同图像携带的互补信息的同时，有效地减少了模态差异对特征对齐的影响。在特征对齐方面，BSFA-F 采用基于两点之间矢量位移路径独立性的双向逐步对齐变形场预测策略。该策略解决了单步对齐中大跨度和变形场预测不准确的问题。最后，Multi-Modal Feature Fusion 模块实现了对齐特征的融合。多个数据集的实验结果证明了我们方法的有效性。源代码可在 https://github.com/slrl123/BSAFusion 上获得。</p></details> | AAAI2025 | [Code Link](https://github.com/slrl123/BSAFusion) | Huafeng Li |
| **[用于多模态医学图像融合的边缘增强扩张残差注意力网络(Arxiv)](http://arxiv.org/abs/2411.11799v1)** | 2024-11-18 | <details><summary>Show</summary><p>多模态医学影像融合是一项关键任务，它将来自不同成像模态的互补信息组合成一个统一的表示，从而提高诊断准确性和治疗计划。虽然深度学习方法，特别是卷积神经网络 （CNN） 和 Transformers，具有显著提高的融合性能，但一些现有的基于 CNN 的方法在捕获细粒度多尺度和边缘特征方面存在不足，导致特征集成次优。另一方面，基于 Transformer 的模型在训练和融合阶段都是计算密集型的，因此对于实时临床使用来说是不切实际的。此外，融合图像的临床应用仍有待探索。在本文中，我们提出了一种新颖的基于 CNN 的架构，通过引入扩张残差注意力网络模块来解决这些限制，以实现有效的多尺度特征提取，并结合梯度运算符来增强边缘细节学习。为了确保快速高效的融合，我们提出了一种基于 softmax 的加权核范数的无参数融合策略，该策略在训练或推理过程中不需要额外的计算。广泛的实验，包括下游脑肿瘤分类任务，表明我们的方法在视觉质量、纹理保留和融合速度方面优于各种基线方法，使其成为实际临床应用的可能实用解决方案。该代码将于 https://github.com/simonZhou86/en_dran 发布。</p></details> | BIBM2024 | [Code Link](https://github.com/simonZhou86/en_dran) |
| **[动态亮度适应，实现稳健的多模态图像融合(Arxiv)](http://arxiv.org/abs/2411.04697v1)** | 2024-11-07 | <details><summary>Show</summary><p>红外和可见光图像融合旨在整合模态优势，以获得视觉增强、信息丰富的图像。在真实场景中，可见光成像容易受到动态环境亮度波动的影响，从而导致纹理退化。现有的融合方法缺乏对这种亮度扰动的鲁棒性，从而严重影响了融合影像的视觉保真度。为了应对这一挑战，我们提出了亮度自适应多模态动态融合框架 （BA-Fusion），该框架在动态亮度波动的情况下实现了稳健的图像融合。具体来说，我们引入了亮度自适应门 （BAG） 模块，该模块旨在从与亮度相关的通道中动态选择特征进行归一化，同时在源图像中保留与亮度无关的结构信息。此外，我们提出了一个亮度一致性损失函数来优化 BAG 模块。整个框架通过交替训练策略进行调整。广泛的实验验证了我们的方法在保留多模态图像信息和视觉保真度方面超越了最先进的方法，同时在不同亮度水平下表现出非凡的稳健性。我们的代码可用：https://github.com/SunYM2020/BA-Fusion。</p></details> | IJCAI2024 | [Code Link](https://github.com/SunYM2020/BA-Fusion) |
| **[Conditional Controllable Image Fusion(Arxiv)](http://arxiv.org/abs/2411.01573v1)** | 2024-11-03 | <details><summary>Show</summary><p>图像融合旨在整合通过各种来源获取的多个输入图像的互补信息，以合成新的融合图像。现有方法通常采用针对特定场景量身定制的不同约束设计，形成固定的融合范式。然而，这种数据驱动的融合方法在不同场景中部署具有挑战性，尤其是在快速变化的环境中。为了解决这个问题，我们提出了一种条件可控融合 （CCF） 框架，用于无需专门训练的通用图像融合任务。由于不同样品的动态差异，我们的 CCF 在实践中对每个样品都采用了特定的融合约束。鉴于去噪扩散模型的强大生成能力，我们首先将特定约束作为自适应融合条件注入预训练的 DDPM 中。动态选择适当的条件，以确保熔融过程在每个反向扩散阶段保持对特定要求的响应。因此，CCF 可以逐步有条件地校准融合图像。广泛的实验验证了我们在不同场景中与竞争方法相比的一般融合任务的有效性，而无需额外培训。</p></details> | NeurIPS2024 | [Code Link](https://github.com/jehovahxu/CCF) |
| **[Text-DiFuse： 一种基于文本调制扩散模型的交互式多模态图像融合框架(Arxiv)](http://arxiv.org/abs/2410.23905v1)** | 2024-10-31 | <details><summary>Show</summary><p>现有的多模态图像融合方法无法解决源图像中出现的复合退化问题，导致融合图像受到噪声、颜色偏差、曝光不当、\textit{etc} 的困扰。此外，这些方法通常忽略了前景对象的特异性，从而削弱了融合图像中感兴趣对象的显著性。为了应对这些挑战，本研究提出了一种基于文本调制扩散模型的新型交互式多模态图像融合框架，称为 Text-DiFuse。首先，该框架将特征级信息集成到扩散过程中，允许自适应退化去除和多模态信息融合。这是首次尝试将信息融合深入、明确地嵌入扩散过程中，有效解决了图像融合中的化合物降解问题。其次，通过将文本和零镜头位置模型的组合嵌入到扩散融合过程中，开发了一种文本控制的融合再调制策略。这样，用户就可以自定义文本控件，以提高融合性能并突出显示融合图像中的前景对象。在各种公共数据集上进行的广泛实验表明，我们的 Text-DiFuse 在各种复杂降级的场景中实现了最先进的融合性能。此外，语义分割实验验证了我们的文本控制融合再调制策略在语义性能上实现的显着增强。该代码在 https://github.com/Leiii-Cao/Text-DiFuse 上公开提供。</p></details> | NeurIPS2024 | [Code Link](https://github.com/Leiii-Cao/Text-DiFuse) | Jiayi Ma |
| **[SFDFusion：用于红外和可见光图像融合的高效空间频域融合网络(Arxiv)](http://arxiv.org/abs/2410.22837v1)** | 2024-10-30 | <details><summary>Show</summary><p>红外和可见光图像融合旨在利用两种模态的互补信息来生成具有突出目标和丰富纹理细节的融合图像。大多数现有算法仅从空间域中的不同模态执行像素级或特征级融合。它们通常忽略了频域中的信息，其中一些由于结构过于复杂而效率低下。为了应对这些挑战，本文提出了一种用于红外和可见光图像融合的高效空间频域融合 （SFDFusion） 网络。首先，我们提出了一个双模态细化模块 （DMRM） 来提取互补信息。该模块从空间域中的红外和可见光模态中提取有用的信息，并增强细粒度的空间细节。接下来，为了引入频域信息，我们构建了一个频域融合模块 （FDFM），该模块通过快速傅里叶变换 （FFT） 将空间域转换为频域，然后对频域信息进行积分。此外，我们设计了一个频域熔融损耗，为熔融过程提供指导。在公共数据集上的广泛实验表明，我们的方法生成的融合图像在各种融合指标和视觉效果方面具有显着优势。此外，我们的方法在图像融合方面表现出高效率和在下游检测任务中的良好性能，从而满足高级视觉任务的实时性需求。</p></details> | ECAI2024 | [Code Link](https://github.com/lqz2/SFDFusion) |
| **[TaGAT： 用于多模态视网膜图像融合的拓扑感知图注意力网络(Arxiv)](http://arxiv.org/abs/2407.14188v1)** | 2024-07-19 | <details><summary>Show</summary><p>在医学图像融合领域，整合来自各种模式的信息对于改进诊断和治疗计划至关重要，尤其是在视网膜健康方面，其中重要特征在不同的成像模式中表现出不同。现有的基于深度学习的方法没有充分关注视网膜图像融合，因此无法在视网膜图像融合中保留足够的解剖结构和精细血管细节。为了解决这个问题，我们提出了用于多模态视网膜图像融合的拓扑感知图注意力网络 （TaGAT），利用带有图注意力网络 （GAT） 的新型拓扑感知编码器 （TAE） 来有效增强视网膜脉管系统跨模态图拓扑的空间特征。TAE 将通过长短距离 （LSR） 编码器从视网膜图像中提取的基本和细节特征编码为从视网膜血管中提取的图形。在 TAE 中，基于 GAT 的图形信息更新 （GIU） 模块动态优化和聚合节点特征，以生成拓扑感知图形特征。具有基础和细节特征的更新图形特征将合并并解码为融合图像。我们的模型在荧光素眼底血管造影 （FFA） 与彩色眼底 （CF） 和光学相干断层扫描 （OCT） 与共聚焦显微镜视网膜图像融合中优于最先进的方法。源代码可以通过 https://github.com/xintian-99/TaGAT 访问。 | MICCAI 2024 | [Code Link](https://github.com/xintian-99/TaGAT) |
| **[D2-LRR：一种用于医学图像融合的双分解 MDLatLRR 方法(Arxiv)](http://arxiv.org/abs/2206.15179v4)** | 2024-07-07 | <details><summary>Show</summary><p>在图像融合任务中，理想的图像分解方法可以带来更好的性能。MDLatLRR 在这方面做得很好，但仍存在一些改进的空间。考虑到 MDLatLRR 仅关注通过潜在低秩表示 （LatLRR） 从输入图像中提取的细节部分 （显著特征），LatLRR 提取的基本部分 （主要特征） 没有得到充分利用。因此，我们引入了一种增强的多级分解方法，称为双分解 MDLatLRR （D2-LRR），它有效地分析和利用了通过 LatLRR 提取的所有图像特征。具体来说，将彩色图像转换为 YUV 色彩空间和灰度图像，并将 Y 通道和灰度图像输入到 LatLRR 的训练参数中，得到包含四轮分解的详细部分和基本部分。随后，使用 average 策略融合 basic 部分，而 detail 部分使用 kernel norm作融合。融合的图像最终会转换回 RGB 图像，从而产生最终的融合输出。我们将 D2-LRR 应用于医学图像融合任务。细节部分采用核范作进行熔合，而基本部分使用平均策略进行熔合。现有方法之间的比较分析表明，我们提出的方法在客观和主观评估中都达到了尖端的融合性能。</p></details> | MVIPIT2023 | [Code Link](https://github.com/songxujay/MDLatLRRv2) | Hui Li |
| **[通过视觉语言模型进行图像融合(Arxiv)](http://arxiv.org/abs/2402.02235v2)** | 2024-07-10 | <details><summary>Show</summary><p>图像融合将来自多个图像的基本信息集成到单个合成中，从而增强结构、纹理并优化缺陷。现有方法主要关注用于识别的像素级和语义视觉特征，但往往忽略了视觉之外更深层次的文本级语义信息。因此，我们首次引入了一种名为 image Fusion via vIsion-Language Model （FILM） 的新型融合范式，利用源图像中的显式文本信息来指导融合过程。具体来说，FILM 从图像中生成语义提示并将其输入到 ChatGPT 中以进行全面的文本描述。这些描述融合在文本域内，并指导视觉信息融合，增强特征提取和上下文理解，由文本语义信息通过交叉注意指导。FILM 在红外-可见光、医疗、多重曝光和多焦点图像融合四个图像融合任务中显示出可喜的成果。我们还提出了一个视觉语言数据集，其中包含 ChatGPT 为四个融合任务中的八个图像融合数据集生成的段落描述，以促进未来基于视觉语言模型的图像融合的研究。代码和数据集可在 https://github.com/Zhaozixiang1228/IF-FILM 获取。</p></details> | ICML2024 | [Code Link](https://github.com/Zhaozixiang1228/IF-FILM) | Zixiang Zhao |
| **[SimpleFusion：用于红外和可见光图像的简单融合框架(Arxiv)](http://arxiv.org/abs/2406.19055v1)** | 2024-06-27 | <details><summary>Show</summary><p>将可见光和红外图像集成到一个高质量图像中，也称为可见光和红外图像融合，对于许多下游视觉任务来说，这是一项具有挑战性但至关重要的任务。大多数现有工作都使用预先训练的深度神经网络或为这项任务设计具有强大先验的复杂框架，这可能不合适或缺乏灵活性。本文介绍了 SimpleFusion，这是一种简单而有效的可见光和红外图像融合框架。我们的框架遵循分解和融合范式，其中可见光和红外图像通过 Retinex 理论分解为反射和照明分量，然后是这些相应元件的融合。整个框架设计了两个无降采样的普通卷积神经网络，可以高效地进行图像分解和融合。此外，我们引入了分解损失和细节到语义的损失，以保留两种融合模式之间的互补信息。我们对具有挑战性的基准进行了广泛的实验，验证了我们的方法优于以前的最先进的方法。</p></details> | PRCV2024 | [Code Link](https://github.com/hxwxss/SimpleFusion-A-Simple-Fusion-Framework-for-Infrared-and-Visible-Images) |
| **[CoMoFusion：红外和可见光图像与一致性模型的快速高质量融合(Arxiv)](http://arxiv.org/abs/2405.20764v3)** | 2024-06-12 | <details><summary>Show</summary><p>生成模型被广泛用于模拟红外和可见光图像融合领域中融合图像的分布。然而，当前基于生成模型的融合方法经常受到训练不稳定和推理速度慢的问题。针对这一问题，该文提出一种基于一致性模型的新型融合方法，称为CoMoFusion，该方法能够生成高质量的图像，并实现较快的图像推理速度。具体来说，一致性模型用于通过正向和反向过程在潜在空间中构建多模态联合特征。然后，将训练后的一致性模型提取的红外和可见光特征馈送到融合模块中，生成最终的融合图像。为了增强融合图像的纹理和显著信息，该文还设计了一种基于像素值选择的新型损失。在公共数据集上的大量实验表明，与现有的融合方法相比，我们的方法获得了 SOTA 融合性能。</p></details> | PRCV2024 | [Code Link](https://github.com/ZhimingMeng/CoMoFusion) | Hui Li |
| **[FusionINN：用于脑肿瘤监测的可分解图像融合(Arxiv)](http://arxiv.org/abs/2403.15769v3)** | 2024-06-10 | <details><summary>Show</summary><p>图像融合通常采用不可逆神经网络将多个源图像合并为单个融合图像。然而，对于临床专家来说，仅仅依靠融合图像可能不足以做出诊断决策，因为融合机制混合了源图像中的特征，因此难以解释潜在的肿瘤病理。我们介绍了 FusionINN，这是一种新颖的可分解图像融合框架，能够有效地生成融合图像并将其分解回源图像。FusionINN 设计为双射，在融合图像旁边包含潜在图像，同时确保从源图像到潜在表示的信息传输最少。据我们所知，我们是第一个研究融合图像可分解性的公司，与多焦点或多曝光图像融合等其他任务相比，这对于医学图像融合等生命敏感应用尤为重要。我们广泛的实验在主观和客观上验证了 FusionINN 优于现有的判别和生成融合方法。此外，与最近的基于降噪扩散的融合模型相比，我们的方法提供了更快、质量更好的融合结果。</p></details> | IJCAIW2024 | [Code Link](https://github.com/nish03/FusionINN) |
| **[脉冲耦合神经网络在计算机视觉和图像处理中的应用综述(Arxiv)](http://arxiv.org/abs/2406.00239v1)** | 2024-06-01 | <details><summary>Show</summary><p>受哺乳动物视觉皮层启发的神经模型研究导致了许多脉冲神经网络，例如脉冲耦合神经网络 （PCNN）。这些模型是振荡的时空模型，通过图像进行刺激，以产生几种基于时间的响应。本文回顾了 PCNN 的最新技术，涵盖了其数学公式、变体和文献中的其他简化。我们介绍了 PCNN 架构成功解决一些基本图像处理和计算机视觉挑战的几种应用，包括图像分割、边缘检测、医学成像、图像融合、图像压缩、对象识别和遥感。在这些应用中取得的结果表明，PCNN 架构生成了与各种计算机视觉任务相关的有用感知信息。</p></details> | IPCV 2021 | None |
| **[用于多光谱和高光谱图像融合的傅里叶增强隐式神经融合网络(Arxiv)](http://arxiv.org/abs/2404.15174v1)** | 2024-04-23 | <details><summary>Show</summary><p>最近，内隐神经表示 （INR） 在各种视觉相关领域取得了重大进展，为多光谱和高光谱图像融合 （MHIF） 任务提供了一种新颖的解决方案。然而，INR 容易丢失高频信息，并且局限于缺乏全局感知能力。为了解决这些问题，本文介绍了一种专为 MHIF 任务设计的傅里叶增强隐式神经融合网络 （FeINFN），针对以下现象：HR-HSI 潜在代码和 LR-HSI 的傅里叶振幅非常相似;然而，它们的阶段表现出不同的模式。在 FeINFN 中，我们创新性地提出了一种空间和频率隐式融合函数 （Spa-Fre IFF），帮助 INR 捕获高频信息并扩大感受野。此外，发明了一种采用复杂 Gabor 小波激活函数的新型解码器，称为空间频率交互式解码器 （SFID），以增强 INR 特征的交互。特别是，我们进一步从理论上证明 Gabor 小波激活具有时频紧密性，有利于在解码器中学习最佳带宽。在两个基准 MHIF 数据集上的实验在视觉和定量上验证了所提出的方法的最新 （SOTA） 性能。此外，消融研究证明了上述贡献。</p></details> | NeurPS2024 | [Code Link](https://github.com/294coder/Efficient-MIF/tree/main-release) | Xiao Wu |
| **[EMMA：等变多模态图像融合(Arxiv)](http://arxiv.org/abs/2305.11443v2)** | 2024-04-15 | <details><summary>Show</summary><p>多模态图像融合是一种将来自不同传感器或模态的信息组合在一起的技术，使融合图像能够保留来自每种模态的互补特征，例如功能亮点和纹理细节。然而，由于地面实况融合数据的稀缺性，这种融合模型的有效训练具有挑战性。为了解决这个问题，我们提出了用于端到端自我监督学习的等变多模态 imAge 融合 （EMMA） 范式。我们的方法植根于自然成像响应对某些转换是等变的先验知识。因此，我们引入了一种新的训练范式，其中包括一个融合模块、一个伪传感模块和一个等变融合模块。这些组件使网络训练能够遵循自然传感成像过程的原理，同时满足等变成像先验。大量实验证实，EMMA 可产生高质量的红外可见光和医学图像融合结果，同时促进下游多模态分割和检测任务。</p></details> | CVPR2024 | [Code Link](https://github.com/Zhaozixiang1228/MMIF-EMMA) | Zixiang Zhao |
| **[Text-IF：利用语义文本指南进行降级感知和交互式图像融合(Arxiv)](http://arxiv.org/abs/2403.16387v1)** | 2024-03-25 | <details><summary>Show</summary><p>Image fusion 旨在将来自不同源图像的信息组合在一起，以创建具有全面代表性的图像。现有的融合方法通常无法处理低质量源图像的退化，并且无法满足多个主观和客观需求。为了解决这些问题，我们引入了一种新方法，该方法利用语义文本引导图像融合模型进行降级感知和交互式图像融合任务，称为 Text-IF。它创新性地将经典图像融合扩展到文本引导图像融合，并能够和谐地解决融合过程中的退化和交互问题。通过文本语义编码器和语义交互融合解码器，Text-IF 可用于一体化的红外和可见光图像退化感知处理和交互式柔性融合结果。通过这种方式，Text-IF 不仅实现了多模态图像融合，而且实现了多模态信息融合。大量实验证明，所提出的文本引导图像融合策略在图像融合性能和退化处理方面优于SOTA方法。该代码可在 https://github.com/XunpengYi/Text-IF 获取。</p></details> | CVPR2024 | [Code Link](https://github.com/XunpengYi/Text-IF) | Jiayi Ma |
| **[用于通用图像融合的任务自定义适配器混合(Arxiv)](http://arxiv.org/abs/2403.12494v2)** | 2024-03-24 | <details><summary>Show</summary><p>通用图像融合旨在整合来自多源图像的重要信息。但是，由于跨任务差距很大，相应的融合机制在实践中差异很大，导致跨子任务的性能有限。为了解决这个问题，我们提出了一种新的任务定制适配器混合 （TC-MoA） 用于通用图像融合，在统一模型中自适应地提示各种融合任务。我们从专家混合 （MoE） 中借鉴了见解，将专家视为有效的调优适配器，以提示预先训练的基础模型。这些适配器在不同的任务之间共享，并受到互信息正则化的约束，确保与不同任务的兼容性，同时为多源图像提供互补性。特定于任务的路由网络自定义这些适配器，以从具有动态主导强度的不同来源提取特定于任务的信息，执行自适应视觉特征提示融合。值得注意的是，我们的 TC-MoA 控制了不同聚变任务的主导强度偏差，成功地将多个聚变任务统一到一个模型中。大量实验表明，TC-MoA 在学习共性方面优于竞争方法，同时保持了与一般图像融合（多模态、多曝光和多焦点）的兼容性，并且在更多的泛化实验中也表现出了惊人的可控性。该代码可在 https://github.com/YangSun22/TC-MoA 获取。</p></details> | CVPR2024 | [Code Link](https://github.com/YangSun22/TC-MoA) |
| **[SAMF：用于目标检测的小区域感知多焦点图像融合(Arxiv)](http://arxiv.org/abs/2401.08357v2)** | 2024-01-31 | <details><summary>Show</summary><p>现有的多焦点图像融合 （MFIF） 方法通常无法保留不确定的过渡区域并准确检测大型散焦区域内的小焦点区域。针对这一问题，该文提出一种新的小区域感知MFIF算法，用于增强目标检测能力。首先，我们增强了小焦点和边界区域内的像素属性，随后将其与视觉显著性检测相结合，以获得用于区分聚焦像素分布的预融合结果。为了准确确保像素聚焦，我们将源图像视为聚焦、散焦和不确定区域的组合，并提出了一种三区域分割策略。最后，我们设计了一个有效的像素选择规则来生成分割决策图并获得最终的融合结果。实验表明，所提出的方法能够准确检测小而平滑的焦点区域，同时提高目标检测性能，在主观和客观评估方面都优于现有方法。源代码可在 https://github.com/ixilai/SAMF 获取。</p></details> | ICASSP2024 | [Code Link](https://github.com/ixilai/SAMF) |  Xiaosong Li |
| **[用于图像融合的互导动态网络(Arxiv)](http://arxiv.org/abs/2308.12538v2)** | 2023-09-01 | <details><summary>Show</summary><p>图像融合旨在从在不同条件下捕获的多张图像中生成高质量的图像。此任务的关键问题是保留互补信息，同时过滤掉与融合结果无关的信息。然而，现有方法通过利用静态卷积神经网络 （CNN） 来解决这个问题，在特征提取过程中存在两个固有的限制，即无法处理空间变体内容和缺乏来自多个输入的指导。在本文中，我们提出了一种用于图像融合的新型互导动态网络 （MGDN），它允许在不同位置和输入之间有效利用信息。具体来说，我们设计了一个用于自适应特征提取的互导动态滤波器 （MGDF），由互导交叉注意力 （MGCA） 模块和动态滤波器预测器组成，其中前者包含来自不同输入的额外指导，后者为不同位置生成空间变体内核。此外，我们引入了并行特征融合 （PFF） 模块，以有效地融合提取特征的局部和全局信息。为了进一步减少提取特征之间的冗余，同时保留它们的共享结构信息，我们设计了一种新的损失函数，它将归一化互信息 （NMI） 的最小化与估计的梯度掩码相结合。在 5 个基准数据集上的实验结果表明，我们提出的方法在 4 个图像融合任务上优于现有方法。代码和模型可在以下网址公开获得：https://github.com/Guanys-dar/MGDN。</p></details> | ACMMM2023 | [Code Link](https://github.com/Guanys-dar/MGDN) |
| **[DDFM：用于多模态图像融合的去噪扩散模型(Arxiv)](http://arxiv.org/abs/2303.06840v2)** | 2023-08-22 | <details><summary>Show</summary><p>多模态图像融合旨在组合不同的模态以生成融合图像，这些图像保留了每种模态的互补特征，例如功能亮点和纹理细节。为了利用强大的生成先验并解决基于 GAN 的生成方法的训练不稳定和缺乏可解释性等挑战，我们提出了一种基于去噪扩散概率模型 （DDPM） 的新型融合算法。在 DDPM 采样框架下，将融合任务表述为条件生成问题，进一步分为无条件生成子问题和最大似然子问题。后者以具有潜在变量的分层贝叶斯方式建模，并通过期望最大化 （EM） 算法进行推断。通过将推理解决方案集成到扩散采样迭代中，我们的方法可以生成具有自然图像生成先验和源图像跨模态信息的高质量融合图像。请注意，我们只需要一个无条件的预训练生成模型，不需要微调。我们大量的实验表明，我们的方法在红外-可见光图像融合和医学图像融合方面产生了有希望的融合结果。代码位于 \url{https://github.com/Zhaozixiang1228/MMIF-DDFM} 。</p></details> | ICCV2023 | [Code Link](https://github.com/Zhaozixiang1228/MMIF-DDFM) | Zixiang Zhao, 效果复现不出来 |
| **[一种具有局部增强和状态共享的新型状态空间模型，用于图像融合(Arxiv)](http://arxiv.org/abs/2404.09293v2)** | 2024-08-21 | <details><summary>Show</summary><p>在图像融合任务中，来自不同来源的图像具有不同的特征。这推动了许多方法的发展，以探索在保留其各自特性的同时融合它们的更好方法。Mamba 作为一种状态空间模型，在自然语言处理领域崭露头角。最近，许多研究试图将 Mamba 扩展到视觉任务。然而，由于图像的性质不同于因果语言序列，Mamba 的有限状态容量削弱了其对图像信息进行建模的能力。此外，Mamba 的序列建模能力仅能够处理空间信息，无法有效捕捉图像中丰富的光谱信息。在这些挑战的推动下，我们定制并改进了专为图像融合任务设计的 Vision Mamba 网络。具体来说，我们提出了局部增强视觉 Mamba 块，称为 LEVM。LEVM 块可以提高对网络的本地信息感知，并同时学习本地和全局空间信息。此外，我们提出了状态共享技术来增强空间细节并整合空间和光谱信息。最后，整个网络是一个基于 vision Mamba 的多尺度结构，称为 LE-Mamba。大量实验表明，所提出的方法在多光谱全色锐化和多光谱和高光谱图像融合数据集上取得了最先进的结果，并证明了所提出的方法的有效性。代码可以在 \url{https://github.com/294coder/Efficient-MIF} 中访问。</p></details> | ACMMM2024 | [Code Link](https://github.com/294coder/Efficient-MIF) | Xiao Wu |
| **[PAIF：用于攻击容忍语义分割的感知感知红外-可见光图像融合(Arxiv)](http://arxiv.org/abs/2308.03979v1)** | 2023-08-08 | <details><summary>Show</summary><p>红外和可见光图像融合是一种强大的技术，它结合了来自不同模态的互补信息，用于下游语义感知任务。现有的基于学习的方法显示出卓越的性能，但受到对抗性攻击的固有脆弱性的影响，导致准确性显着下降。在这项工作中，提出了一个感知感知融合框架，以促进对抗场景中的分割鲁棒性。我们首先对图像融合的组成部分进行系统分析，研究对抗性扰动下与分割稳健性的相关性。基于这些分析，我们提出了一种具有基于分解的结构的协调架构搜索，以平衡标准准确性和稳健性。我们还提出了一种自适应学习策略来提高图像融合的参数鲁棒性，它可以在各种对抗性扰动下学习有效的特征提取。因此，可以从架构和学习策略的角度实现图像融合的目标 （\textit{i.e.，} 从源模态中提取互补特征并防御攻击）。广泛的实验结果表明，我们的方案大大增强了鲁棒性，与高级竞争对手相比，在对抗场景中分割的 mIOU 提高了 15.3%。源代码可在 https://github.com/LiuZhu-CV/PAIF 上获得。</p></details> | ACMMM2023 | [Code Link](https://github.com/LiuZhu-CV/PAIF) | Jinyuan Liu |
| **[学习具有跨模态交互的图神经网络以进行图像融合(Arxiv)](http://arxiv.org/abs/2308.03256v1)** | 2023-08-07 | <details><summary>Show</summary><p>红外和可见光图像融合已逐渐被证明是多模态成像技术领域的一个重要分支。在最近的发展中，研究人员不仅关注融合图像的质量，还评估它们在下游任务中的性能。然而，大多数方法很少关注不同模态的相互学习，导致融合图像缺乏重要的细节和纹理。为了克服这个问题，我们提出了一种基于交互式图形神经网络 （GNN） 的跨模态融合架构，称为 IGNet。具体来说，我们首先应用一个多尺度提取器来实现浅层特征，这些特征被用作构建图形结构的必要输入。然后，图交互模块可以将提取的红外/可见分支的中间特征构造成图结构。同时，两个分支的图结构交互进行跨模态和语义学习，使融合图像能够保持重要的特征表达，增强下游任务的性能。此外，所提出的领导节点可以改善相同模态下的信息传播。最后，我们合并所有图特征得到融合结果。在不同数据集（TNO、MFNet 和 M3FD）上进行的广泛实验表明，我们的 IGNet 可以生成视觉上吸引人的融合图像，同时在检测和分割方面的得分平均高出 2.59% mAP@.5% 和 7.77% mIoU，比最先进的方法高出。拟议的 IGNet 的源代码可在 https://github.com/lok-18/IGNet 上获得。| ACMMM2023 | [Code Link](https://github.com/lok-18/IGNet) | Jinyuan Liu |
| **[SegMiF: 用于图像融合和分割的多交互式特征学习和全职多模态基准测试(Arxiv)](http://arxiv.org/abs/2308.02097v1)** | 2023-08-04 | <details><summary>Show</summary><p>多模态图像融合和分割在自动驾驶和机器人作中起着至关重要的作用。早期的工作只集中在提高一项任务的性能，\emph{eg.，} 融合或分割，这使得它很难达到~'两全其美'。为了克服这个问题，在本文中，我们提出了一种用于图像融合和 Segmentation 的 Multi-interactive Feature 学习架构，即 SegMiF，并利用双任务相关性来促进这两个任务的性能。SegMiF 采用级联结构，包含一个融合子网络和一个常用的分割子网络。通过巧妙地桥接两个组件之间的中间特征，从分割任务中学到的知识可以有效地协助融合任务。此外，受益的融合网络支持分割网络执行得更自命不凡。此外，建立了分层交互式注意力块，以确保两个任务之间所有重要信息的细粒度映射，从而使模态/语义特征完全互交互。此外，还引入了动态权重因子，可以自动调整每个任务的相应权重，可以平衡交互特征的对应关系，突破费力调优的限制。此外，我们构建了一个智能多波双目成像系统，并收集了一个全职多模态基准，具有 15 个带注释的像素级类别，用于图像融合和分割。对几个公共数据集和我们的基准测试进行的广泛实验表明，所提出的方法输出视觉上吸引人的融合图像，并且性能一般在现实世界中，分割 mIoU 高于最先进的方法。</p></details> | ICCV2023 | [Code Link](https://github.com/JinyuanLiu-CV/SegMiF) | Jinyuan Liu |
| **[LLVIP：用于弱光视觉的可见光-红外配对数据集(Arxiv)](http://arxiv.org/abs/2108.10831v4)** | 2023-06-14 | <details><summary>Show</summary><p>由于失去了有效的目标区域，对于图像融合、行人检测和弱光条件下的图像到图像转换等各种视觉任务来说，这是非常具有挑战性的。在这种情况下，红外和可见光图像可以一起使用，以提供丰富的细节信息和有效的目标区域。在本文中，我们提出了 LLVIP，这是一个用于弱光视觉的可见光-红外配对数据集。该数据集包含 30976 张图像，即 15488 对，其中大部分是在非常黑暗的场景中拍摄的，并且所有图像在时间和空间上都严格对齐。数据集中的行人将被标记。我们将该数据集与其他可见光-红外数据集进行比较，并评估了一些流行的视觉算法的性能，包括数据集上的图像融合、行人检测和图像到图像的转换。实验结果证明了融合对图像信息的互补作用，并发现了在极低光照条件下三种视觉任务的现有算法的不足。我们相信 LLVIP 数据集将通过在极低光应用中促进图像融合、行人检测和图像到图像的转换，为计算机视觉社区做出贡献。该数据集将于 https://bupt-ai-cz.github.io/LLVIP 年发布。还提供原始数据用于进一步研究，例如图像配准。</p></details> | ICCVW | [Code Link](https://bupt-ai-cz.github.io/LLVIP) |
| **[双向动态学习，实现联合多模态图像融合等(Arxiv)](http://arxiv.org/abs/2305.06720v1)** | 2023-05-11 | <details><summary>Show</summary><p>近年来，图像融合、场景理解等多模态场景感知任务引起了人们对智能视觉系统的广泛关注。然而，早期的努力总是考虑单方面提升一个任务，而忽略其他任务，很少调查它们的潜在联系以共同推动。为了克服这些限制，我们建立了分层双任务驱动的深度模型来桥接这些任务。具体来说，我们首先构建了一个图像融合模块，以融合互补特征并级联双任务相关模块，包括用于视觉效果的判别器和用于特征测量的语义网络。我们提供双层次视角来制定图像融合和后续下游任务。为了在图像融合中纳入不同的任务相关响应，我们将图像融合视为主要目标，将双模块视为可学习的约束。此外，我们开发了一种有效的一阶近似来计算相应的梯度，并提出动态加权聚合来平衡梯度以进行融合学习。广泛的实验证明了我们方法的优越性，与最先进的方法相比，该方法不仅产生了视觉上令人愉悦的融合结果，而且实现了对检测和分割的显著促进。</p></details> | IJCAI2023 | [Code Link](https://github.com/LiuZhu-CV/BDLFusion) | Jinyuan Liu |
| **[从全色和多光谱图像融合的分层角度重新审视空间频率信息集成](https://openaccess.thecvf.com/content/CVPR2024/html/Tan_Revisiting_Spatial-Frequency_Information_Integration_from_a_Hierarchical_Perspective_for_Panchromatic_CVPR_2024_paper.html)** | 2024 | <details><summary>Show</summary><p> 全色锐化是一个超分辨率问题，它本质上依赖于全色 (PAN) 图像和低分辨率多光谱 (LRMS) 图像的光谱融合。以往的方法验证了信息融合在整个图像的傅里叶空间中的有效性。然而，他们没有充分探索 PAN 和 LRMS 图像之间不同层次结构下的傅立叶关系。为此，我们提出了一种分层频率集成网络(HFIN)，以促进泛锐化的分层傅立叶信息集成。具体来说，我们的网络由两个设计组成：信息分层和信息集成。对于信息分层，我们将 PAN 和 LRMS 信息分层分解为空间、全局傅里叶和局部傅里叶信息，并独立融合它们。对于信息集成，处理上述分层融合信息，进一步增强它们之间的关系并进行综合集成。我们的方法扩展了一个新的空间来探索PAN和LRMS图像之间的关系，增强了空间频率信息的整合。大量实验表明，该网络的有效性，与其他最先进的方法相比，在真实场景和其他融合任务中的泛化作为通用图像融合框架，展示了其优越的性能。 </p></details> | CVPR2024 |  [Code Link](https://github.com/JosephTiTan/HFIN) | Feng Zhao |
| **[用于遥感全色锐化的内容自适应非局部卷积](https://openaccess.thecvf.com/content/CVPR2024/html/Tan_Revisiting_Spatial-Frequency_Information_Integration_from_a_Hierarchical_Perspective_for_Panchromatic_CVPR_2024_paper.html)** | 2024 | <details><summary>Show</summary><p> 目前，基于机器学习的遥感全色锐化方法发展迅速。然而，现有的pansharpening方法通常没有充分利用区分非局部空间中的区域信息，从而限制了方法的有效性，导致学习参数冗余。在本文中，我们介绍了一种所谓的内容自适应非局部卷积（CANConv），这是一种为遥感图像全色锐化量身定制的新方法。具体来说，CANConv 采用自适应卷积，确保空间适应性，并通过相似关系分区 (SRP) 和分区自适应卷积 (PWAC) 子模块合并非局部自相似性。此外，我们还提出了一个相应的网络架构，称为CANNet，它主要利用多尺度自相似性。大量实验表明，与最近的有前途的融合方法相比，CANConv 的优越性能。此外，我们通过可视化、消融实验以及与多个测试集上现有方法的比较来证实该方法的有效性。 </p></details> | CVPR2024 |  [Code Link](https://github.com/duanyll/CANConv) | Xiao Wu |
| **[MetaFusion：基于目标检测的元特征嵌入的红外和可见光图像融合](https://paperswithcode.com/paper/metafusion-infrared-and-visible-image-fusion)** | 2023 | <details><summary>Show</summary><p> 融合红外和可见光图像可以为后续的目标检测任务提供更多的纹理细节。相反，检测任务提供对象语义信息来改进红外和可见光图像的融合。因此，使用相互促进的联合融合和检测学习越来越受到关注。然而，这两个不同级别任务之间的特征差距阻碍了进展。针对这一问题，本文提出了一种基于目标检测元特征嵌入的红外和可见光图像融合。核心思想是元特征嵌入模型旨在根据融合网络能力生成对象语义特征，因此语义特征自然与融合特征兼容。它通过模拟元学习进行了优化。此外，我们进一步实现了融合和检测任务之间的相互促进学习，以提高其性能。在三个公共数据集上的综合实验证明了我们方法的有效性。 </p></details> | CVPR2023 |  [Code Link](https://github.com/wdzhao123/MetaFusion) | Feng Zhao |
| **[SHIP: 探测红外与可见光图像融合的协同高阶相互作用](https://openaccess.thecvf.com/content/CVPR2024/html/Zheng_Probing_Synergistic_High-Order_Interaction_in_Infrared_and_Visible_Image_Fusion_CVPR_2024_paper.html)** | 2024 | <details><summary>Show</summary><p> 红外和可见光图像融合旨在通过整合和区分互补信息从多个来源来生成融合图像。虽然具有全局空间交互的交叉注意力机制看起来很有希望，但它只捕获二阶空间交互，而忽略了空间和通道维度上的高阶交互。这种限制阻碍了多模态之间协同作用的利用。为了弥补这一差距，我们引入了一种协同高阶交互范式(SHIP)，旨在系统地研究红外图像和可见光图像在两个基本维度上的空间细粒度和全局统计协作:1)空间维度:我们通过元素乘法构建空间细粒度交互，在数学上等价于全局交互，然后通过迭代聚合和进化互补信息来促进高阶格式，提高效率和灵活性;2)通道维度:在与一阶统计量(均值)的通道交互上扩展，我们设计了高阶通道交互，以促进基于全局统计的源图像之间的相互依赖关系的识别。利用高阶交互显着提高了我们的模型利用多模态协同作用的能力，从而比最先进的替代方案具有更好的性能，如在各种基准上的综合实验所示。 </p></details> | CVPR2024/TPAMI2024 |  [Code Link](https://github.com/zheng980629/SHIP) | Feng Zhao |
| **[优雅与精确相遇之处： 迈向一个紧凑、自动、灵活的多模态图像融合和应用框架](https://www.ijcai.org/proceedings/2024/123)** | 2024 | <details><summary>Show</summary><p> 多模态图像融合旨在整合来自多个传感器的图像，产生视觉上吸引人的图像，并提供比任何单一图像更全面的信息。为了确保高视觉质量并促进准确的后续感知任务，以前的方法通常使用加权损失函数级联网络。然而，这种简单化的策略很难真正实现“两全其美”，并且调整众多手工制作的参数变得很麻烦。为了应对这些挑战，本文介绍了一个紧凑，自动和灵活的框架，称为CAF，设计用于红外和可见光图像融合，以及后续任务。具体地说，我们将融合和感知的组合问题重组成一个单一的目标，允许从两个任务中相互优化信息。然后，我们还利用感知任务通知融合损失函数的设计，促进自动识别适合于该任务的最佳融合目标。此外，CAF可以轻松地支持与现有方法的无缝集成，在适应各种任务和网络结构方面提供灵活性。大量的实验证明了CAF的优越性，它不仅产生了视觉上令人满意的融合结果，而且实现了比最先进的方法高1.7倍的检测图和高2.0倍的分割mIoU。 </p></details> | IJCAI2024 |  [Code Link](https://github.com/RollingPlain/CAF_IVIF) |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> | 2024 |  [](https://github.com/RollingPlain/CAF_IVIF) |

## 推荐
### 红外-可见光图像融合(IVIF)
| **题目** | **日期** | **摘要** | **录用信息** | **代码** | **备注**
| --- | --- | --- | --- | --- | --- |
| **[SHIP: 探测红外与可见光图像融合的协同高阶相互作用](https://openaccess.thecvf.com/content/CVPR2024/html/Zheng_Probing_Synergistic_High-Order_Interaction_in_Infrared_and_Visible_Image_Fusion_CVPR_2024_paper.html)** | 2024 | <details><summary>Show</summary><p> 红外和可见光图像融合旨在通过整合和区分互补信息从多个来源来生成融合图像。虽然具有全局空间交互的交叉注意力机制看起来很有希望，但它只捕获二阶空间交互，而忽略了空间和通道维度上的高阶交互。这种限制阻碍了多模态之间协同作用的利用。为了弥补这一差距，我们引入了一种协同高阶交互范式(SHIP)，旨在系统地研究红外图像和可见光图像在两个基本维度上的空间细粒度和全局统计协作:1)空间维度:我们通过元素乘法构建空间细粒度交互，在数学上等价于全局交互，然后通过迭代聚合和进化互补信息来促进高阶格式，提高效率和灵活性;2)通道维度:在与一阶统计量(均值)的通道交互上扩展，我们设计了高阶通道交互，以促进基于全局统计的源图像之间的相互依赖关系的识别。利用高阶交互显着提高了我们的模型利用多模态协同作用的能力，从而比最先进的替代方案具有更好的性能，如在各种基准上的综合实验所示。 </p></details> | CVPR2024/TPAMI2024 |  [Code Link](https://github.com/zheng980629/SHIP) | 对比实验 |
| **[CrossFuse：一种基于交叉注意力机制的新型红外和可见光图像融合方法(Arxiv)](http://arxiv.org/abs/2406.10581v1)** | 2024-06-15 | <details><summary>Show</summary><p>多模态视觉信息融合旨在将多传感器数据整合到一张图像中，其中包含更多的互补信息和更少的冗余特征。然而，互补信息很难提取，尤其是对于红外和可见光图像，这两种模态之间存在很大的相似性差距。常见的交叉注意力模块只考虑相关性，相反，图像融合任务需要关注互补性（uncorrelation）。因此，本文提出了一种新的交叉注意力机制 （CAM） 来增强互补信息。此外，提出了一种基于两阶段训练策略的融合方案来生成融合图像。在第一阶段，针对每种模态训练两个具有相同架构的自动编码器网络。然后，使用固定编码器，在第二阶段训练 CAM 和解码器。通过训练的 CAM，从两种模态中提取的特征被集成到一个融合的特征中，其中互补信息得到增强，冗余特征得到减少。最后，经过训练的解码器可以生成融合图像。实验结果表明，与现有的融合网络相比，我们提出的融合方法获得了SOTA融合性能。这些代码可在 https://github.com/hli1221/CrossFuse | Inf. Fusion | [Code Link](https://github.com/hli1221/CrossFuse) | Hui Li |
| **[DCINN: 用于图像融合的细节保持条件可逆网络的一般范式](https://link.springer.com/article/10.1007/s11263-023-01924-5)** | 2024 | <details><summary>Show</summary><p> 现有的图像融合深度学习技术要么直接学习图像映射(LIM)，由于每个像素的考虑相等，这使得它们在保留细节方面无效，要么学习细节映射(LDM)，它只获得有限的性能水平，因为只有细节用于推理。最近的无损可逆网络(INN)已经证明了其细节保持能力。然而，INN对图像融合任务的直接适用性受到保体积约束的限制。此外，缺乏一致的保细节图像融合框架来产生令人满意的结果。为此，我们提出了一种基于新条件 INN（称为 DCINN）的图像融合通用范式。DCINN范式有三个核心组件:将图像映射转换为细节映射的分解模块;一个直接从源图像中提取辅助特征的辅助网络(ANet);以及一个条件INN (CINN)，它学习基于辅助特征的细节映射。新颖的设计得益于 INN、LIM 和 LDM 方法的优点，同时避免它们的缺点。特别是，使用 INN 到 LDM 可以轻松满足保体积约束，同时仍然保留细节。此外，由于辅助特征作为条件特征，ANet 允许在不影响细节映射的情况下使用不仅仅是推理的细节。在三个基准融合问题上的广泛实验，即全色锐化、高光谱和多光谱图像融合，以及红外和可见光图像融合，证明了我们的方法与最近的最先进方法相比的优越性。 </p></details> | IJCV2024 |  [Code Link](https://github.com/wwhappylife/DCINN) | 对比实验 |
| **[SegMiF: 用于图像融合和分割的多交互式特征学习和全职多模态基准测试(Arxiv)](http://arxiv.org/abs/2308.02097v1)** | 2023 | <details><summary>Show</summary><p>多模态图像融合和分割在自动驾驶和机器人作中起着至关重要的作用。早期的工作只集中在提高一项任务的性能，\emph{eg.，} 融合或分割，这使得它很难达到~'两全其美'。为了克服这个问题，在本文中，我们提出了一种用于图像融合和 Segmentation 的 Multi-interactive Feature 学习架构，即 SegMiF，并利用双任务相关性来促进这两个任务的性能。SegMiF 采用级联结构，包含一个融合子网络和一个常用的分割子网络。通过巧妙地桥接两个组件之间的中间特征，从分割任务中学到的知识可以有效地协助融合任务。此外，受益的融合网络支持分割网络执行得更自命不凡。此外，建立了分层交互式注意力块，以确保两个任务之间所有重要信息的细粒度映射，从而使模态/语义特征完全互交互。此外，还引入了动态权重因子，可以自动调整每个任务的相应权重，可以平衡交互特征的对应关系，突破费力调优的限制。此外，我们构建了一个智能多波双目成像系统，并收集了一个全职多模态基准，具有 15 个带注释的像素级类别，用于图像融合和分割。对几个公共数据集和我们的基准测试进行的广泛实验表明，所提出的方法输出视觉上吸引人的融合图像，并且性能一般在现实世界中，分割 mIoU 高于最先进的方法。</p></details> | ICCV2023 | [Code Link](https://github.com/JinyuanLiu-CV/SegMiF) | 对比实验 |
| **[MetaFusion：基于目标检测的元特征嵌入的红外和可见光图像融合](https://paperswithcode.com/paper/metafusion-infrared-and-visible-image-fusion)** | 2023 | <details><summary>Show</summary><p> 融合红外和可见光图像可以为后续的目标检测任务提供更多的纹理细节。相反，检测任务提供对象语义信息来改进红外和可见光图像的融合。因此，使用相互促进的联合融合和检测学习越来越受到关注。然而，这两个不同级别任务之间的特征差距阻碍了进展。针对这一问题，本文提出了一种基于目标检测元特征嵌入的红外和可见光图像融合。核心思想是元特征嵌入模型旨在根据融合网络能力生成对象语义特征，因此语义特征自然与融合特征兼容。它通过模拟元学习进行了优化。此外，我们进一步实现了融合和检测任务之间的相互促进学习，以提高其性能。在三个公共数据集上的综合实验证明了我们方法的有效性。 </p></details> | CVPR2023 |  [Code Link](https://github.com/wdzhao123/MetaFusion) | 对比实验 |
| **[LRRNet：一种用于红外和可见光图像的新型表示学习引导融合网络](https://ieeexplore.ieee.org/document/10105495)** | 2023 | <details><summary>Show</summary><p> 基于深度学习的融合方法在图像融合任务中取得了良好的性能。这归因于网络架构在融合过程中起着非常重要的作用。然而，一般来说，很难指定一个好的融合架构，因此融合网络的设计仍然是黑艺术，而不是科学。为了解决这个问题，我们在数学上制定了融合任务，并在其最优解和可以实现它的网络架构之间建立联系。这种方法导致本文提出了一种新颖的构建轻量级融合网络的方法。它通过试错策略避免了耗时的经验网络设计。特别是我们对融合任务采用了可学习的表示方法，其中融合网络架构的构建由产生可学习模型的优化算法指导。低秩表示 (LRR) 目标是我们可学习模型的基础。将解心的矩阵乘法转化为卷积运算，将优化过程替换为特殊的前馈网络。基于这种新的网络架构，构建了一个端到端轻量级融合网络来融合红外和可见光图像。它的成功训练是由一个细节到语义信息的损失函数来促进的，该函数旨在保留图像细节并增强源图像的显着特征。我们的实验表明，所提出的融合网络在公共数据集上表现出比最先进的融合方法更好的融合性能。有趣的是，我们的网络比其他现有方法需要更少的训练参数。T </p></details> | TPAMI2023 |  [Code Link](https://ieeexplore.ieee.org/document/10105495) | 对比实验 |
| **[Dif-Fusion: 基于扩散模型的红外与可见光图像融合](https://ieeexplore.ieee.org/document/10286359)** | 2023 | <details><summary>Show</summary><p> 颜色在人类视知觉中扮演重要角色，反映物体的光谱。然而，现有的红外和可见光图像融合方法很少探索如何直接处理多光谱 / 通道数据，实现高色彩保真度。针对上述问题，提出了一种基于扩散模型的多通道输入数据分布生成方法 Dif-Fusion，该方法增强了多源信息聚合的能力，提高了颜色的保真度。具体地说，在现有的融合方法中，我们没有将多通道图像转换为单通道数据，而是在潜在空间中利用正向和反向扩散过程，通过去噪网络来创建多通道数据分布。然后，利用去噪网络提取可见光和红外信息的多通道扩散特征。最后，将多通道扩散特征反馈给多通道融合模块，直接生成三通道融合图像。为了保留纹理和强度信息，我们提出了多通道梯度损失和强度损失。随着目前测量纹理和强度保真度的评价指标，我们引入 Delta E 作为一种新的评价指标来量化颜色保真度。大量的实验表明，我们的方法是更有效的比其他国家的最先进的图像融合方法，特别是在彩色保真度。 </p></details> | TIP2023 |  [Code Link](https://github.com/GeoVectorMatrix/Dif-Fusion) | 适用彩色图像 |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |

### 医学图像融合(MIF)
| **题目** | **日期** | **摘要** | **录用信息** | **代码** | **备注**
| --- | --- | --- | --- | --- | --- |
| **[MMIF-INet: 可逆网络的多模态医学图像融合](https://www.sciencedirect.com/science/article/abs/pii/S1566253524004445)** | 2024-09 | <details><summary>Show</summary><p> 多模态医学图像融合 (MMIF) 技术旨在生成全面反映组织、器官和代谢信息的融合图像，从而辅助医学诊断，提高临床诊断的可靠性。然而，大多数方法在特征提取和融合过程中都存在信息丢失问题，很少探索如何直接处理多通道数据。针对上述问题，本文提出了一种新的可逆融合网络 (MMIF-INet) ，该网络接受三通道彩色图像作为模型的输入，并以过程可逆的方式生成多通道数据分布。具体来说，离散小波变换 (dWT) 用于下采样，目的是将源图像对分解为高频和低频分量。同时，可逆块 (IB) 促进了初步的特征融合，实现了跨域互补信息和多源信息的无损集成。IB 和 DWT 的结合保证了初始融合的可逆性和跨不同尺度的语义特征提取。为了适应融合任务，采用了多尺度融合模块，集成了来自不同模式和多尺度特征的不同组件。最后，设计了一种混合损耗，从结构、梯度、强度和色度的角度对模型训练进行约束，从而实现对源图像的亮度、颜色和详细信息的有效保留。在多个医学数据集上的实验表明，MMIF-INet 在视觉质量、定量度量和融合效率方面优于现有的方法，特别是在色彩保真度方面。扩展到红外 - 可见光图像融合，七个最佳评价标准进一步证实了 MMIF-INet 的优越融合性能。 </p></details> | Inf.Fusion2024 |  [Code Link](https://github.com/HeDan-11/MMIF-INet) | 适用彩色图像融合 |
| **[MM-Net：一种基于MixFormer的解剖和功能图像融合多尺度网络](https://ieeexplore.ieee.org/document/10471275)** | 2024-03-25 | <details><summary>Show</summary><p> 解剖和功能图像融合是各种医学和生物应用中的一项重要技术。近年来，基于深度学习的方法已成为多模态图像融合领域的主流方向。然而，现有的基于dl的融合方法很难同时有效地捕获局部特征和全局上下文信息。此外，在图像融合中，特征的尺度多样性是关键问题，在大多数现有工作中往往缺乏足够的关注。在本文中，为了解决上述问题，我们提出了一种基于MixFormer的多尺度网络，称为MM-Net，用于解剖和功能图像融合。在我们的方法中，引入了一个改进的基于MixFormer的主干，从源图像中在多个尺度上充分提取局部特征和全局上下文信息。基于多源空间注意的跨模态特征融合(CMFF)模块，将不同源图像的特征在多个尺度上融合。融合特征的尺度多样性通过一系列多尺度特征交互(MSFI)模块和特征聚合上采样(FAU)模块进一步丰富。此外，设计了一个由空间域和频域分量组成的损失函数来训练所提出的融合模型。实验结果表明，我们的方法在定性和定量比较上都优于几种最先进的融合方法，所提出的融合模型具有良好的泛化能力。 </p></details> | TIP2024 |  [Code Link](https://github.com/yuliu316316) | Yu Liu，亮度恢复较差，功能图像融合 |
| **[ALMFNet:学习搜索一种用于医学图像融合的轻量级广义网络]()** | 2023-12-14 | <details><summary>Show</summary><p> 图像融合在综合医学成像管道中是必不可少的。通过采用深度学习技术，医学图像融合在过去几年中取得了巨大的进展。然而，现有的方法对特定类型的医学图像融合任务做出了努力，在泛化方面可能会遇到困难。此外，它们中的大多数对每个神经进行应变，以增加深度宽度来设计各种架构，将障碍置于运行效率中。为了解决上述问题，我们提出了一种自动搜索轻量级多源融合网络，即ALMFnet，旨在以网络架构搜索的方式结合软件和硬件知识进行医学图像融合。具体来说，开发了由两个不同的特征提取模块和一个融合模块组成的ALMFnet，用于在广义模型中提取和细化多源特征。此外，受协作原则的启发，我们引入了硬件约束来充分搜索每个特定组件，进一步降低了所获得模型的复杂性。此外，为了保留病理图像区域的重要细节，我们在所开发的方法中引入了一个分割掩码。实验结果表明，我们的广义模型不仅在定量分数方面优于以前的方法，而且在模型复杂度方面也优于以前的方法 </p></details> | TCSVT2024 |  [Code Link](https://github.com/RollingPlain/ALMFnet) | Jinyuan Liu |
| **[MsgFusion：用于多模态脑图像融合的医学语义引导双分支网络](https://ieeexplore.ieee.org/document/10120980)** | 2023-04 | <details><summary>Show</summary><p> 多模态图像融合在医学图像分析和应用中起着至关重要的作用，其中计算机断层扫描 (CT)、磁共振 (MR)、单光子发射计算机断层扫描 (SPECT) 和正电子发射断层扫描 (PET) 是常用的模式，尤其是对于脑疾病诊断。现有的融合方法大多没有考虑医学图像的特点，采用类似的策略和评估标准进行自然图像融合。虽然独特的医学语义信息（MS-Info）隐藏在不同的模式中，但融合结果的最终临床评估被忽略了。我们的 MsgFusion 首先在 MR/CT/PET/SPECT 图像和图像特征的关键 MS-Info 之间建立关系，使用两个分支和图像融合框架的设计来指导 CNN 特征提取。对于 MR 图像，我们结合空间域特征和频域特征 (SF) 开发了一个分支。对于 PET/SPECT/CT 图像，我们集成了灰色颜色空间特征并调整 HSV 颜色空间特征 (GV) 来开发另一个分支。还提出了一种基于分类的分层融合策略来重建融合图像以持续和增强反映解剖结构和功能代谢的显着 MS-Info。融合实验是在许多对 MR-PET/SPECT 和 MR-CT 图像上进行的。根据七个经典客观质量评估和 30 名临床医生的一项新的主观临床质量评估，所提出的 MsgFusion 的融合结果优于现有的代表性方法。 </p></details> | TMM2023 |  []() | 实验分析设计较好 |
| **[GeSeNet：一种用于医学图像融合的通用耦掩模集成语义引导网络](https://ieeexplore.ieee.org/abstract/document/10190200)** | 2023-07 | <details><summary>Show</summary><p> 目前，多模态医学图像融合技术已成为研究人员和医生预测疾病和研究病理学的重要手段。然而，如何在保证时间效率的前提下，如何从不同的模态源图像中保留更多独特的特征是一个棘手的问题。为了解决这个问题，我们提出了一种灵活的语义引导架构，该架构具有端到端的掩码优化框架，称为 GeSeNet。具体来说，设计了一个区域掩码模块来加深重要信息的学习，同时修剪冗余计算以减少运行时间。提出了一种边缘增强模块和全局细化模块来修改提取的特征，以提高边缘纹理并调整整体视觉性能。此外，我们引入了一个语义模块，该模块与所提出的融合网络级联，将语义信息传递到我们生成的结果中。在我们提出的方法和十种最先进的方法之间部署了足够的定性和定量比较实验（即 MRI-CT、MRI-PET 和 MRISPECT），这表明我们生成的图像带来了方式。此外，我们还进行了操作效率比较和消融实验，以证明我们提出的方法可以在多模态医学图像融合领域表现出色。 </p></details> | TNNLS2023 |  [Code Link](https://github.com/lok18/GeSeNet) | Jinyuan Liu，各方面均衡，适合做对比方法 |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |


### 统一模型(IVIF和MIF)
| **题目** | **日期** | **摘要** | **录用信息** | **代码** | **备注**
| --- | --- | --- | --- | --- | --- |
| **[EMMA：等变多模态图像融合(Arxiv)](http://arxiv.org/abs/2305.11443v2)** | 2024 | <details><summary>Show</summary><p>多模态图像融合是一种将来自不同传感器或模态的信息组合在一起的技术，使融合图像能够保留来自每种模态的互补特征，例如功能亮点和纹理细节。然而，由于地面实况融合数据的稀缺性，这种融合模型的有效训练具有挑战性。为了解决这个问题，我们提出了用于端到端自我监督学习的等变多模态 imAge 融合 （EMMA） 范式。我们的方法植根于自然成像响应对某些转换是等变的先验知识。因此，我们引入了一种新的训练范式，其中包括一个融合模块、一个伪传感模块和一个等变融合模块。这些组件使网络训练能够遵循自然传感成像过程的原理，同时满足等变成像先验。大量实验证实，EMMA 可产生高质量的红外可见光和医学图像融合结果，同时促进下游多模态分割和检测任务。</p></details> | CVPR2024 | [Code Link](https://github.com/Zhaozixiang1228/MMIF-EMMA) | Zixiang Zhao，MIF主观不佳，客观OK |
| **[MMDRFuse：用于多模态图像融合的具有动态刷新功能的提炼迷你模型(Arxiv)](http://arxiv.org/abs/2408.15641v1)** | 2024 | <details><summary>Show</summary><p>近年来，多模态图像融合 （MMIF） 已应用于多个领域，吸引了众多学者致力于提高融合性能。然而，普遍的关注点主要集中在架构设计上，而不是训练策略上。作为一项低级视觉任务，图像融合应该快速提供输出图像以供观察和支持下游任务。因此，应避免多余的计算和存储开销。在这项工作中，提出了一种具有动态刷新策略 （MMDRFuse） 的轻量级 Distilled Mini-Model 来实现这一目标。为了追求模型简洁性，通过三个精心设计的监督获得了一个极小的卷积网络，总共有 113 个可训练参数 （0.44 KB）。首先，通过强调外部空间特征的一致性来构建可消化的蒸馏，为目标网络提供具有平衡细节和显著性的软监督。其次，我们开发一个综合损失来平衡来自源图像的像素、梯度和感知线索。第三，使用创新的动态刷新训练策略在训练期间协作历史参数和当前监督，以及自适应调整功能以优化融合网络。在多个公共数据集上的广泛实验表明，我们的方法在模型效率和复杂度方面表现出有希望的优势，在多图像融合任务和下游行人检测应用中具有优异的性能。这项工作的代码在 https://github.com/yanglinDeng/MMDRFuse 上公开提供。</p></details> | ACMMM2024</p></details> | [Code Link](https://github.com/yanglinDeng/MMDRFuse) | 效果一般，目前速度最快 |
| **[CDDFuse: 多模态图像融合的相关驱动双分支特征分解](https://openaccess.thecvf.com/content/CVPR2023/html/Zhao_CDDFuse_Correlation-Driven_Dual-Branch_Feature_Decomposition_for_Multi-Modality_Image_Fusion_CVPR_2023_paper.html)** | 2023 | <details><summary>Show</summary><p> 多模态(MM)图像融合旨在渲染融合图像，以保持不同模态的优点，如功能突出和详细的纹理。为了解决跨模态特征建模和分解理想的模态特定和模态共享特征的挑战，我们提出了一种新的相关驱动特征分解融合(CDDFuse)网络。首先，CDDFuse 使用 Restormer 块来提取跨模态浅层特征。然后，我们引入了一个双分支 Transformer-CNN 特征提取器，它具有 Lite Transformer (LT) 块，利用远程注意力来处理低频全局特征和可逆神经网络 (INN) 块，专注于提取高频局部信息。进一步提出了一种相关驱动损失，使低频特征与基于嵌入信息不相关的高频特征相关。然后，基于lt的全局融合和基于inn的局部融合层输出融合图像。大量的实验表明，我们的CDDFuse在多个融合任务中取得了很好的结果，包括红外可见光图像融合和医学图像融合。我们还表明，CDDFuse可以在统一的基准测试中提高下游红外可见语义分割和目标检测的性能。 </p></details> | CVPR2023 |  [Code Link](https://github.com/Zhaozixiang1228/MMIF-CDDFuse) | Zixiang Zhao，均衡，对比实验首选 |
| **[MUFusion: 一种通用的基于存储单元的无监督图像融合网络](https://www.sciencedirect.com/science/article/abs/pii/S1566253522002202)** | 2023 | <details><summary>Show</summary><p> 现有的图像融合方法都致力于利用单一的深层网络来解决不同的图像融合问题，近年来取得了很好的效果。然而，在这些方法中，由于缺乏地面真实输出，在训练过程中只能利用源图像的外观来生成融合图像，从而导致次优解的产生。为此，我们提出了一种自我进化的训练公式，引入了一种新的记忆单元结构 (MUFusion)。在这个单元中，我们利用在训练过程中获得的中间融合结果进一步协同监督融合图像。这样，我们的融合结果不仅可以从原始输入图像中学习，而且可以从网络本身的中间输出中获益。在此基础上，设计了由内容损失和内存损失两个损失项组成的自适应统一损失函数。特别是基于源图像的活动级映射计算内容丢失，从而约束输出图像包含特定信息。另一方面，在模型输出的基础上得到内存损失，从而使网络产生更高质量的融合结果。考虑到手工制作的活动水平图不能始终如一地反映准确的显著性判断，我们在它们之间放置了两个自适应权重项，以防止这种退化现象。一般来说，我们的 MUFusion 可以有效地处理一系列图像融合任务，包括红外与可见光图像融合、多聚焦图像融合、多曝光图像融合和医学图像融合。特别地，源图像是在信道维度中串联的。然后，采用两个尺度的密集连通特征提取网络对源图像进行深度特征提取。然后，通过特征提取网络中的两个跳跃连接的特征重建块得到融合结果。通过对 4 个图像融合子任务的定性和定量实验，证明了该算法相对于最先进的融合方法的优越性。 </p></details> | Inf.Fusion2023 | 2021 [Code Link](https://github.com/AWCXV/MUFusion) |
| **[SwinFusion:通过Swin Transformer进行通用图像融合的跨域远程学习](https://ieeexplore.ieee.org/document/9812535)** | 2022 | <details><summary>Show</summary><p> 本研究提出了一种新的基于跨域远程学习和Swin Transformer的通用图像融合框架，称为SwinFusion。一方面，设计了一个注意力引导的跨域模块来实现互补信息和全局交互的充分集成。更具体地说，所提出的方法涉及基于自注意力的域内融合单元和基于交叉注意力的域间融合单元，该单元在同一域和跨域中挖掘和集成长依赖关系。通过远程依赖建模，该网络能够完全实现特定领域的信息提取和跨域互补信息集成，并从全局角度保持适当的表观强度。特别是，我们将移位窗口机制引入到自我注意和交叉注意中，这使得我们的模型能够接收任意大小的图像。另一方面，多场景图像融合问题被推广到具有结构维护、细节保存和适当强度控制的统一框架。此外，由SSIM损失、纹理损失和强度损失组成的精细损失函数驱动网络保留丰富的纹理细节和结构信息，并给出最佳的表观强度。在多模态图像融合和数字摄影图像融合上的大量实验表明，与最先进的统一图像融合算法和特定任务替代方案相比，我们的 SwinFusion 的优越性。 </p></details> | JAS2022 |  [Code Link](https://github.com/Linfeng-Tang/SwinFusion) | Jiayi Ma，均衡，Qabf和SSIM较高 |
| **[SDNet：一种用于实时图像融合的通用挤压和分解网络](https://link.springer.com/article/10.1007/s11263-021-01501-8)** | 2021 | <details><summary>Show</summary><p> 本文提出了一种挤压分解网络(SDNet)，实时实现多模态和数字摄影图像融合。首先，我们将多个融合问题转化为梯度和强度信息的提取和重构，并相应地设计了一个通用的损失函数形式，它由强度项和梯度项组成。对于梯度项，我们引入了一个自适应决策块，根据像素尺度上的纹理丰富度来决定梯度分布的优化目标，从而引导融合图像包含更丰富的纹理细节。对于强度项，我们调整了每个强度损失项的权重，以改变来自不同图像的强度信息的比例，使其可以适应多个图像融合任务。其次，我们将挤压分解的思想引入到图像融合中。具体来说，我们不仅考虑从源图像到融合结果的挤压过程，还考虑了从融合结果到源图像的分解过程。由于分解图像的质量直接取决于融合结果，它可以迫使融合结果包含更多的场景细节。实验结果表明，我们的方法在各种融合任务中的主观视觉效果和定量指标方面优于最先进的方法。此外，我们的方法比最先进的方法快得多，可以处理实时融合任务。 </p></details> | IJCV2021 |  [Code Link](https://github.com/HaoZhang1018/SDNet) | Jiayi Ma，TF框架 |
| **[U2Fusion：统一的无监督图像融合网络](https://ieeexplore.ieee.org/abstract/document/9151265)** | 2020 | <details><summary>Show</summary><p> 本研究提出了一种新颖的统一且无监督的端到端图像融合网络，称为 U2Fusion，它能够解决不同的融合问题，包括多模态、多曝光和多焦点情况。使用特征提取和信息测量，U2Fusion 自动估计相应源图像的重要性并得出自适应信息保存度。因此，不同的融合任务被统一在同一个框架中。基于自适应度，训练网络以保持融合结果和源图像之间的自适应相似性。因此，大大减少了将深度学习应用于图像融合的绊脚石，例如对地面实况和专门设计的指标的要求。通过为不同任务顺序训练单个模型时避免先前融合能力的损失，我们获得了适用于多个融合任务的统一模型。此外，还发布了一个新的对齐的红外和可见图像数据集 RoadScene（可在https://github.com/hanna-xu/RoadScene 获取），为基准评估提供新的选择。三种典型图像融合任务的定性和定量实验结果验证了U2Fusion的有效性和普适性。 </p></details> | TPAMI2020 |  [Code Link](https://github.com/hanna-xu/RoadScene) |  Jiayi Ma，TF框架 |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |
