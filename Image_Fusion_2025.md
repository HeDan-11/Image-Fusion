# 图像融合(2025年 期刊和会议论文汇总)
## 目录
- [Arxiv](#Arxiv(未录用))
- [期刊](#期刊)
- [会议](#会议)
- [推荐](#推荐)

## Arxiv(未录用)
| **题目** | **日期** | **摘要** | **录用信息** | **代码** | **备注**
| --- | --- | --- | --- | --- | --- |
| **[MaeFuse：通过引导式培训，使用预训练的掩蔽自动编码器传输 Omni 特征以实现红外和可见光图像融合(Arxiv)](http://arxiv.org/abs/2404.11016v2)** | 2025-02-09 | <details><summary>Show</summary><p>在本文中，我们介绍了 MaeFuse，这是一种专为红外和可见光图像融合 （IVIF） 设计的新型自动编码器模型。现有的图像融合方法通常依赖于训练与下游任务相结合来获得高级视觉信息，这对于强调目标对象并在视觉质量和特定任务应用中提供令人印象深刻的结果非常有效。我们的模型名为 MaeFuse ，它不是由下游任务驱动，而是利用了掩码自动编码器 （MAE） 的预训练编码器，该编码器为低级重建和高级视觉任务提供了全向特征提取，以低成本获得感知友好的特征。为了消除不同模态特征的域差距和 MAE 编码器引起的阻塞效应，我们进一步开发了一种引导式训练策略。这种策略经过精心设计，以确保融合层无缝适应编码器的特征空间，从而逐渐增强融合性能。所提出的方法可以促进来自红外和可见光模态的特征向量的全面整合，从而保留每个模态中固有的丰富细节。MaeFuse 不仅在融合技术领域引入了新颖的视角，而且在各种公共数据集中都以令人印象深刻的性能脱颖而出。</p></details> |  | None | Jiayi Ma |
| **[Hazy条件下可见光和红外图像联合复原融合的红外辅助单阶段框架(Arxiv)](http://arxiv.org/abs/2411.12586v2)** | 2025-02-08 | <details><summary>Show</summary><p>红外和可见光 （IR-VIS） 图像融合因其广泛的应用价值而受到广泛关注。然而，现有的方法往往忽视了红外图像在朦胧条件下恢复可见光图像特征的互补作用。为了解决这个问题，我们提出了一个联合学习框架，利用红外图像来恢复和融合朦胧的 IR-VIS 图像。为了减轻 IR-VIS 图像之间特征多样性的不利影响，我们引入了一种提示生成机制来调节特定于模态的特征不兼容性。这将从非共享图像信息创建一个提示选择矩阵，然后是从提示池生成的提示嵌入。这些嵌入有助于生成用于去雾的候选特征。我们进一步设计了一种红外辅助特征恢复机制，根据雾度选择候选特征，从而在单阶段框架内实现同步恢复和融合。为了提高融合质量，我们构建了一个多阶段的提示嵌入融合模块，该模块利用了提示生成模块的特征补充。我们的方法有效地融合 IR-VIS 图像，同时去除雾霾，产生清晰、无雾的融合结果。与先去雾再融合的两阶段方法相比，我们的方法支持在单阶段框架中进行协同训练，使模型相对轻量级，适合实际部署。实验结果验证了其有效性，并证明了与现有方法相比的优势。论文的源代码可以在 https://github.com/fangjiaqi0909/IASSF。 |  | [Code Link](https://github.com/fangjiaqi0909/IASSF) |
| **[HetSSNet： 用于全色和多光谱图像融合的时空-光谱异构图学习网络(Arxiv)](http://arxiv.org/abs/2502.04623v1)** | 2025-02-07 | <details><summary>Show</summary><p>遥感全色锐化旨在重建全色 （PAN） 图像和低分辨率多光谱 （LR-MS） 图像融合过程中的时空光谱特性，最终生成高分辨率多光谱 （HR-MS） 图像。在主流建模策略中，即 CNN 和 Transformer，输入图像被视为欧几里得空间中大小相等的像素网格。它们在面对具有不规则地面物体的遥感图像时存在局限性。图是更灵活的结构，但是，使用图对空间-光谱属性进行建模时，存在两个主要挑战：\emph{1） 为空间-光谱关系先验构建自定义的图形结构};\emph{2） 通过图学习统一的时空-谱表示}。为了应对这些挑战，我们提出了名为 \textbf{HetSSNet} 的时空-光谱异构图学习网络。具体来说， HetSSNet 最初构建了用于全色锐化的异构图形结构，它明确描述了特定于全色锐化的关系。随后，设计了基本关系模式生成模块，从异构图中提取多种关系模式。最后，利用关系模式聚合模块协同学习节点间不同关系的统一时空-谱表示，并从局部和全局角度进行自适应重要性学习。广泛的实验证明了 HetSSNet 的显著优越性和泛化性。</p></details> |  | None |
| **[MATCNN： 基于多尺度 CNN 的带注意力变换器的红外可见光图像融合方法(Arxiv)](http://arxiv.org/abs/2502.01959v1)** | 2025-02-04 | <details><summary>Show</summary><p>虽然基于注意力的方法在增强图像融合和解决长距离特征依赖性带来的挑战方面取得了相当大的进展，但它们在捕获局部特征方面的功效因缺乏多样化的感受野提取技术而受到损害。为了克服现有融合方法在提取多尺度局部特征和保留全局特征方面的不足，该文提出了一种基于注意力变换器的多尺度卷积神经网络 （MATCNN） 的新型跨模态图像融合方法。MATCNN 利用多尺度融合模块 （MSFM） 提取不同尺度的局部特征，并利用全局特征提取模块 （GFEM） 提取全局特征。将两者结合起来可以减少细节特征的损失，并提高全局特征表示的能力。同时，使用信息掩码标记图像中的相关细节，旨在提高融合图像中红外图像中重要信息和可见光图像中背景纹理的保留比例。随后，开发了一种新的优化算法，利用掩码通过内容整合、结构相似性指数测量和全局特征损失来指导特征提取。对各种数据集进行了定量和定性评估，表明 MATCNN 有效地突出了红外突出目标，保留了可见光图像中的其他细节，并为跨模态图像实现了更好的融合结果。MATCNN 的代码将在 https://github.com/zhang3849/MATCNN.git 上提供。</p></details> |  | [Code Link](https://github.com/zhang3849/MATCNN.git) |
| **[使用多模态图像融合进行深度集成，用于肺癌的高效分类(Arxiv)](http://arxiv.org/abs/2502.00078v1)** | 2025-01-31 | <details><summary>Show</summary><p>本研究的重点是从多模态肺部图像中对癌性和健康切片进行分类。研究中使用的数据包括计算机断层扫描 （CT） 和正电子发射断层扫描 （PET） 图像。所提出的策略通过利用主成分分析 （PCA） 和自动编码器实现了 PET 和 CT 图像的融合。随后，开发了一种新的基于集成的分类器，即深度集成多模态融合 （DEMF），采用多数投票对正在检查的样本图像进行分类。采用梯度加权类激活映射 （Grad-CAM） 来可视化受癌症影响图像的分类准确性。鉴于样本量有限，在训练阶段采用了随机图像增广策略。DEMF 网络有助于缓解计算机辅助医学图像分析中稀缺数据的挑战。拟议的网络与三个公开可用的数据集中最先进的网络进行了比较。该网络在准确性、F1-Score、Accuracy 和 Recall 等指标上优于其他网络。调查结果突出了所提出的网络的有效性。</p></details> |  | None |
| **[作为图像的任意数据：跨模态和不规则间隔的患者数据与 Vision Transformer 的融合(Arxiv)](http://arxiv.org/abs/2501.18237v1)** | 2025-01-30 | <details><summary>Show</summary><p>患者在每次住院时会接受多次检查，每次检查提供不同的健康状态信息。这些检查包括不同采样率的时间序列数据、离散的单点测量、药物治疗等干预，以及影像数据。尽管医生能够直观地处理和整合不同的模态，但神经网络需要为每个模态进行特定建模，增加了训练的复杂性。我们展示了将所有信息作为图像与非结构化文本进行可视化，并随后训练常规的视觉-文本变换器，可以显著减少这种复杂性。我们的方法——基于视觉变换器的非规则采样多模态数据融合（ViTiMM）——不仅简化了数据预处理和建模过程，还在预测住院期间死亡率和表型分析方面超越了当前最先进的方法。该方法在MIMIC-IV数据集上的6,175名患者数据上进行评估，包括临床测量、药物、X光影像和心电图扫描。我们希望我们的工作能激发多模态医学人工智能的进展，通过将训练复杂性简化为（视觉）提示工程，降低了进入门槛，并启用了无代码解决方案的训练。源代码将公开发布</p></details> |  | None |
| **[E2E-MFD：迈向端到端同步多模态融合检测(Arxiv)](http://arxiv.org/abs/2403.09323v4)** | 2025-01-27 | <details><summary>Show</summary><p>多模态图像融合和物体检测在自动驾驶中至关重要。虽然现有方法在融合纹理细节和语义信息方面取得了进展，但它们复杂的训练过程限制了更广泛的应用。为了解决这一挑战，我们提出了E2E-MFD，一种新颖的端到端多模态融合检测算法。E2E-MFD简化了这一过程，通过单一的训练阶段实现了高性能。它通过同步联合优化各个组件，避免了与单独任务相关的次优解决方案。此外，它在梯度矩阵中实现了一种全面的优化策略，用于共享参数，确保收敛到最优的融合检测配置。我们在多个公开数据集上的广泛测试展示了E2E-MFD的卓越能力，既展示了视觉上令人满意的图像融合，也展示了令人印象深刻的检测结果，例如在水平物体检测数据集M3FD和定向物体检测数据集DroneVehicle上分别提高了3.9%和2.0%的mAP50，相较于最先进的其他方法。代码已发布：https://github.com/icey-zhang/E2E-MFD。</p></details> |  | [Code Link](https://github.com/icey-zhang/E2E-MFD) |
| **[红外和可见光图像融合：从数据兼容性到任务适应(Arxiv)](http://arxiv.org/abs/2501.10761v1)** | 2025-01-18 | <details><summary>Show</summary><p>红外-可见光图像融合 （IVIF） 是计算机视觉中的一项关键任务，旨在将红外光谱和可见光谱的独特特征集成到一个统一的表示中。自 2018 年以来，该领域已进入深度学习时代，越来越多的方法引入了一系列网络和损失函数来提高视觉性能。然而，数据兼容性、感知准确性和效率等挑战仍然存在。不幸的是，最近缺乏针对这个迅速扩张的领域的综合调查。本白皮书通过提供涵盖广泛主题的全面调查来填补这一空白。我们引入了一个多维框架来阐明常见的基于学习的 IVIF 方法，从视觉增强策略到数据兼容性和任务适应性。我们还对这些方法进行了详细分析，并附有一个查找表来阐明它们的核心思想。此外，我们总结了定量和定性的性能比较，重点关注配准、融合和随后的高级任务。除了技术分析之外，我们还讨论了该领域的潜在未来方向和开放问题。有关更多详细信息，请访问我们的 GitHub 存储库：https://github.com/RollingPlain/IVIF_ZOO.</p></details> |  | [Code Link](https://github.com/RollingPlain/IVIF_ZOO) | Jinyuan Liu |
| **[HyFusion：用于高光谱图像融合的增强型接收场变压器(Arxiv)](http://arxiv.org/abs/2501.04665v3)** | 2025-01-14 | <details><summary>Show</summary><p>高光谱图像 （HSI） 融合解决了从高分辨率多光谱图像 （HR-MSI） 和低分辨率 HSI （LR-HSI） 重建高分辨率 HSI （HR-HSI） 的挑战，鉴于与获取高质量 HSI 相关的高成本和硬件限制，这是一项关键任务。虽然现有方法利用空间和光谱关系，但它们通常受到感受野有限和特征利用率不足的问题，导致性能欠佳。此外，高质量 HSI 数据的稀缺凸显了有效利用数据以最大限度地提高重建质量的重要性。为了解决这些问题，我们提出了 HyFusion，这是一种新颖的双耦合网络 （DCN） 框架，旨在增强跨域特征提取并实现有效的特征图重用。该框架首先通过专门的子网络处理 HR-MSI 和 LR-HSI 输入，这些子网络在特征提取过程中相互增强，保留互补的空间和光谱细节。HyFusion 的核心是利用增强型接收场块 （ERFB），它结合了移动窗口注意力和密集连接来扩大感受野，有效捕获长距离依赖性，同时最大限度地减少信息损失。大量实验表明，HyFusion 在 HR-MSI/LR-HSI 融合中实现了最先进的性能，在保持紧凑的模型尺寸和计算效率的同时，显著提高了重建质量。通过将增强的感受野和特征图重用集成到耦合网络架构中，HyFusion 为资源受限场景中的 HSI 融合提供了实用有效的解决方案，为高光谱成像树立了新的标杆。我们的代码将公开可用。</p></details> |  | None |
| **[DAE-Fuse：用于多模态图像融合的自适应判别自动编码(Arxiv)](http://arxiv.org/abs/2409.10080v2)** | 2024-12-24 | <details><summary>Show</summary><p>在夜间或低能见度环境等极端场景中，实现可靠的感知对于自动驾驶、机器人和监控等应用至关重要。多模态图像融合，特别是集成红外成像，通过结合来自不同模态的互补信息来增强场景理解和决策，从而提供强大的解决方案。然而，目前的方法面临重大限制：基于 GAN 的方法经常产生缺乏精细细节的模糊图像，而基于 AE 的方法可能会偏向于特定模式，从而导致不自然的融合结果。为了应对这些挑战，我们提出了 DAE-Fuse，这是一种新颖的两相判别自动编码器框架，可生成清晰自然的融合图像。此外，我们率先将图像融合技术从静态图像扩展到视频领域，同时保持帧之间的时间一致性，从而提高自主导航所需的感知能力。对公共数据集的广泛实验表明，DAE-Fuse 在多个基准测试中实现了最先进的性能，对医学图像融合等任务具有卓越的泛化性。</p></details> |  | None |
| **[互补优势：利用交叉场频率相关技术进行 NIR 辅助图像去噪(Arxiv)](http://arxiv.org/abs/2412.16645v1)** | 2024-12-21 | <details><summary>Show</summary><p>现有的单图像去噪算法在处理复杂的噪点图像时通常难以恢复细节。近红外 （NIR） 图像的引入为 RGB 图像去噪提供了新的可能性。然而，由于 NIR 和 RGB 图像之间的不一致，现有工作在图像融合过程中仍然难以平衡两个场的贡献。为此，在本文中，我们开发了一种用于 NIR 辅助图像去噪的跨场频率相关开发网络 （FCENet）。我们首先基于对 NIR-RGB 图像对的深入统计频率分析提出了频率相关性先验。先验揭示了 NIR 和 RGB 图像在频域中的互补相关性。利用频率相关先验，我们建立了一个由频率动态选择机制 （FDSM） 和频率穷举融合机制 （FEFM） 组成的频率学习框架。FDSM 在频域中从 NIR 和 RGB 图像中动态选择互补信息，FEFM 加强了 NIR 和 RGB 特征融合过程中对共性和差分特征的控制。对模拟和真实数据的广泛实验验证了我们的方法在图像质量和计算效率方面优于各种最先进的方法。该代码将向公众发布。</p></details> |  | None |
| **[通过具有可编辑模式的 Distilled 3D LUT 网格进行多重曝光图像融合(Arxiv)](http://arxiv.org/abs/2412.13749v1)** | 2024-12-18 | <details><summary>Show</summary><p>随着手持设备成像分辨率的提高，现有的多重曝光图像融合算法难以实时生成具有超高分辨率的高动态范围图像。除此之外，还有一种趋势是设计一个可管理和可编辑的算法，以满足实际应用场景的不同需求。为了解决这些问题，我们引入了 3D LUT 技术，该技术可以在资源受限的设备上实时增强具有超高清 （UHD） 分辨率的图像。然而，由于来自不同曝光率的多个图像的信息融合是不确定的，而这种不确定性极大地考验了 3D LUT 网格的泛化能力。为了解决这个问题并确保模型的稳健学习空间，我们建议使用师生网络来模拟 3D LUT 网格上的不确定性。此外，我们使用隐式表示函数为多重曝光图像融合算法提供了一种可编辑模式，以匹配不同场景下的需求。大量实验表明，我们提出的方法在效率和准确性方面具有很强的竞争力。</p></details> |  | None |
| **[模态解耦就是您所需要的：无监督高光谱图像融合的简单解决方案(Arxiv)](http://arxiv.org/abs/2412.04802v1)** | 2024-12-06 | <details><summary>Show</summary><p>高光谱图像融合 （HIF） 旨在融合低分辨率高光谱图像 （LR-HSI） 和高分辨率多光谱图像 （HR-MSI） 以重建高空间和高光谱分辨率图像。目前的方法通常在没有有效监督的情况下应用两种模态的直接融合，未能完全感知深层模态互补信息，因此，导致对模态间连接的理解肤浅。为了弥合这一差距，我们为无监督 HIF 提出了一种简单有效的解决方案，并假设模态解耦对 HIF 至关重要。我们引入了模态聚类损失，它确保了模态的清晰指导，向模态共享特征解耦，同时避开模态互补特征。此外，我们还提出了一个端到端模态解耦空间光谱融合 （MossFuse） 框架，该框架将跨模态的共享和互补信息解耦，并聚合 LR-HSI 和 HR-MSI 的简洁表示，以减少模态冗余。在多个数据集上的系统实验表明，我们简单有效的方法始终优于现有的 HIF 方法，同时需要的参数要少得多，推理时间更短。</p></details> |  | None |
| **[Hipandas：通过全色图像的图像融合实现高光谱图像联合去噪和超分辨率(Arxiv)](http://arxiv.org/abs/2412.04201v1)** | 2024-12-05 | <details><summary>Show</summary><p>高光谱图像（HSIs）由于成像设备的限制，通常噪声较大且分辨率较低。最近发射的卫星能够同时获取高光谱（HSI）和全色（PAN）图像，从而通过融合全色图像进行去噪和超分辨率处理，恢复高分辨率的清晰高光谱图像。然而，之前的研究将这两个任务视为独立的过程，导致误差积累。本文提出了一个新颖的学习范式——Hyperspectral Image Joint Pan denoising and Pan sharpening (Hipandas)，该方法通过融合噪声较大的低分辨率高光谱图像（LRHS）和高分辨率全色图像（PAN）来重建高分辨率高光谱图像（HRHS）。所提出的零-shot Hipandas框架由引导去噪网络、引导超分辨率网络和全色图像重建网络组成，利用高光谱图像低秩先验和新提出的注重细节的低秩先验。由于这些网络之间的相互连接，训练过程变得复杂，需要采用两阶段训练策略以确保有效训练。通过在模拟和真实数据集上的实验结果表明，所提出的方法优于当前最先进的算法，能够生成更准确、视觉上更令人满意的高分辨率高光谱图像。</p></details> |  | None |
| **[用于红外和可见光图像融合的多尺度信息集成框架(Arxiv)](http://arxiv.org/abs/2312.04328v2)** | 2024-11-20 | <details><summary>Show</summary><p>红外可见光图像融合旨在生成包含源图像强度和细节信息的融合图像，关键问题是有效测量和整合来自同一场景的多模态图像的互补信息。现有的方法大多采用损失函数中的简单权重来确定每种模态的信息保留，而不是自适应地测量不同图像对的互补信息。在这项研究中，我们提出了一个用于红外和可见光图像融合的多尺度双注意力 （MDA） 框架，该框架旨在测量和整合图像和斑块级别结构和损失函数中的互补信息。在我们的方法中，残差下采样块首先将源图像分解为三个尺度。然后，双注意力融合块整合互补信息，并在每个尺度上生成空间和通道注意力图，用于特征融合。最后，通过残差重建块重建输出图像。损失函数由图像级、特征级和补丁级三部分组成，其中图像级和补丁级两部分的计算基于互补信息测量产生的权重。实际上，为了限制输出和红外图像之间的像素强度分布，添加了样式损失。我们的融合结果在不同场景中表现稳健且信息丰富。两个数据集的定性和定量结果表明，我们的方法能够保留来自两种模式的热辐射和详细信息，并与其他最先进的方法相比获得可比的结果。消融实验显示了我们的信息集成架构的有效性，并自适应地测量损失函数中的互补信息保留。</p></details> |  | None | Xinbo Gao |
| **[Rethinking Normalization Strategies and Convolutional Kernels for Multimodal Image Fusion(Arxiv)](http://arxiv.org/abs/2411.10036v1)** | 2024-11-15 | <details><summary>Show</summary><p>多模态图像融合 （MMIF） 旨在整合来自不同模态的信息以获得全面的图像，从而辅助下游任务。然而，现有方法往往优先考虑自然图像融合，并侧重于信息互补和网络训练策略。他们忽视了自然图像融合和医学图像融合之间的本质区别以及基础成分的影响。本文剖析了这两项任务在融合目标、统计特性和数据分布方面的显著差异。基于此，我们重新思考了归一化策略和卷积内核对端到端 MMIF 的适用性。具体来说，本文提出了实例归一化和组归一化的混合，以保持样本独立性并加强内在特征相关性。这种策略促进了丰富特征图的潜力，从而提高了融合性能。为此，我们进一步引入了大核卷积，有效地扩大了感受野，增强了图像细节的保留。此外，所提出的多径自适应融合模块以各种尺度和感受野的特征重新校准解码器输入，确保关键信息的传输。大量实验表明，我们的方法在多种聚变任务中表现出最先进的性能，并显著改善了下游应用。该代码可在 https://github.com/HeDan-11/LKC-FUNet 获取。</p></details> |  | [Code Link](https://github.com/HeDan-11/LKC-FUNet) |
| **[红外-可见光图像的指令驱动融合：为不同的下游任务量身定制(Arxiv)](http://arxiv.org/abs/2411.09387v1)** | 2024-11-14 | <details><summary>Show</summary><p>红外和可见光图像融合技术的主要价值在于将融合结果应用于下游任务。然而，现有方法面临着挑战，例如在同时处理多个下游任务时，训练复杂性增加和单个任务的性能严重受损。为了解决这个问题，我们提出了面向任务的自适应调节 （T-OAR），这是一种专为多任务环境设计的自适应机制。此外，我们还介绍了与任务相关的动态提示注入 （T-DPI） 模块，该模块从用户输入的文本指令中生成特定于任务的动态提示，并将其集成到目标表示中。这将指导特征提取模块生成与下游任务的特定要求更紧密一致的表示。通过将 T-DPI 模块整合到 T-OAR 框架中，我们的方法可以生成根据特定任务要求量身定制的融合图像，而无需单独的训练或特定任务权重。这不仅降低了计算成本，还增强了多个任务的适应性和性能。实验结果表明，该方法在目标检测、语义分割和显著目标检测方面表现出色，表现出较强的适应性、灵活性和任务特异性。这为多任务环境中的图像融合提供了有效的解决方案，突出了该技术在各种应用中的潜力。</p></details> | 10 pages | None | Huafeng Li |
| **[高光谱图像分类的全面调查：从传统模型到 Transformers 和 Mamba 模型的演变(Arxiv)](http://arxiv.org/abs/2404.14955v4)** | 2024-11-14 | <details><summary>Show</summary><p>由于高光谱 （HS） 数据的高维性和复杂性，高光谱图像分类 （HSC） 带来了重大挑战。虽然传统的机器学习 （TML） 方法已证明有效，但它们在实际应用中经常遇到重大障碍，包括最佳特征集的可变性、人工驱动设计中的主观性、固有偏见和方法限制。具体来说，TML 遭受了维度的诅咒、特征选择和提取困难、对空间信息的考虑不足、对噪声的鲁棒性有限、可扩展性问题以及对复杂数据分布的适应性不足。近年来，深度学习 （DL） 技术已成为应对这些挑战的强大解决方案。该调查全面概述了 HSC 的当前趋势和未来前景，强调了从 DL 模型到 Transformer 和 Mamba 模型架构的日益采用的进步。我们系统地回顾了 HSC DL 中的关键概念、方法和最先进的方法。此外，我们研究了基于 Transformer 的模型和 Mamba 模型在 HSC 中的潜力，详细介绍了它们的优势和挑战。探讨了 HSC 的新兴趋势，包括对可解释的 AI 和互作性概念的深入讨论，以及用于图像去噪、特征提取和图像融合的扩散模型。对三个 HS 数据集进行了全面的实验结果，以证实各种常规 DL 模型和 Transformer 的有效性。此外，我们还确定了 HSC 领域的几个开放挑战和相关研究问题。最后，我们概述了旨在提高 HSC 准确性和效率的未来研究方向和潜在应用。</p></details> |  | None |
| **[全天候多模态图像融合：统一框架和 100k 基准测试(Arxiv)](http://arxiv.org/abs/2402.02090v2)** | 2024-11-11 | <details><summary>Show</summary><p>多模态图像融合 （MMIF） 结合了来自不同图像模态的互补信息，以提供对场景的更全面、更客观的解读。然而，现有的 MMIF 方法缺乏抵抗现实世界场景中不同天气干扰的能力，因此无法在自动驾驶等实际应用中发挥作用。为了弥合这一研究差距，我们提出了一个全天候 MMIF 模型。由于天气条件的复杂性和多样性，在这种情况下实现有效的多任务处理尤其具有挑战性。一个关键障碍在于当前深度学习架构的“黑匣子”性质，这限制了它们的多任务处理能力。为了克服这个问题，我们将网络分解为两个模块：融合模块和恢复模块。对于融合模块，我们引入了一个可学习的低秩表示模型，将图像分解为低秩和稀疏分量。这种可解释的特征分离使我们能够更好地观察和理解图像。对于恢复模块，我们提出了一个基于大气散射模型的物理感知清晰特征预测模块，该模块可以从场景照明和反射率中推断出透光率的变化。我们还构建了一个大规模多模态数据集，其中包含 100,000 个图像对，涵盖雨、霾和雪况，涵盖各种退化程度和不同场景，以全面评估恶劣天气下的图像融合方法。在真实世界和合成场景中的实验结果表明，所提算法在细节恢复和多模态特征提取方面表现出色。该代码可在 https://github.com/ixilai/AWFusion 获取。</p></details> |  | [Code Link](https://github.com/ixilai/AWFusion) | Huafeng Li |
| **[基于l0-正则化稀疏编码的多模态图像融合可解释网络(Arxiv)](http://arxiv.org/abs/2411.04519v1)** | 2024-11-07 | <details><summary>Show</summary><p>多模态图像融合 （MMIF） 通过结合从不同模态传感器图像获得的独特和通用特征来增强融合图像的信息内容，从而改进可视化、对象检测和更多任务。在这项工作中，我们为MMIF任务引入了一个名为FNet的可解释网络，该网络基于l0正则化多模态卷积稀疏编码（MCSC）模型。具体来说，为了解决 l0 正则化 CSC 问题，我们开发了一种基于算法展开的 l0 正则化稀疏编码 （LZSC） 块。给定不同的模态源图像，FNet 首先使用 LZSC 块从中分离出独特和常见的特征，然后将这些特征组合起来生成最终的融合图像。此外，我们提出了一个用于逆融合过程的 l0 正则化 MCSC 模型。基于这个模型，我们引入了一个名为 IFNet 的可解释逆融合网络，该网络在 FNet 的训练中使用。大量实验表明，FNet 在五个不同的 MMIF 任务中实现了高质量的融合结果。此外，我们表明 FNet 增强了可见光-热图像对中的下游目标检测。我们还可视化了 FNet 的中间结果，这证明了我们网络的良好可解释性。</p></details> |  | None |
| **[DDF： 一种基于无监督域自适应的遥感图像语义分割的新型双域图像融合策略(Arxiv)](http://arxiv.org/abs/2403.02784v2)** | 2024-10-24 | <details><summary>Show</summary><p>由于存在大量未标注的数据，遥感图像的语义分割是一个具有挑战性的热点问题。事实证明，无监督域适应 （UDA） 在整合来自目标域的非机密信息方面具有优势。但是，在源域和目标域上独立微调 UDA 模型对结果的影响有限。该文提出了一种混合训练策略，以及一种新颖的双域图像融合策略，该策略有效利用了原始图像、转换图像和中间域信息。此外，为了提高伪标签的精度，我们提出了一种伪标签区域特异性权重策略。在 ISPRS Vaihingen 和 Potsdam 数据集上进行的广泛的基准实验和消融研究证实了我们方法的有效性。</p></details> |  | None |
| **[重新思考可见光和红外图像融合的评估(Arxiv)](http://arxiv.org/abs/2410.06811v1)** | 2024-10-09 | <details><summary>Show</summary><p>可见光和红外图像融合 （VIF） 在广泛的高级视觉任务中引起了极大的兴趣，例如对象检测和语义分割。然而，由于缺乏基本实况，对 VIF 方法的评估仍然具有挑战性。本文提出了一种面向分割的评估方法 （SEA），通过结合语义分割任务并利用最新 VIF 数据集中可用的分割标签来评估 VIF 方法。具体来说， SEA 利用能够处理各种图像和类别的通用分割模型来预测融合图像的分割输出，并将这些输出与分割标签进行比较。我们对最近使用 SEA 的 VIF 方法的评估表明，尽管近一半的红外图像表现出比可见光图像更好的性能，但它们的性能与仅使用可见光图像相当甚至不如。进一步的分析表明，与我们的 SEA 最相关的两个指标是基于梯度的融合指标QABF和 Visual Information Fidelity （视觉信息保真度）。在传统的 VIF 评估指标中，当分段标签不可用时，这些指标可以用作代理。我们希望我们的评估将指导新颖实用的 VIF 方法的开发。代码已在 \url{https://github.com/Yixuan-2002/SEA/} 中发布。/p></details> |  | [Code Link](https://github.com/Yixuan-2002/SEA) |

## 期刊
| **题目** | **日期** | **摘要** | **录用信息** | **代码** | **备注**
| --- | --- | --- | --- | --- | --- |
| **[FusionMamba：使用 Mamba 进行多模态图像融合的动态功能增强(Arxiv)](http://arxiv.org/abs/2404.09498v3)** | 2025-02-02 | <details><summary>Show</summary><p>多模态图像融合旨在整合来自不同成像技术的信息，为下游视觉任务生成全面、细节丰富的单张图像。基于局部卷积神经网络 （CNN） 的现有方法难以有效地捕获全局特征，而基于 Transformer 的模型尽管擅长全局建模，但计算成本很高。Mamba 通过利用选择性结构化状态空间模型 （S4） 来有效处理长期依赖关系，同时保持线性复杂性，从而解决了这些限制。在本文中，我们提出了 FusionMamba，这是一种新颖的动态特征增强框架，旨在克服 CNN 和视觉转换器 （ViT） 在计算机视觉任务中面临的挑战。该框架通过集成动态卷积和通道注意力机制，对视觉状态空间模型 Mamba 进行了改进，不仅保留了其强大的全局特征建模能力，还大大减少了冗余，增强了局部特征的表现力。此外，我们还开发了一个名为动态特征融合模块 （DFFM） 的新模块。它将用于纹理增强和视差感知的动态特征增强模块 （DFEM） 与跨模态融合 Mamba 模块 （CMFM） 相结合，后者专注于增强模态间相关性，同时抑制冗余信息。实验表明，FusionMamba 在各种多模态图像融合任务以及下游实验中取得了最先进的性能，证明了其广泛的适用性和优越性。</p></details> | Visual Intelligence | [Code Link](https://github.com/millieXie/FusionMamba) |
| **[MLFuse: 多场景特征联合学习在多模态图像融合中的应用](https://ieeexplore.ieee.org/document/10856398)** | 2025 | <details><summary>Show</summary><p> 多模态图像融合 (MMIF) 需要合成具有详细纹理和突出对象的图像。现有方法倾向于使用一般特征提取来处理不同的融合任务。然而，由于缺乏有针对性的学习途径，这些方法难以跨越各种模式的融合障碍。在这项工作中，我们提出了一个多场景特征联合学习架构 MLFuse，它利用多模态图像的共性来解构融合过程。具体来说，我们构建了一个跨模态知识增强网络，该网络采用多路径标定策略来促进不同图像之间的信息交流。此外，两个专业网络的发展，以维护显着和纹理信息的融合结果。空间 - 光谱域优化网络可以借助空间注意和光谱注意来学习源图像上下文的重要关系。该边缘引导学习网络利用各种接收域的卷积运算来获取图像纹理信息。通过聚合三个网络的输出，得到了期望的融合结果。大量的实验证明了 MLFuse 在红外-可见光图像融合和医学图像融合中的优越性。下游任务 (即目标检测和语义分割) 的出色结果进一步验证了我们方法的高质量融合性能。 </p></details> | TMM2025 |  [Code Link](https://github.com/jialei-sc/MLFuse) | Jinyuan Liu |
| **[用于多焦点图像融合的双路径深度无监督学习](https://ieeexplore.ieee.org/document/10812788)** | 2024-12 | <details><summary>Show</summary><p> 多焦点图像融合 （MFIF） 旨在合并以不同焦距拍摄的多张图像，以创建全聚焦图像。本文介绍了一种用于 MFIF 的完全无监督学习方法，该方法仅使用成对的散焦图像进行端到端训练，绕过了监督学习中对地面事实的需求。与通过融合图像和源图像之间的相似性损失进行现有方法训练不同，我们提出了一个双路径学习框架，包括两个网络：图像融合器和掩码预测器。掩码预测器被建模为不完美融合掩码上的自监督去噪网络，并使用基于掩码的无监督学习方案进行训练。通过深度展开制作的图像定影器利用蒙版预测器的输出来监督每个展开步骤的蒙版生成。此外，我们引入了融合一致性损失，以确保图像融合器和掩码预测器之间的对齐。在广泛的实验中，我们提出的方法显示出优于现有的端到端无监督方法，并且与有监督方法相比具有竞争力。 </p></details> | TMM2025 |  []() |
| **[LFDT-Fusion：用于一般图像融合的潜在特征引导扩散 Transformer 模型](https://www.sciencedirect.com/science/article/pii/S1566253524004172)** | 2024-08 | <details><summary>Show</summary><p> 对于图像融合任务，扩散模型在原始分辨率图像上多次迭代以进行特征映射是低效的。为了解决这个问题，本文提出了一种用于一般图像融合的高效潜在特征引导扩散模型。该模型由一个像素空间自动编码器和一个基于 Transformer 的紧凑扩散网络组成。具体来说，像素空间自动编码器是一种基于 UNet 的新型潜在扩散策略，它通过下采样将输入压缩到低分辨率的潜在空间中。同时，跳跃连接将多尺度中间特征从编码器传输到解码器进行解码，从而保留原始输入的高分辨率信息。与现有的基于 VAE-GAN 的潜在扩散策略相比，所提出的基于 UNet 的策略明显更加稳定，并且无需对抗优化即可生成高度详细的图像。基于 Transformer 的扩散网络由一个去噪网络和一个融合头组成。前者捕获长程扩散依赖关系并学习分层扩散表示，而后者则促进扩散特征交互以理解复杂的跨域信息。此外，扩散模型在噪声水平、降噪步骤和采样器选择方面的改进在六项图像融合任务中产生了卓越的融合性能。所提出的方法说明了定性和定量的优势，公共数据集和工业环境中的实验结果都证明了这一点。 </p></details> | Inf. Fusion2025 |  [Code](https://github.com/BOYang-pro/LFDT-Fusion) |
| **[用于多焦点图像融合的展开耦合卷积稀疏表示](https://www.sciencedirect.com/science/article/abs/pii/S1566253525000478)** | 2025-01 | <details><summary>Show</summary><p> 多焦点图像融合 （MFIF） 旨在从使用不同焦距设置捕获的同一场景的多个部分聚焦图像生成全聚焦图像。在本文中，我们提出了一种用于 MFIF 的耦合卷积稀疏表示 （CCSR） 模型。所提出的 CCSR 模型不是通过迭代阈值算法来解决，而是使用深度展开技术展开为可学习的神经网络（称为 CCSR-Net），从而继承了传统的基于 SR 模型的方法（即坚实的理论基础和良好的可解释性）和基于深度学习 （DL） 的方法（即、有效的特征学习能力和通过 GPU 加速实现的高推理速度）。然后，基于 CCSR-Net，提出了一种具有良好可解释性的新型 MFIF 方法。此外，鉴于图像内容的复杂性和各种尺度上特征的存在，我们通过基本细节分解策略进一步将 CCSR 模型扩展到名为 MCCSR 的多分量版本，旨在实现输入源图像的更有效表示。MCCSR 模型也被展开为 MFIF 的可学习神经网络（称为 MCCSR-Net）。在三个流行的 MFIF 数据集上的实验结果表明，所提出的 CCSR-Net 和 MCCSR-Net 在视觉质量和客观评估方面都能获得最先进的性能。 </p></details> | Inf. Fusion2025 |  [Code](https://github.com/yuliu316316/CCSR-Net-Fusion) |
| **[DFuse：一种基于跨领域知识蒸馏的高级视觉任务驱动的红外与可见光图像融合方法](https://www.sciencedirect.com/science/article/pii/S156625352500017X)** | 2025-01 | <details><summary>Show</summary><p> 了增强融合特征的全面性并满足高级视觉任务的要求，一些融合方法试图通过与高级语义特征直接交互来协调融合过程。然而，由于高级语义域和融合表示域之间的显著差异，有可能提高直接交互的协作方法的有效性。为了克服这一障碍，该文提出一种基于跨域知识蒸馏的高级视觉任务驱动的红外与可见光图像融合方法，简称KDFuse。KDFuse 通过跨域知识蒸馏将多任务感知表示引入同一领域。通过促进语义信息和融合信息在等效级别的交互，它有效地缩小了语义域和融合域之间的差距，实现了多任务协同融合。具体来说，为了获得指导融合网络所必需的高级语义表示，通过多域交互蒸馏模块 （MIDM） 建立教学关系以实现多任务协作。多尺度语义感知模块 （MSPM） 旨在通过跨领域知识蒸馏来学习捕获语义信息的能力，构建语义细节集成模块 （SDIM） 以将融合级语义表示与融合级视觉表示集成。此外，为了平衡融合过程中的语义和视觉表示，在损失函数中引入了傅里叶变换。广泛的综合实验证明了所提出的方法在图像融合和下游任务中的有效性。 </p></details> | Inf. Fusion2025 |  [Code](https://github.com/lxq-jnu/KDFuse) |
| **[高光谱和多光谱图像融合：当模型驱动与数据驱动策略相遇时]()** | 2024-11 | <details><summary>Show</summary><p> 高光谱图像 （HSI） 和多光谱图像 （MSI） 融合旨在将高分辨率 MSI （HR MSI） 与低分辨率 HSI （LR HSI） 相结合，从而产生包含前者空间分辨率和后者光谱分辨率的融合图像。这种方法为直接获取高分辨率 HSI （HR HSI） 提供了一种经济高效的替代方案。在这项调查中，我们为寻求对主题有更深入见解的学生和专业人士提供了广泛的文献综述。我们深入研究了现有的 HSI-MSI 融合方法，并揭示了一系列方法，从模型驱动技术（扩展 CS 和 MRA、贝叶斯、矩阵分解和张量表示）到数据驱动方法（CNN、GAN 和 Transformer）和模型数据驱动方法（模型引导网络和半监督或无监督方法）。这项探索旨在优化各种应用的融合策略。本文不仅全面概述了 HSI-MSI 融合策略，还总结和对比了它们的独特特征、优势和局限性。此外，它还审查了图像质量评估指数（完整参考和无参考）和广泛使用的数据集。此外，利用混合数据、大视场卫星数据和真实卫星数据对，对 3 种策略的各种算法进行了降分和全分辨实验对比分析。最后，本文确定了尚未解决的挑战，并概述了这一不断发展的领域未来的潜在研究方向。 </p></details> | Inf. Fusion2025 |  []() |
| **[SMAE-Fusion：将显著性感知掩蔽自动编码器与混合注意力转换器集成，以实现红外-可见光图像融合](https://www.sciencedirect.com/science/article/pii/S1566253524006195)** | 2024-12 | <details><summary>Show</summary><p> 红外-可见光图像融合 （IVIF） 的目标是从多种模态生成合成图像，以增强视觉表示并支持高级视觉任务。然而，现有的大多数 IVIF 方法主要侧重于增强视觉效果，而高级任务驱动方法受到特定感知网络和复杂训练策略的限制，导致不同场景下的灵活性有限。掩码图像建模作为一种强大的自我监督训练范式，能够学习适用于各种下游任务的稳健特征表示。为此，本研究引入了 SMAE-Fusion，这是一种为红外-可见光图像融合量身定制的显著性感知掩蔽自动编码器框架。最初，SMAE-Fusion 采用显著感知的动态掩码策略，并在重建阶段利用自我监督的预训练范式自适应地强调显着区域和语义细节，从而改进特征表示以减轻上下游任务之间的语义差距。此外，SMAE-Fusion 的主干集成了混合注意力增强转换器，通过利用卷积和自注意力机制来促进局部和全局特征之间的有效交互。此外，构建了渐进式特征融合模块，通过自注意力对齐和交叉注意力互补，逐步优化跨模态特征的整合。在各种公共数据集上进行的综合实验表明，SMAE-Fusion 在融合质量和下游任务增强方面取得了最先进的性能。 </p></details> | Inf. Fusion2025 |  []() |
| **[TextFusion：揭示文本语义在可控图像融合中的力量](https://www.sciencedirect.com/science/article/pii/S1566253524005682)** | 2024-12 | <details><summary>Show</summary><p> 高级图像融合技术旨在通过整合源输入提供的互补信息来合成融合结果。然而，不同模态捕获和表示同一场景的方式的固有差异对设计稳健且可控的融合过程构成了重大挑战。我们认为，从文本模态中整合高级语义信息可以缓解这个问题，允许生成文本条件融合内容 （TextFusion） 以更可控的方式增强可视化或支持下游任务。为此，我们采用视觉和语言模型来建立从粗到细的关联机制。利用关联映射，引入了仿射融合单元，以在特征级别无缝组合文本和视觉模态。此外，我们引入了一种文本注意力机制，旨在改进传统的基于平均值的图像融合评估指标，这些指标通常无法准确捕捉融合结果的真实质量。为了鼓励更广泛地采用我们的可控图像融合框架，我们发布了一个公开可用的文本注释图像融合数据集 IVT。广泛的实验表明，我们的方法在主观和客观评估方面始终优于传统的基于外观的融合方法。 </p></details> | Inf. Fusion2025 |  [Code](https://github.com/AWCXV/TextFusion) |
| **[DAFusion: 用于红外和可见光图像的退化感知引导融合网络](https://www-sciencedirect-com-s-225.libdb.csu.edu.cn/science/article/pii/S1566253525000041?via%3Dihub)** | 2025-01 | <details><summary>Show</summary><p> 大多数 IVIF 方法只关注视觉特征融合，而忽略了降级的场景信息，这会导致次优解，不能完全反映隐式场景信息。为了应对这一挑战，我们开发了一种用于红外和可见光图像的退化感知融合网络。通过学习隐式退化估计，我们的模型不仅有效地整合了来自源图像的互补信息，而且增强了其对场景退化的鲁棒性。我们的方法假设所有源图像都包含不同程度的降级。基于这个假设，我们使用编码器变体和高质量、无退化的图像构建了稳定的正样本和动态负样本，从而驱动模型在未配对表示特征的对比学习过程中识别和优化源图像中的退化。此外，协同细化融合模块 （CrFM） 利用了表示特征和源信息之间的相互依赖性，使其能够挖掘每个源中的专业信息以及跨源的互补信息。这有助于有效的特征聚合，同时减少融合过程中的信息丢失。为了进一步增强模型，我们引入了图像级显著性掩码和特征级能量变化掩码来减少解决方案域，鼓励模型优先考虑内在源内容，尤其是被退化掩盖的细节。静态数据统计和高级视觉任务的广泛实验验证了所提方法的优越性，其强大的抗降解能力使其在面对未知降解时比其他 SOTA 方法更稳定. </p></details> | Inf. Fusion2025 |  [Code](https://github.com/wang-x-1997/DAFusion) |
| **[DGFD：用于图像融合和弱光目标检测的双图卷积网络](https://www-sciencedirect-com-s-225.libdb.csu.edu.cn/science/article/pii/S1566253525000983?via%3Dihub)** | 2025-02 | <details><summary>Show</summary><p> 传统的卷积运算主要集中在局部特征提取上，这可能会导致全局特征丢失。然而，目前用于提取全局特征的融合方法表现出很高的时间复杂度，并且难以捕获长距离依赖关系。本文构建了一个双图卷积神经网络，基于红外和可见光图像的模态结构进行跨模态图推理。具体来说，为了聚合不同模态图像的全局和局部特征，提出了一种上下文图卷积模块，该模块允许网络结构适应不同模态图像的特征，并促进不同层次特征的提取。该文还提出了一种内容图卷积模块，用于构建红外和可见光图像之间的关联关系，无需人工设计干扰即可实现特征融合。此外，融合特征被馈送到一个统一的框架中，以集成融合和检测任务。大量的定性和定量实验表明，所提出的统一网络显著降低了时间复杂度，提高了弱光条件下的检测性能。 </p></details> | Inf. Fusion2025 |  []() |
| **[KDFuse：一种基于跨领域知识蒸馏的高级视觉任务驱动的红外与可见光图像融合方法](https://www-sciencedirect-com-s-225.libdb.csu.edu.cn/science/article/pii/S156625352500017X?via%3Dihub)** | 2025-01 | <details><summary>Show</summary><p> 为了增强融合特征的全面性并满足高级视觉任务的要求，一些融合方法试图通过与高级语义特征直接交互来协调融合过程。然而，由于高级语义域和融合表示域之间的显著差异，有可能提高直接交互的协作方法的有效性。为了克服这一障碍，该文提出一种基于跨域知识蒸馏的高级视觉任务驱动的红外与可见光图像融合方法，简称KDFuse。KDFuse 通过跨域知识蒸馏将多任务感知表示引入同一领域。通过促进语义信息和融合信息在等效级别的交互，它有效地缩小了语义域和融合域之间的差距，实现了多任务协同融合。具体来说，为了获得指导融合网络所必需的高级语义表示，通过多域交互蒸馏模块 （MIDM） 建立教学关系以实现多任务协作。多尺度语义感知模块 （MSPM） 旨在通过跨领域知识蒸馏来学习捕获语义信息的能力，构建语义细节集成模块 （SDIM） 以将融合级语义表示与融合级视觉表示集成。此外，为了平衡融合过程中的语义和视觉表示，在损失函数中引入了傅里叶变换。广泛的综合实验证明了所提出的方法在图像融合和下游任务中的有效性。 </p></details> | Inf. Fusion2025 |  [Code](https://github.com/lxq-jnu/KDFuse) |
| **[SSEFusion：使用 Mamba 和动态脉冲神经网络进行多模态医学图像融合的显著语义增强](https://www-sciencedirect-com-s-225.libdb.csu.edu.cn/science/article/pii/S1566253525001046?via%3Dihub)** | 2025-02 | <details><summary>Show</summary><p> 多模态医学图像融合技术增强了医学表征，并在临床诊断中发挥着至关重要的作用。然而，由于病变的随机性和器官结构的复杂，融合医学图像仍然是一个挑战。尽管最近提出了许多融合方法，但大多数方法都难以在保留显着语义特征的同时有效地建立全局上下文依赖性，从而导致关键医学信息的丢失。因此，我们提出了一种新的显著语义增强融合 （SSEFusion） 框架，其关键组件包括一个结合了 Mamba 和脉冲神经网络 （SNN） 模型的双分支编码器 （Mamba-SNN 编码器）、特征交互注意力 （FIA） 块和配备细节增强 （DE） 块的解码器。在编码器中，基于 Manba 的分支引入了视觉状态空间 （VSS） 块，以有效地建立全局依赖关系并提取全局特征，从而有效识别病变区域。同时，基于 SNN 的分支动态提取细粒度的显著特征，以增强复杂结构中医学语义信息的保留。全局特征和细粒度显著特征在语义上交互，通过 FIA 块实现特征互补。得益于 DE 块，SSEFusion 可生成具有突出边缘纹理的融合图像。此外，我们提出了一种基于 leaky-integrate-and-fire （LIF） 神经元的显着语义损失，以增强提取关键特征的指导。广泛的融合实验表明，SSEFusion 在显着的医学语义信息保留方面优于最先进的融合方法。 </p></details> | Inf. Fusion2025 |  [Code](https://github.com/Shiqiang-Liu/SSEFusion) |
| **[用于遥感全色锐化的深度时空-光谱融合Transformer](https://www-sciencedirect-com-s-225.libdb.csu.edu.cn/science/article/pii/S1566253525000533?via%3Dihub)** | 2025-02 | <details><summary>Show</summary><p> 全色锐化是在高分辨率全色 （PAN） 与低分辨率多光谱 （LR-MS） 图像融合过程中重建空间光谱特性以生成高分辨率多光谱 （HR-MS） 图像的任务。最近的方法通常对空间和光谱属性进行建模，并使用端到端深度学习网络将它们融合在一起，而这些网络没有考虑到它们与任务相关的关键关系先验。在本文中，我们提出了一种新的深度空间-光谱融合变压器 （SSFT），其灵感来自两个与任务相关的关键发现：（i） 与空间属性相关的先验，即 PAN 图像本身可以提供足够的空间属性来重建目标 HR-MS 图像所需的空间属性;（ii） 与光谱特性相关的先验，即 LR-MS 和 PAN 都应该参与目标 HR-MS 图像的光谱特性建模过程。具体来说，我们的方法由三个新颖的模块组成：傅里叶引导的谱重建 （FGSR） 模块创新性地将复杂的特征交互策略应用于傅里叶表示2LR-MS 和 PAN 图像，用于重建显示目标 HR-MS 光谱特性的振幅;Top-k 空间重建 （TKSR） 块利用Top-k从 PAN 图像中选择最相关的空间区域的选择策略，用于对目标 HR-MS 所需的空间特性进行建模;和空间-光谱融合 （SSF） 块重新加权值矩阵，从而实现空间和光谱特性的无缝集成。广泛的实验表明，我们的 SSFT 在广泛使用的 WorldView-3、QuickBird 和 GaoFen-2 数据集上明显优于最先进的 （SOTA） 方法。 </p></details> | Inf. Fusion2025 |  [Code](https://github.com/Florina2333/SSFT) |
| **[用于全色锐化的双域不确定性门控递归网络](https://www-sciencedirect-com-s-225.libdb.csu.edu.cn/science/article/pii/S1566253525000119?via%3Dihub)** | 2025-01 | <details><summary>Show</summary><p> 全色锐化旨在整合卫星图像不同模态的互补信息，例如纹理丰富的 PAN 图像和多光谱 （MS） 图像，为各种实际任务生成信息量更大的融合图像。目前，大多数基于深度学习的全色锐化技术主要集中在开发各种精心设计的架构上，以增强其表示能力。尽管取得了重大进步，但这些启发式和复杂的网络设计导致模型缺乏可解释性，并且在实际场景中表现出较差的泛化能力。为了缓解这些问题，我们提出了一种简单而有效的空间频率（双域）不确定性门控渐进融合框架，用于全色锐化，称为 BUGPan。具体来说，BUGPan 的主体由多个不确定性门控递归模块 （UGRM） 组成，这些模块负责不同空间分辨率的跨模态表示学习。与之前执行固定和手动设置递归次数的递归设计相比，UGRM 引入了一种创新的时频不确定性门控递归机制，具有两个关键设计，即双域不确定性估计 （BUE） 和不确定性感知门控 （UAG）。这种机制战略性地编排了特征的递归嵌入，并根据特定的结果上下文定制了过程，从而实现了更透明的特征表示学习。据我们所知，这不仅是在全色锐化中引入空间和频率不确定性的第一次尝试，而且还扩展了不确定性在新颖的功能机制设计中的作用。广泛的实验结果突出了所提出的 BUGPan 的优越性，在多个卫星数据集的定性和定量上都超过了其他最先进的方法。特别值得注意的是，它能够有效地推广到真实场景和新的卫星数据。 </p></details> | Inf. Fusion2025 |  [Code](https://github.com/coder-JMHou/BUGPan) |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> | Inf. Fusion2025 |  []() |
| **[ACFNet：用于红外和可见光图像融合的自适应交叉融合网络](https://www.sciencedirect.com/science/article/abs/pii/S0031320324008495)** | 2024-11 | <details><summary>Show</summary><p> 考虑到图像融合的前景，需要引导融合适应下游视觉任务。在本文中，我们提出了一种自适应交叉融合网络 （ACFNet），它利用自适应方法来融合红外和可见光图像，解决跨模态差异以提高目标检测性能。在 ACFNet 中，设计了一个分层交叉融合模块，以丰富重建图像各个级别的特征。此外，提出了一种特殊的自适应选通模块，以自适应的方式实现特征融合，从而获得融合的图像，而不受人工设计的干扰。广泛的定性和定量实验表明，ACFNet 优于当前最先进的融合方法，并在保留目标信息和纹理细节方面取得了出色的结果。当融合框架与对象检测框架结合使用时，有可能显著提高弱光条件下对象检测的精度。 </p></details> | PR2025 |  []() |
| **[SpTFuse: 用于红外和可见光图像融合的 SAM 引导多级协同Transformer](https://www.sciencedirect.com/science/article/abs/pii/S0031320325000512)** | 2025-01 | <details><summary>Show</summary><p> 图像融合的主要价值在于更有效地支持下游任务。然而，现有方法的融合表示包含的语义信息不足，从而削弱了与后续任务的兼容性。为了克服这个问题，本稿提出了一种用于红外和可见光图像融合的 SAM 引导多级协同变压器，称为 SpTFuse。考虑到 Segment Anything Model （SAM） 强大的零镜头泛化能力，引入基于 SAM 的语义先验分支与多尺度视觉表示分支交互，以提高融合表示的完整性和兼容性。交互过程分为三个级别，以逐步集成多分支信息。在第一级，设计了一个模态融合模块（IEB），带有一个单步协作变压器（SCT）和一个模态集成模块（MIM）。SCT 聚合了语义先验和视觉表示的相关特征。然后，MIM 旨在融合 SAM 语义先验引导的多模态视觉表示。为了平衡视觉和语义表示以获得完整的融合表示，在以下级别构建了模态内交互块 （IAB）。具体来说，IAB 由双通道协作变压器 （DCT） 和语义增强模块 （SEM） 组成。DCT 以级联方式构建两条路径，其中前一条协作路径继续获取语义先验，而视觉细化路径在保持语义完整性的同时平衡视觉信息。随后，SEM 进一步结合 Semantic Before 以增强融合表示的完整性。为了减少图像恢复过程中丢弃的语义信息，通过语义补偿块将前一层的协同信息纳入相应的解码器层。最后，所提出的损失函数包括语义先验损失、梯度损失和强度损失。实验表明，SpTFuse 不仅取得了有效的融合结果，而且在分割和检测等下游任务中也显示出明显的优势。 </p></details> | PR2025 |  [Code](https://github.com/lxq-jnu/SpTFuse) |
| **[空间-光谱展开网络，具有多光谱和高光谱图像融合的相互引导](https://www.sciencedirect.com/science/article/abs/pii/S0031320324010288)** | 2024-12 | <details><summary>Show</summary><p> 融合来自不同模态的低空间分辨率高光谱 （LR HS） 和高空间分辨率多光谱 （HR MS） 图像，旨在获得高空间分辨率高光谱 （HR HS） 图像。然而，大多数基于深度神经网络 （DNN） 的方法都忽略了空间域和谱域之间的相关性，导致融合性能有限。为了解决这个问题，我们提出了具有相互引导的时空-光谱展开网络 （SMGU-Net）。具体来说，将源图像中不同模态的信息视为相互补充的组成部分，以推导出重建模型。然后，使用半二次分割和梯度下降算法对模型进行优化，并将其展开到一个网络中，该网络利用 DNN 的强大学习能力来探索深度特征空间中的更多潜在信息。通过这种方式，网络实现了跨模态信息的交互和互补性，从而生成融合图像。在四个基准数据集上进行了实验，以证明 SMGU-Net 的有效性。 </p></details> | PR2025 |  [Code]( https://github.com/yansql/SMGU-Net) |
| **[AMLCA：用于可见光和红外图像融合的加法多层卷积引导交叉注意力网络](https://www.sciencedirect.com/science/article/abs/pii/S0031320325001281)** | 2025-02 | <details><summary>Show</summary><p> 多模态图像融合广泛用于多光谱信号的处理，例如。G.，可见光和红外图像，旨在通过组合来自不同波段的互补信息来创建信息丰富的融合图像。当前的融合方法在从传感器中提取互补信息，同时保留局部细节和全局依赖关系方面面临着重大挑战。为了应对这一挑战，我们提出了一种用于可见光和红外图像融合的加法多层卷积引导交叉注意力网络 （AMLCA），它由两个子模态 i 组成。e.，加法交叉注意力模块 （ACAM） 和小波卷积导向变压器模块 （WCGTM）。具体来说，前者通过使用加法交叉注意力机制增强了特征交互并捕获全局整体信息，而后者则依靠小波卷积来引导转换器，增强了对两个来源细节的保留，并改进了局部细节信息的提取。此外，我们提出了一种多层融合策略，该策略利用了各个层中隐藏的互补特征。因此，AMLCA 可以有效地从本地详细信息和全局依赖关系中提取互补信息，从而显著提高整体性能。对公共数据集的广泛实验和消融分析证明了 AMLCA 的优越性和有效性。 </p></details> | PR2025 |  [Code](https://github.com/Wangdl2000/AMLCA-code) |
| **[MMAE：一种基于掩码注意力机制的通用图像融合方法](https://www.sciencedirect.com/science/article/abs/pii/S0031320324007921)** | 2024-10 | <details><summary>Show</summary><p> 图像作为数据的重要载体，包含着海量的信息。图像融合的目的是将源图像中的信息集成到单个图像中。由于源图像来自同一场景，因此它们之间存在许多冗余信息。常见的熔融方法不会过滤这些信息，熔融过程经常受到干扰。这会导致重建的融合图像的质量下降。针对这一问题，该文探索了信息过滤和融合控制的策略，提出了一种基于掩码注意力机制的通用图像融合方法。可分为预训练阶段和正式融合阶段。在预训练阶段，生成粗粒度的掩码映射，用于改进掩码自动编码器和掩码注意力机制。在图像形式融合阶段，在粗粒度掩码映射的帮助下，掩码自动编码器改变了随机掩码的过程，丢弃了源图像之间的冗余特征。同时，掩码注意力机制侧重于各种源图像之间的区别，并保留有效的互补信息。在不同模态数据集上的定性和定量扩展实验验证了该模型在多焦点图像融合、红外可见光图像融合和医学图像融合中的适用性。我们的方法在所有这些任务中都取得了出色的性能，并且比现有的融合方法表现得更好。 </p></details> | PR2025 |  [Code](https://github.com/xiangxiang-wang/MMAE) | 独立参数，已跑通 |
| **[SSDFusion：一种用于可见光和红外图像融合的场景语义分解方法](https://www-sciencedirect-com-s-225.libdb.csu.edu.cn/science/article/pii/S0031320325001177?via%3Dihub)** | 2025-02 | <details><summary>Show</summary><p> 可见光和红外图像融合旨在生成具有全面场景理解和详细上下文信息的融合图像。然而，现有方法通常难以充分处理不同模态之间的关系并针对下游应用进行优化。为了应对这些挑战，我们提出了一种基于场景语义分解的新型可见光和红外图像融合方法，称为 SSDFusion。我们的方法采用多级编码器融合网络，融合模块实现所提出的场景语义分解和融合策略，分别提取和融合场景相关和语义相关组件，并将融合的语义注入场景特征中，丰富融合特征中的上下文信息，同时保持融合图像的保真度。此外，在训练过程中，我们进一步融入了元特征嵌入，将编码器融合网络与下游应用网络连接起来，增强了我们的方法提取语义、优化融合效果以及服务语义分割等任务的能力。大量实验表明，SSDFusion 实现了最先进的图像融合性能，同时增强了语义分割任务的结果。我们的方法弥合了基于特征分解的图像融合与高级视觉应用之间的差距，为多模态图像融合提供了更有效的范式。 </p></details> | PR2025 | [Code](https://github.com/YiXian-Xiao/SSDFusion) |
| **[通过基于密集连接的双编码器实现红外和可见光图像融合](https://www-sciencedirect-com-s-225.libdb.csu.edu.cn/science/article/pii/S0031320325001360?via%3Dihub)** | 2025-02 | <details><summary>Show</summary><p> 针对红外和可见光图像融合过程中容易出现的因梯度特征丢失而导致的信息丢失和边缘模糊的问题，该文提出了一种基于密集连通性的双编码器图像融合方法（DEFusion）。所提出的方法通过不同的方式处理红外和可见光图像，从而保证最好地保留原始图像的特征。构建了一种新的渐进融合策略，以确保网络能够更好地捕获可见光图像中存在的详细信息，同时最大限度地减少红外图像的梯度损失。此外，本文提出了一种包含梯度损失和内容损失的新型损失函数，确保融合结果同时考虑源图像的详细信息和梯度，以促进融合过程。在 TNO 和 RoadScene 数据集上使用最先进方法的实验结果验证了所提方法在大多数指数中表现出优异的性能。融合图像表现出出色的主观对比度和清晰度，提供强烈的视觉感知。比较实验结果表明，该方法在泛化和鲁棒性方面表现出良好的特性。 </p></details> | PR2025 |  []() |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> | PR2025 |  []() |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> | PR2025 |  []() |
| **[VDMUFusion：一种基于扩散模型的多功能无监督图像融合框架](https://ieeexplore.ieee.org/document/10794610)** | 2024-12 | <details><summary>Show</summary><p> 图像融合有助于将来自同一场景的各种源图像的信息集成到合成图像中，从而有利于感知、分析和理解。最近，扩散模型在计算机视觉领域展示了令人印象深刻的生成能力，这表明在图像融合中具有巨大的应用潜力。扩散模型中的正向过程需要逐渐向原始数据添加噪声。然而，典型的无监督图像融合任务（例如，红外-可见光、医学和多重曝光图像融合）缺乏地面实况图像（对应于扩散模型中的原始数据），从而阻止了扩散模型的直接应用。为了解决这个问题，我们提出了一种基于扩散模型的多功能无监督图像融合框架，称为 VDMUFusion。在所提出的方法中，我们通过将图像融合公式化为加权平均过程并建立关于扩散模型中噪声的适当假设，将融合问题集成到扩散采样过程中。为了简化训练过程，我们提出了一个多任务学习框架，它取代了原来的噪声预测网络，允许同时预测噪声和融合权重。同时，我们的方法在各种融合任务中采用联合训练，与单个任务的训练相比，这显着提高了噪声预测的准确性并产生了更高质量的融合图像。大量的实验结果表明，所提出的方法在各种图像融合任务中提供了非常有竞争力的性能。 </p></details> | TIP2025 |  [Code](https://github.com/yuliu316316/VDMUFusion) | 迭代2000次，推理速度极慢，近十分钟一对 |
| **[MaeFuse：通过指导培训，使用预训练的掩蔽自动编码器传输 Omni 特征，以实现红外和可见光图像融合](https://ieeexplore.ieee.org/document/10893688)** | 2025-02 | <details><summary>Show</summary><p> 在本文中，我们介绍了 MaeFuse，这是一种专为红外和可见光图像融合 （IVIF） 设计的新型自动编码器模型。现有的图像融合方法通常依赖于训练与下游任务相结合来获得高级视觉信息，从而有效地强调目标对象并在视觉质量和特定任务应用中提供令人印象深刻的结果。我们的模型名为 MaeFuse ，它不是由下游任务驱动，而是利用了掩码自动编码器 （MAE） 的预训练编码器，该编码器为低级重建和高级视觉任务提供了全向特征提取，以低成本获得感知友好的特征。为了消除不同模态特征的域差距和 MAE 编码器引起的阻塞效应，我们进一步开发了一种引导式训练策略。这种策略经过精心设计，以确保融合层无缝适应编码器的特征空间，从而逐渐增强融合性能。所提出的方法可以促进来自红外和可见光模态的特征向量的全面整合，从而保留每个模态中固有的丰富细节。MaeFuse 不仅在融合技术领域引入了新颖的视角，而且在各种公共数据集中都以令人印象深刻的性能脱颖而出。 </p></details> | TIP2025 |  [Code](https://github.com/Henry-Lee-real/MaeFuse) |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> | TIP2025 |  []() |
| **[用于多焦点图像融合的焦点亲和力感知和超分辨率嵌入](https://ieeexplore.ieee.org/document/10461111)** |  | <details><summary>Show</summary><p> 尽管在多焦点图像融合方面取得了显著的成就，但大多数现有方法只有在给定源图像的分辨率较低时才能生成低分辨率图像。显然，独立进行图像融合和图像超分辨率是一种天真的策略。但是，如果第一步的结果与伪影相遇，则这种两步方法将不可避免地在最终结果中引入和放大伪影。为了解决这个问题，在本文中，我们提出了一种新的方法，可以在一个框架中同时实现图像融合和超分辨率，避免了融合和超分辨率的逐步处理。由于小感受野可以区分细节区域像素的聚焦特性，而大感受野对平滑区域的像素更鲁棒，因此首先提出了一个子网络来计算不同类型感受野下特征的亲和力，有效地提高了聚焦像素的可区分性。同时，为了防止失真，该文还提出了一种基于梯度嵌入的超分辨率子网络，其中综合考虑了浅层、深层和梯度映射的特征，从而得到了高分辨率的上采样图像。与现有独立实现融合和超分辨率的方法相比，我们提出的方法直接以并行方式完成了这两项任务，避免了因图像融合或超分辨率输出不佳而造成的伪影。在真实数据集上进行的实验证实了我们提出的方法与最先进的方法相比的优越性。 </p></details> | TNNLS2025 |  []() |
| **[STFuse：通过半监督迁移学习进行红外和可见光图像融合](https://ieeexplore.ieee.org/document/10312808)** |  | <details><summary>Show</summary><p> 红外和可见光图像融合 （IVIF） 旨在获得包含有关源图像的互补信息的图像。然而，在缺乏基本事实且不借用先验知识的情况下，定义源图像之间的互补信息是具有挑战性的。因此，我们提出了一种基于 IVIF 的半监督迁移学习方法，称为 STFuse，旨在将知识从信息源域转移到目标域，从而打破上述限制。我们方法的关键方面是从多焦点图像融合 （MFIF） 任务中借用监督知识，并使用指导损失过滤掉特定于任务的属性知识Lg，这激发了它在 IVIF 任务中的跨任务使用。利用这种跨任务知识有效缓解了缺乏对融合性能的基本事实的限制，并且在监督知识的约束下互补表达能力比先验知识更具指导性。此外，我们设计了一个跨特征增强模块 （CEM），它利用自我注意和互注意特征来指导每个分支细化特征，然后促进跨模态互补特征的集成。广泛的实验表明，与其他最先进的方法相比，我们的方法在视觉质量和统计指标以及高级视觉任务的对接方面具有良好的优势。 </p></details> | TNNLS2025 |  []() |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> | TNNLS2025 |  []() |
| **[AFDFusion：一种用于多模态图像的自适应频率解耦融合网络](https://libdb.csu.edu.cn/https/vpn/154/P75YPLUUMNVXK5UDMWTGT6UFMN4C6Z5QNF/science/article/pii/S0957417424025612)** |  | <details><summary>Show</summary><p> 多模态图像融合的目标是通过整合有关不同模态优点的互补信息（例如，红外图像的显着强度和可见光图像的细节纹理）来创建提供全面场景描述并符合视觉感知的单个图像。尽管一些作品探索了多模态图像的解耦表示，但它们在复杂的非线性关系、精细的模态解耦和噪声处理中挣扎。为了解决这个问题，我们提出了一个自适应频率解耦模块，通过动态调整内核的可学习低频权重来感知跨模态之间的关联不变量和固有特异性。具体来说，我们利用对比学习损失来限制特征解耦的解空间，以学习多模态图像中不变性和特异性的表示。基本思想是：在解耦中，在表示空间中相似的低频特征应该彼此拉得更近，表示结合不变量，而高频被推得更远，也表示内在特异性。此外，我们的框架中引入了多阶段训练方式，以实现解耦和融合。第一阶段，具有相同架构但参数不同的 MixEncoder 和 MixDecoder 被训练为在对比自监督机制的监督下执行解耦和重建。阶段 II，增加两个特征融合模块，将不变特征和特定特征进行融合，输出融合后的图像。广泛的实验表明，所提出的方法在对两个多模态图像融合任务的定性和定量评估方面优于最先进的方法。 </p></details> | ESWA2025 |  []() |
| **[基于模型的红外可见光图像融合网络，协同优化](https://www.sciencedirect.com/science/article/pii/S0957417424025065)** | 2024-11 | <details><summary>Show</summary><p> 红外和可见光图像融合的主要目标是将来自多模态图像的信息融合成包含突出目标和丰富细节的融合图像。现有的融合方法要么主要依赖于缺乏可解释性和可推广性的 “黑盒 ”模型，要么专注于提高图像的视觉吸引力，而忽略了语义信息。为了解决这些缺点，本文介绍了一种面向任务的红外可见光图像融合网络，该网络将基于模型和数据驱动的正则化与协同优化相结合。首先，基于展开的协同优化公式，我们设计了一个循环协同训练策略，将图像融合和语义分割模块级联，从而用语义信息增强内容信息。然后，考虑到深度神经网络的高计算效率和传统优化模型的可解释性，我们将一个经过充分研究的融合算法显式编码到一个展开的自编码器网络中，该网络具有用于融合模块的双私有编码器。此外，我们的模型很紧凑，可以在资源有限的情况下提供有效的解决方案。对图像融合和语义感知评估的实验结果进行全面的定性和定量分析表明，所提出的方法完全保留了热目标和背景纹理细节，在图像质量和高级语义方面超越了最先进的替代方案。 </p></details> | ESWA2025 |  []() |
| **[MixFuse：用于多模态图像融合的迭代混合注意力Transformer](https://www.sciencedirect.com/science/article/pii/S0957417424022942)** | 2024-10 | <details><summary>Show</summary><p> 多模态图像融合在各种视觉系统中起着至关重要的作用。然而，现有的方法通常涉及多阶段的管道，即特征提取、集成和重建，这限制了特征交互和聚合的有效性和效率。在本文中，我们提出了 MixFuse，一种基于 Transformers 的紧凑型多模态图像融合框架。它平滑地统一了特征提取和集成的过程，作为其核心，混合注意力转换器模块（MATB）集成了交叉注意力转换器模块（CATM）和自注意力转换器模块（SATM）。CATM 引入了一种对称的交叉注意力机制来识别特定于模态和一般特征的特征，过滤掉不相关和冗余的信息。同时，SATM 旨在通过自我注意机制来完善组合特征，增强特征的内部一致性和适当保留。这些连续的交叉和自我注意模块协同工作，以增强更准确和精细的特征图的生成，这对于以后的重建至关重要。在五个公共数据集上对 MixFuse 进行的广泛评估表明，与最先进的方法相比，它的性能和适应性更胜一筹。 </p></details> | ESWA2025 |  [Code](https://github.com/Bitlijinfu/MixFuse) | 没有公布测试代码和预训练代码 |
| **[FusionGCN：使用超像素特征生成 GCN 和像素级特征重建 CNN 进行多焦点图像融合](https://www.sciencedirect.com/science/article/pii/S0957417424026095)** | 2024-11 | <details><summary>Show</summary><p> 近年来，卷积神经网络在计算机视觉领域取得了重大进步，有效地解决了许多以前具有挑战性的问题。越来越多的研究人员将研究重点放在这一领域，提出了创新的网络架构。然而，许多现有网络需要复杂的模块设计和大量参数才能实现令人满意的融合结果，这给计算资源受限的轻量级设备带来了挑战。为了减轻这种担忧，本研究引入了一种将块分割与像素优化相结合的新方法。具体来说，我们最初采用图卷积网络对通过超像素聚类生成的大规模、不规则区域执行灵活的卷积，从而在块级实现粗略分割。随后，我们利用并行轻量级卷积网络提供像素级指导，最终产生更准确的决策图。此外，为了利用这两个网络的优势并促进从非欧几里得数据的图卷积网络优化特征生成，我们精心设计了一个基于超像素的图解码器以及一个基于像素的卷积神经网络提取模块，以增强特征获取和传播。与众多最先进的方法相比，我们的方法在定性和定量分析以及效率评估中都表现出值得称道的竞争力。 </p></details> | ESWA2025 |  [Code](https://github.com/ouyangbaicai/FusionGCN) |
| **[一种基于混合跨多尺度光谱-空间Transformer网络的高光谱和多光谱图像融合方法](https://www.sciencedirect.com/science/article/pii/S0957417424026095)** | 2024-11 | <details><summary>Show</summary><p> 卷积神经网络 （CNN） 为高光谱图像 （HSI） 的生成做出了重大贡献。然而，由于 CNN 的局部感受野的限制，使用 CNN 捕获长程依赖性可能具有挑战性，这可能导致融合图像失真。Transformer 擅长捕获长距离依赖关系，但处理精细细节的能力有限。此外，以前的工作经常忽视图像预处理阶段对全局特征的提取，从而导致精细细节的潜在丢失。为了解决这些问题，我们提出了一种混合跨多尺度光谱空间转换器 （HCMSST），它结合了 CNN 在特征提取方面的优势和 Transformer 在捕获远程依赖性方面的优势。为了在浅层特征提取阶段完全提取和保留本地和全局信息，该网络将 CNN 与交错级联密集残差块 （SCDRB） 相结合。该块采用交错残差在分支内部和分支之间建立直接连接，并集成注意力模块以增强对重要特征的响应。这种方法有助于不受限制的信息交换，并促进更深入的要素表示。为了解决 Transformer 在处理精细细节方面的局限性，我们引入了多尺度空间-光谱-编码-解码结构，以获得全面的空间-频谱特征，这些特征用于通过跨多尺度光谱-空间 Transformer （CMSST） 捕获远程依赖关系。此外，CMSST 还采用了跨级双流特征交互策略，该策略集成了来自不同级别的空间和光谱特征，然后将融合的特征反馈给相应的分支进行信息交互。实验结果表明，与许多最先进的 （SOTA） 方法相比，所提出的 HCMSST 取得了优异的性能。具体来说，与 CAVE 数据集上的 SOTA 方法相比，HCMSST 将 ERGAS 指标降低了 3.05%，而在哈佛数据集上，与 SOTA 结果相比，它的 ERGAS 减少了 2.69%。 </p></details> | ESWA2025 |  []() |
| **[一种基于跨模态强化和多注意力融合策略的新型红外可见光图像融合网络](https://www.sciencedirect.com/science/article/pii/S0957417424025491)** | 2024-11 | <details><summary>Show</summary><p> 红外与可见光图像融合注重互补信息的整合，其融合结果与丰富的纹理细节和突出的目标可以很好地满足人类的视觉观察需求。然而，丢失一些重要细节仍然是图像融合过程中的一个挑战。该文提出了一种基于跨模态强化模块和多注意力融合策略的新方法，以增强端到端的 CNN 骨干。具体来说，应用跨模态架构来补偿异质图像的光谱差异;多尺度条带池用作进一步的特征表示工具，以精确模拟长程独立性;然后，设计细节注入块以满足纹理对比度和目标强度的增强要求;依次提出多注意力融合模块，逐步集成特征。在多个数据集上进行了广泛的对比实验，以证明所提方法在定量度量和视觉感知方面的优越性，例如基于所有实验样本的视觉信息保真度度量的平均值达到 0.964。 </p></details> | ESWA2025 |  []() |
| **[SFINet：用于全时红外和可见光图像融合的语义特征交互式学习网络](https://www.sciencedirect.com/science/article/pii/S095741742402339X)** | 2024-10 | <details><summary>Show</summary><p> 红外和可见光图像融合旨在组合来自各种源图像的数据以生成高质量的图像。然而，许多融合方法通常将视觉质量置于语义信息之上。为了解决这个问题，我们提出了一个用于全时红外和可见光图像的语义特征交互式学习网络 （SFINet）。SFINet 通过语义特征交互 （SFI） 模块包含图像融合网络和图像分割网络。图像融合网络采用多尺度特征提取 （MFE） 模块来捕获多个尺度的全局和局部信息。同时，它使用双注意力特征融合 （DAFF） 模块执行互补信息的自适应融合。图像分割网络使用 SFI 模块引导图像融合网络进行语义特征交互。对比结果表明，所提方法在图像融合和语义分割任务中优于最先进的 （SOTA） 模型。 </p></details> | ESWA2025 |  [Code](https://github.com/songwenhao123/SFINet) |
| **[过度曝光的红外和可见光图像融合基准和基线](https://www.sciencedirect.com/science/article/pii/S0957417424028914)** | 2024-12 | <details><summary>Show</summary><p> 夜间驾驶场景中，远光灯经常会过度曝光可见光图像，这会导致很多视觉智能算法失效。红外和可见光图像的融合可以缓解这个问题。然而，目前的红外和可见光图像融合方法在处理过度曝光挑战方面效果较差。针对这一问题，本文提出了一种即插即用的过曝先验模块，通过将其集成到网络架构和损失函数中，可以有效改进现有的红外和可见光融合方法。具体来说，我们首先收集了一个专用的红外和可见光图像融合数据集 OpIVF，该数据集专注于过度曝光场景，有 1869 个图像对用于训练，40 个图像对用于评估。然后，我们基于自适应饱和度的过度曝光区域检测方法设计了一个简单的过曝先验模块。过度曝光先验可以被视为一个软注意力图，用于重新加权源图像中过度曝光区域的贡献。接下来，为了验证过曝先验的有效性，我们设计了三种过曝先验集成方法，包括基于图像、基于特征和基于损失。这些策略的主要目标是在融合图像的过度曝光区域中尽可能多地保留红外图像的信息。最后，我们将先验过曝纳入四种不同的红外和可见光图像融合方法中。对所提出的数据集的广泛实验表明，过度曝光先验在定量指标、视觉效果和下游视觉任务方面都显着改善了这些方法。 </p></details> | ESWA2025 |  [Code](https://github.com/xierenping5/OpIVF) | 可以看 |
| **[用于高光谱和多光谱图像融合的循环跨模态交互](https://ieeexplore.ieee.org/document/10681101)** | 2024-09 | <details><summary>Show</summary><p> 将低分辨率高光谱影像与高分辨率多光谱影像集成是获取高分辨率高光谱影像的有效方法。最近，许多基于深度学习的方法被用来直接对融合的映射关系进行建模。然而，这些方法往往忽视了光谱特性，无法促进异构模态的全局特征之间的全面交互。在本文中，我们提出了一种基于跨模态时空-谱交互的新型循环 Transformer，利用多种交互模式来探索跨模态特征之间的相似性和互补性。具体来说，我们设计了一种循环交互架构，以充分利用低分辨率高光谱图像中丰富的光谱先验信息和高分辨率多光谱图像中丰富的空间先验信息。通过将空间和光谱先验整合到 Transformer 模块的注意力机制中，我们探索了跨模态特征中的长程依赖信息。此外，为了增强来自不同模态的特征之间的交互，我们设计了空间和光谱维度的跨模态自适应交互机制，以促进不同模态之间的信息互易。广泛的实验表明，所提出的方法在定量和视觉上都优于最先进的聚变方法。 </p></details> | TCSVT2025 |  [Code](https://github.com/Tomchenshi/CYformer) |
| **[FreqGAN：通过统一频率对抗学习实现红外和可见光图像融合](https://ieeexplore.ieee.org/document/10680110)** | 2024-09 | <details><summary>Show</summary><p> 基于深度学习的传统融合方法主要采用卷积或自注意作来模拟局部或全局依赖关系，这通常会导致对频域信息的忽视。为了解决这一缺陷，我们引入了一个统一的频率对抗学习网络，称为 FreqGAN。我们的方法涉及一个频率补偿生成器，它采用离散小波变换将编码的空间特征分解为多个频带。利用跳跃连接，低频和高频分量分别被引导到编码器和解码器中，补偿额外的轮廓和细节。此外，我们构建了一个混合频率聚合模块，该模块可以在多个尺度上逐步优化活动水平，并使各个频段相关联。作为对生成模型的补充，我们设计了双频率约束判别器。这些判别器的任务是动态调整每个输入频带的权重，从而使发生器能够准确地从不同的模态图像中重建突出的频率信息。此外，还制定了频率监控函数，以进一步防止频率信息的丢失。我们全面的实验评估，包括广泛的聚变任务和后续应用，清楚地突出了 FreqGAN 的卓越性能，与现有的最先进的替代方案相比，它成为领跑者。 </p></details> | TCSVT2025 |  [Code](https://github.com/Zhishe-Wang/FreqGAN) |
| **[用于全色锐化的频率解耦域无关特征学习](https://ieeexplore-ieee-org-s-225.libdb.csu.edu.cn/document/10718360)** | 2024-10 | <details><summary>Show</summary><p> 全色锐化旨在通过融合全色 （PAN） 和多光谱 （MS） 图像来生成高细节的多光谱图像 （HRMS）。但是，现有的全色锐化方法在处理分布外数据时通常会严重降低性能，因为它们假定训练和测试数据集是独立的并且分布相同。为了克服这一挑战，我们提出了一种新的频域无关特征学习框架，该框架表现出卓越的泛化能力。我们的方法涉及从输入图像的振幅和相位分量中并行提取和处理域无关的信息。具体来说，我们设计了一个频率信息分离模块来提取配对图像的幅度和相位分量。然后，使用可学习的高通滤波器从幅度谱中消除域特定信息。之后，我们设计了两个专门的子网络（AFL-Net 和 PFL-Net）来对频域无关的信息进行有针对性的学习。这使我们的方法能够有效地捕获图像的幅度和相位谱中包含的互补域无关信息。最后，信息融合与恢复模块动态调整特征通道权重，使网络能够输出高质量的 HRMS 图像。通过这个频域无关的特征学习框架，我们的方法在训练数据集的分布上平衡了泛化能力和网络性能。在各种卫星数据集上进行的广泛实验证明了我们的广义全色锐化方法的有效性。我们提出的网络在定量指标和视觉质量方面都优于最先进的方法，展示了其处理多样化、分布式外数据的卓越能力。 </p></details> | TCSVT2025 |  []() |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> | TCSVT2025 |  []() |
| **[夜间红外和可见光图像的色彩感知融合](https://www.sciencedirect.com/science/article/pii/S0952197624016798)** | 2024-11 | <details><summary>Show</summary><p> 可见光和红外图像的像素级融合已证明在增强信息表示方面有前景。然而，由于光线不足且不均匀，夜间图像融合仍然具有挑战性。现有的融合方法忽视了夜间颜色相关信息的保留，导致亮度不足的结果不令人满意。本文提出了一种新的彩色图像融合框架，以防止颜色失真，从而产生更符合人类感知的结果。首先，我们设计了一个图像融合网络，以保留弱光条件下可见图像的颜色信息。其次，我们将成熟的低光增强技术作为柔性组件整合到网络中，以在正常照明下产生融合结果。训练过程经过精心设计，以解决过度曝光或噪声放大的潜在问题。最后，我们利用知识蒸馏创建一个轻量级的端到端网络，在正常照明条件下直接从成对的低光图像生成融合结果。实验结果表明，我们提出的框架在夜间场景中优于现有方法。 </p></details> | EAAI2025 |  []() | 可以看 |
| **[用于多源图像融合的全连接 Transformer](https://ieeexplore.ieee.org/abstract/document/10874856)** | 2025-02 | <details><summary>Show</summary><p> 多源图像融合将来自多个图像的信息合并到一个数据中，从而提高成像质量。这个话题在社区中引起了极大的兴趣。如何整合来自不同来源的信息仍然是一个巨大的挑战，尽管现有的基于自我注意的 transformer 方法可以捕捉空间和通道的相似性。在本文中，我们首先讨论了所提出的广义自我注意机制背后的数学概念，其中现有的自我注意被认为是基本形式。所提出的机制采用多线性代数来推动一种新的全连接自我注意 （FCSA） 方法的发展，以充分利用多源图像之间的局部和非局部域特定相关性。此外，我们提出了一种多源图像表示，将其作为优化问题中的非局部先验嵌入到 FCSA 框架中。一些不同的熔接问题被展开到所提出的全连接变压器熔融网络 （FC-Former） 中。更具体地说，广义自我注意的概念可以促进自我注意的潜在发展。因此，FC-Former 可以被视为统一不同融合任务的网络模型。与最先进的方法相比，所提出的 FC-Former 方法表现出稳健和优越的性能，显示出其忠实保存信息的能力。 </p></details> | TPAMI2025 |  []() |
| **[SHIP++: 探测多模态图像融合的协同高阶交互](https://ieeexplore.ieee.org/abstract/document/10706703)** | 2024-10 | <details><summary>Show</summary><p> 多模态图像融合旨在通过整合和区分来自多个源图像的跨模态互补信息来生成融合图像。虽然具有全局空间交互的交叉注意力机制看起来很有希望，但它只捕获了二阶空间交互，忽略了空间和通道维度的高阶交互。这种限制阻碍了多模式之间协同作用的利用。为了弥合这一差距，我们引入了一种协同高阶交互范式 （SHIP），旨在系统地研究多模态图像在两个基本维度上的空间细粒度和全局统计协作：1） 空间维度：我们通过元素乘法构建空间细粒度交互，在数学上等同于全局交互，然后通过迭代聚合和发展互补来培养高阶格式信息，提高效率和灵活性。2） 频道维度：通过一阶统计量（均值）扩展频道交互，我们设计了高阶频道交互，以促进基于全局统计数据识别源图像之间的相互依赖关系。我们进一步介绍了 SHIP 模型的增强版本，称为 SHIP++，它通过跨阶注意力进化机制、跨阶信息集成和残差信息记忆机制增强了跨模态信息交互表示。利用高阶交互显着增强了我们的模型利用多模态协同的能力，从而比最先进的替代方案具有卓越的性能，这在两个重要的多模态图像融合任务（全色锐化以及红外和可见光图像融合）中跨各种基准的综合实验中得到了证明。 </p></details> | TPAMI2025 |  []() |
| **[MulFS-CAP： 用于未配准红外可见光图像融合的多模态融合监督跨模态对齐感知](https://ieeexplore-ieee-org-s-225.libdb.csu.edu.cn/document/10856402)** | 2025-01 | <details><summary>Show</summary><p> 在这项研究中，我们提出了多模态融合监督跨模态对齐感知 （MulFS-CAP），这是一种用于未配准红外可见光图像单阶段融合的新框架。传统的两阶段方法依赖于显式配准算法在空间上对齐源图像，这通常会增加复杂性。相比之下，MulFS-CAP 将隐式配准与熔融无缝融合，简化了流程并增强了对实际应用的适用性。MulFS-CAP 利用共享的浅层特征编码器将未配准的红外可见光图像合并到一个阶段中。为了满足特征级对齐和融合的特定要求，我们通过可学习的模态字典开发了一种一致的特征学习方法。该字典为单峰特征提供补充信息，从而保持单个和融合多模态特征之间的一致性。因此，MulFS-CAP 有效降低了模态方差对跨模态特征对齐的影响，从而允许同时配准和融合。此外，在 MulFS-CAP 中，我们提出了一种新的跨模态对齐方法，创建了一个相关矩阵来详细说明源图像之间的像素关系。该矩阵有助于在红外和可见光图像中对齐特征，进一步完善融合过程。上述设计使 MulFS-CAP 更加轻量级、有效且无需显式注册。来自不同数据集的实验结果表明了我们提出的方法的有效性及其相对于最先进的两阶段方法的优越性。 </p></details> | TPAMI2025 |  [Code](https://github.com/YR0211/MulFS-CAP) |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |
| **[LEFuse：夜间红外和可见光图像的联合低光增强和图像融合](https://www-sciencedirect-com-s-225.libdb.csu.edu.cn/science/article/pii/S0925231225002644?via%3Dihub)** | 2025-02 | <details><summary>Show</summary><p> 红外和可见光图像融合 （IVIF） 旨在通过整合来自两种模式的信息来更丰富、更准确地表示场景。然而，现有的 IVIF 方法通常是为正常照明条件设计的，旨在通过保持与源图像的紧密相似性来获得更高的分数。在夜景中，由于昏暗的环境和局部光源的干扰，可见光图像经常受到低光和局部过度曝光的影响。这些方法无法探索隐藏在可见图像暗区中的信息，导致注入图像缺乏纹理细节，整体显得较暗，视觉质量较差。为了解决这个问题，我们提出了一种名为 LEFuse 的新型图像融合网络。LEFuse 不仅集成了来自可见光和红外图像的互补信息，还专注于恢复可见光图像中隐藏的纹理细节。通过这样做，LEFuse 增强了融合图像的可见性和对比度，从而产生更明亮、更生动的表现。为了实现这一目标，我们提出了一组无监督损失函数来驱动网络的学习。这组包括用于图像融合和低光增强的最大基于熵的融合增强损失，以及用于减轻局部过度曝光不可见图像对融合结果的影响的感知损失。这些损失可以应用于任何现有的图像融合网络，从而在不影响融合性能的情况下增强融合图像。广泛的实验表明，我们的 LEFuse 在视觉质量和定量评估方面取得了可喜的结果，尤其是在夜间环境中。 </p></details> | NEUROCOMPUTING2025 | [Code](https://github.com/cheng411523/LEFuse) | 可以看 |
| **[用于脑部 CT 和 MRI 的对比学习引导融合网络](https://ieeexplore-ieee-org-s-225.libdb.csu.edu.cn/document/10902172)** | 2025-02 | <details><summary>Show</summary><p> 医学影像融合技术为专业人员提供更详细、更精确的诊断信息。本文介绍了一种基于对比学习引导网络的新型高效 CT 和 MRI 融合网络 CLGFusion。CLGFusion 在特征编码阶段包括两个编码分支，使它们能够相互交互和学习。该方法从训练单视图编码器开始，以从不同的增强视图中预测图像的特征表示。同时，使用单视图编码器的指数移动平均线对多视图编码器进行了改进。通过创建特征对比空间而不构建负样本，将对比学习集成到医学图像融合中。这个特征对比空间巧妙地利用了源图像及其对应的增强图像的特征乘积中的差异信息。它通过结合结构相似度损失的方法，不断引导网络不断优化其融合效果，以实现更准确、更高效的图像融合。这种方法代表了端到端的无监督融合模型。实验验证表明，我们提出的方法在主观评价和客观指标方面都表现出与最先进的技术相当的性能。 </p></details> | JBHI2025 |  []() |
| **[SCDFuse：用于联合红外和可见光图像融合和去噪的语义互补蒸馏框架](https://www-sciencedirect-com-s-225.libdb.csu.edu.cn/science/article/pii/S0950705125003090?via%3Dihub)** | 2025-03 | <details><summary>Show</summary><p> 红外和可见光融合因其在计算机视觉领域的广泛应用而受到前所未有的关注。然而，现有的算法单方面专注于清晰场景图像的融合，容易受到噪声干扰。虽然可以通过部署独立的预降噪模块来缓解这个问题，但具有不同功能的其他模块的级联会带来补充复杂性、计算开销，甚至模块间干扰。为了克服这一限制并实现多任务统一，我们提出了一个用于端到端同步特征去噪和聚合的知识蒸馏框架。在这个框架中，我们利用蒸馏架构的优势来生成软标签，减轻因缺乏标签指导而导致的不稳定融合性能。为了实现训练过程中函数学习的准确指导，该文设计了一种非对称噪声感知训练策略，以促进聚合鲁棒性和去噪能力。此外，为保证特征挖掘和语义互补能力，构建了一种混合串并联 CNN 变压器双分支 En-Decoder。所提出的编码器结合了自主设计的纹理感知 ConvNextV2、条带池注意力和渐进式剩余变压器，以组成双分支架构。此外，还开发了语义互补特征聚合 （SCFA） 模块，以实现从粗到细的特征增强。对规则和噪声熔融材料进行了广泛的实验，以验证所提出的方法的积分和去噪性能。值得注意的是，在 TNO 数据集上，与次优算法相比，所提出的方法在 MSSIM 和 UQI 指标上分别提高了 4% 和 4.2%。此外，我们还通过对象检测实验研究了它对高级视觉任务的便利性。 </p></details> |  |  []() |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |


## 会议
| **题目** | **日期** | **摘要** | **录用信息** | **代码** | **备注**
| --- | --- | --- | --- | --- | --- |
| **[One Model for ALL: Low-Level Task Interaction Is a Key to Task-Agnostic Image Fusion](https://arxiv.org/abs/2502.19854)** | 2025-02 | <details><summary>Show</summary><p> 先进的图像融合方法大多优先考虑高级任务，其中任务交互与语义差距斗争，需要复杂的桥接机制。相比之下，我们建议利用低水平的视觉任务从数字摄影融合，允许通过像素级监督有效的特征交互。这种新的模式为无监督的多模态融合提供了强有力的指导，不依赖于抽象语义，增强了任务共享特征学习的广泛适用性。由于混合图像特征和增强的通用表示，所提出的 GIFNet 支持多种融合任务，通过单一模型实现了在可见和未见情景下的高性能。独特的实验结果表明，我们的框架也支持单模态增强，为实际应用提供了优越的灵活性。 </p></details> | CVPR2025 | [Code Link](https://github.com/AWCXV/GIFNet) | 代码已跑通 |
| **[A2RNet：用于强大红外和可见光图像融合的对抗性攻击弹性网络(Arxiv)](http://arxiv.org/abs/2412.09954v2)** | 2024-12-18 | <details><summary>Show</summary><p>红外和可见光图像融合 （IVIF） 是一种通过将来自不同模态的独特信息集成到一个融合图像中来提高视觉性能的关键技术。现有的方法更注重使用未受干扰的数据进行融合，而忽视了故意干扰对融合结果有效性的影响。为了研究融合模型的鲁棒性，在本文中，我们提出了一种新的对抗性攻击弹性网络，称为 $\textrm{A}^{\textrm{2}}$RNet。具体来说，我们开发了一个具有抗攻击损失函数的对抗范式来实现对抗性攻击和训练。它是基于 IVIF 的内在本质构建的，为未来的研究进展提供了坚实的基础。在这种范式下，我们采用 Unet 作为管道，并使用基于 transformer 的防御优化模块 （DRM），以稳健的粗到精方式保证融合图像质量。与以前的工作相比，我们的方法减轻了对抗性扰动的不利影响，始终保持高保真融合结果。此外，下游任务的性能也可以在对抗性攻击下得到很好的维护。代码可在 https://github.com/lok-18/A2RNet 获取。</p></details> | AAAI2025 | [Code Link](https://github.com/lok-18/A2RNet) | Jinyuan Liu |
| **[BSAFusion：用于未对齐医学图像融合的双向逐步特征对齐网络(Arxiv)](http://arxiv.org/abs/2412.08050v2)** | 2024-12-13 | <details><summary>Show</summary><p>如果未对齐的多模态医学图像可以在统一的处理框架内使用单阶段方法同时对齐和融合，不仅可以实现双重任务的相互促进，还有助于降低模型的复杂性。然而，该模型的设计面临着特征融合和对齐要求不兼容的挑战;具体来说，特征对齐需要相应特征之间的一致性，而特征融合需要特征之间相互补充。为了应对这一挑战，本文提出了一种称为双向逐步特征对齐和融合 （BSFA-F） 策略的未对齐医学图像融合方法。为了减少模态差异对跨模态特征匹配的负面影响，我们将模态无差异特征表示 （MDF-FR） 方法纳入 BSFA-F 中。MDF-FR 利用模态特征表示头 （MFRH） 来集成输入图像的全局信息。通过将当前图像的 MFRH 中包含的信息注入到其他模态图像中，在保留不同图像携带的互补信息的同时，有效地减少了模态差异对特征对齐的影响。在特征对齐方面，BSFA-F 采用基于两点之间矢量位移路径独立性的双向逐步对齐变形场预测策略。该策略解决了单步对齐中大跨度和变形场预测不准确的问题。最后，Multi-Modal Feature Fusion 模块实现了对齐特征的融合。多个数据集的实验结果证明了我们方法的有效性。源代码可在 https://github.com/slrl123/BSAFusion 上获得。</p></details> | AAAI2025 | [Code Link](https://github.com/slrl123/BSAFusion) | Huafeng Li |
| **[TDFusion: 具有可学习融合损失的任务驱动图像融合](https://arxiv.org/abs/2412.03240v2)** |  | <details><summary>Show</summary><p> 多模态图像融合聚合了来自多个传感器源的信息，与单源图像相比，实现了卓越的视觉质量和感知特征，通常可以改进下游任务。然而，当前下游任务的融合方法仍然使用预定义的融合目标，这可能会与下游任务不匹配，从而限制自适应指导并降低模型灵活性。为了解决这个问题，我们提出了任务驱动图像融合 （TDFusion），这是一个融合框架，其中包含由任务损失指导的可学习融合损失。具体来说，我们的融合损失包括由称为损失生成模块的神经网络建模的可学习参数。该模块以元学习方式由下游任务损失监督。学习目标是在使用融合损失优化融合模块后，最大限度地减少融合图像的任务损失。融合模块和损失模块之间的迭代更新确保融合网络朝着最小化任务损失的方向发展，引导融合过程朝着任务目标发展。TDFusion 的训练完全依赖于下游任务损失，使其能够适应任何特定任务。它可以应用于融合和任务网络的任何架构。实验通过在四个不同的数据集上进行的融合实验，以及对语义分割和对象检测任务的评估，证明了 TDFusion 的性能。 </p></details> | CVPR2025 |  []() | Zixiang Zhao
| **[DCEvo：用于红外和可见光图像融合的判别性跨维度进化学习](https://arxiv.org/abs/2503.17673v1)** |  | <details><summary>Show</summary><p> 红外和可见光图像融合整合了来自不同光谱波段的信息，通过利用每种模式的优势和减轻其局限性来提高图像质量。现有方法通常将图像融合和随后的高级任务视为单独的过程，导致融合图像在任务性能上仅提供边际收益，并且无法为优化融合过程提供建设性反馈。为了克服这些限制，我们提出了一种判别性跨维度进化学习框架，称为 DCEvo，它同时提高了视觉质量和感知准确性。利用进化学习的强大搜索能力，我们的方法通过采用进化算法 （EA） 动态平衡损失函数参数，将双重任务的优化表述为多目标问题。受视觉神经科学的启发，我们在编码器和解码器中集成了判别增强器 （DE），能够有效地学习来自不同模态的互补特征。此外，我们的跨维嵌入 （CDE） 块促进了高维任务特征和低维融合特征之间的相互增强，确保了内聚和高效的特征集成过程。三个基准的实验结果表明，我们的方法明显优于最先进的方法，在视觉质量上平均提高了 9.32%，同时还增强了后续的高级任务。 </p></details> |  |  [Code](https://github.com/Beate-Suy-Zhang/DCEvo) | JInyuan Liu |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |
| **[VIIS：用于严重低光图像增强的可见光和红外信息合成(Arxiv)](http://arxiv.org/abs/2412.13655v2)** | 2025-02-13 | <details><summary>Show</summary><p>在严重的低光环境下拍摄的图像通常会出现大量信息缺失的情况。现有的奇异模态图像增强方法难以恢复缺乏有效信息的图像区域。通过利用不透光的红外图像，可见光和红外图像融合方法有可能揭示隐藏在黑暗中的信息。然而，它们主要强调模态间互补，而忽视了模态内增强，限制了输出图像的感知质量。为了解决这些限制，我们提出了一项称为可见光和红外信息合成 （VIIS） 的新任务，旨在实现两种模式的信息增强和融合。鉴于在 VIIS 任务中难以获得地面实况，我们设计了一个基于图像增强的信息合成前置任务 （ISPT）。我们采用扩散模型作为框架，设计了一种稀疏基于注意力的双模态残差 （SADMR） 条件反射机制，以增强两种模态之间的信息交互。这种机制使具有两种模态先验知识的特征能够在去噪过程中自适应和迭代地关注每个模态的信息。我们广泛的实验表明，我们的模型不仅在定性和定量上优于相关领域最先进的方法，而且优于能够进行信息增强和融合的新设计的基线。该代码可在 https://github.com/Chenz418/VIIS 获取。</p></details> | WACV2025 | [Code Link](https://github.com/Chenz418/VIIS) |
| **[重新思考改进多模态图像分割的早期融合策略(Arxiv)](http://arxiv.org/abs/2501.10958v1)** | 2025-01-19 | <details><summary>Show</summary><p>RGB和热成像图像融合在低光照条件下具有显著的潜力，能够提高语义分割的效果。现有方法通常采用双分支编码器框架进行多模态特征提取，并设计复杂的特征融合策略以实现多模态语义分割的特征提取和融合。然而，这些方法在特征提取和融合过程中需要大量的参数更新和计算工作。为了解决这一问题，我们提出了一种新颖的多模态融合网络（EFNet），基于早期融合策略和简单但有效的特征聚类方法，用于高效训练RGB-T语义分割。此外，我们还提出了一种基于欧几里得距离的轻量级高效多尺度特征聚合解码器。我们在不同数据集上验证了我们方法的有效性，并在参数和计算量更少的情况下超越了先前的最先进方法。</p></details> | ICASSP2025 | None |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> | 2025 |  []() |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> | 2025 |  []() |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> | 2025 |  []() |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> | 2025 |  []() |

## 复现工作整理
### 红外-可见光图像融合(IVIF)
| **题目** | **日期** | **摘要** | **录用信息** | **代码** | **备注**
| --- | --- | --- | --- | --- | --- |
| **[A2RNet：用于强大红外和可见光图像融合的对抗性攻击弹性网络(Arxiv)](http://arxiv.org/abs/2412.09954v2)** | 2024-12-18 | <details><summary>Show</summary><p>红外和可见光图像融合 （IVIF） 是一种通过将来自不同模态的独特信息集成到一个融合图像中来提高视觉性能的关键技术。现有的方法更注重使用未受干扰的数据进行融合，而忽视了故意干扰对融合结果有效性的影响。为了研究融合模型的鲁棒性，在本文中，我们提出了一种新的对抗性攻击弹性网络，称为 $\textrm{A}^{\textrm{2}}$RNet。具体来说，我们开发了一个具有抗攻击损失函数的对抗范式来实现对抗性攻击和训练。它是基于 IVIF 的内在本质构建的，为未来的研究进展提供了坚实的基础。在这种范式下，我们采用 Unet 作为管道，并使用基于 transformer 的防御优化模块 （DRM），以稳健的粗到精方式保证融合图像质量。与以前的工作相比，我们的方法减轻了对抗性扰动的不利影响，始终保持高保真融合结果。此外，下游任务的性能也可以在对抗性攻击下得到很好的维护。代码可在 https://github.com/lok-18/A2RNet 获取。</p></details> | AAAI2025 | [Code Link](https://github.com/lok-18/A2RNet) | Jinyuan Liu |
| **[SHIP: 探测红外与可见光图像融合的协同高阶相互作用](https://openaccess.thecvf.com/content/CVPR2024/html/Zheng_Probing_Synergistic_High-Order_Interaction_in_Infrared_and_Visible_Image_Fusion_CVPR_2024_paper.html)** | 2024 | <details><summary>Show</summary><p> 红外和可见光图像融合旨在通过整合和区分互补信息从多个来源来生成融合图像。虽然具有全局空间交互的交叉注意力机制看起来很有希望，但它只捕获二阶空间交互，而忽略了空间和通道维度上的高阶交互。这种限制阻碍了多模态之间协同作用的利用。为了弥补这一差距，我们引入了一种协同高阶交互范式(SHIP)，旨在系统地研究红外图像和可见光图像在两个基本维度上的空间细粒度和全局统计协作:1)空间维度:我们通过元素乘法构建空间细粒度交互，在数学上等价于全局交互，然后通过迭代聚合和进化互补信息来促进高阶格式，提高效率和灵活性;2)通道维度:在与一阶统计量(均值)的通道交互上扩展，我们设计了高阶通道交互，以促进基于全局统计的源图像之间的相互依赖关系的识别。利用高阶交互显着提高了我们的模型利用多模态协同作用的能力，从而比最先进的替代方案具有更好的性能，如在各种基准上的综合实验所示。 </p></details> | CVPR2024/TPAMI2024 |  [Code Link](https://github.com/zheng980629/SHIP) | 对比实验 |
| **[CrossFuse：一种基于交叉注意力机制的新型红外和可见光图像融合方法(Arxiv)](http://arxiv.org/abs/2406.10581v1)** | 2024-06-15 | <details><summary>Show</summary><p>多模态视觉信息融合旨在将多传感器数据整合到一张图像中，其中包含更多的互补信息和更少的冗余特征。然而，互补信息很难提取，尤其是对于红外和可见光图像，这两种模态之间存在很大的相似性差距。常见的交叉注意力模块只考虑相关性，相反，图像融合任务需要关注互补性（uncorrelation）。因此，本文提出了一种新的交叉注意力机制 （CAM） 来增强互补信息。此外，提出了一种基于两阶段训练策略的融合方案来生成融合图像。在第一阶段，针对每种模态训练两个具有相同架构的自动编码器网络。然后，使用固定编码器，在第二阶段训练 CAM 和解码器。通过训练的 CAM，从两种模态中提取的特征被集成到一个融合的特征中，其中互补信息得到增强，冗余特征得到减少。最后，经过训练的解码器可以生成融合图像。实验结果表明，与现有的融合网络相比，我们提出的融合方法获得了SOTA融合性能。这些代码可在 https://github.com/hli1221/CrossFuse | Inf. Fusion | [Code Link](https://github.com/hli1221/CrossFuse) | Hui Li |
| **[DCINN: 用于图像融合的细节保持条件可逆网络的一般范式](https://link.springer.com/article/10.1007/s11263-023-01924-5)** | 2024 | <details><summary>Show</summary><p> 现有的图像融合深度学习技术要么直接学习图像映射(LIM)，由于每个像素的考虑相等，这使得它们在保留细节方面无效，要么学习细节映射(LDM)，它只获得有限的性能水平，因为只有细节用于推理。最近的无损可逆网络(INN)已经证明了其细节保持能力。然而，INN对图像融合任务的直接适用性受到保体积约束的限制。此外，缺乏一致的保细节图像融合框架来产生令人满意的结果。为此，我们提出了一种基于新条件 INN（称为 DCINN）的图像融合通用范式。DCINN范式有三个核心组件:将图像映射转换为细节映射的分解模块;一个直接从源图像中提取辅助特征的辅助网络(ANet);以及一个条件INN (CINN)，它学习基于辅助特征的细节映射。新颖的设计得益于 INN、LIM 和 LDM 方法的优点，同时避免它们的缺点。特别是，使用 INN 到 LDM 可以轻松满足保体积约束，同时仍然保留细节。此外，由于辅助特征作为条件特征，ANet 允许在不影响细节映射的情况下使用不仅仅是推理的细节。在三个基准融合问题上的广泛实验，即全色锐化、高光谱和多光谱图像融合，以及红外和可见光图像融合，证明了我们的方法与最近的最先进方法相比的优越性。 </p></details> | IJCV2024 |  [Code Link](https://github.com/wwhappylife/DCINN) | 对比实验 |
| **[SegMiF: 用于图像融合和分割的多交互式特征学习和全职多模态基准测试(Arxiv)](http://arxiv.org/abs/2308.02097v1)** | 2023 | <details><summary>Show</summary><p>多模态图像融合和分割在自动驾驶和机器人作中起着至关重要的作用。早期的工作只集中在提高一项任务的性能，\emph{eg.，} 融合或分割，这使得它很难达到~'两全其美'。为了克服这个问题，在本文中，我们提出了一种用于图像融合和 Segmentation 的 Multi-interactive Feature 学习架构，即 SegMiF，并利用双任务相关性来促进这两个任务的性能。SegMiF 采用级联结构，包含一个融合子网络和一个常用的分割子网络。通过巧妙地桥接两个组件之间的中间特征，从分割任务中学到的知识可以有效地协助融合任务。此外，受益的融合网络支持分割网络执行得更自命不凡。此外，建立了分层交互式注意力块，以确保两个任务之间所有重要信息的细粒度映射，从而使模态/语义特征完全互交互。此外，还引入了动态权重因子，可以自动调整每个任务的相应权重，可以平衡交互特征的对应关系，突破费力调优的限制。此外，我们构建了一个智能多波双目成像系统，并收集了一个全职多模态基准，具有 15 个带注释的像素级类别，用于图像融合和分割。对几个公共数据集和我们的基准测试进行的广泛实验表明，所提出的方法输出视觉上吸引人的融合图像，并且性能一般在现实世界中，分割 mIoU 高于最先进的方法。</p></details> | ICCV2023 | [Code Link](https://github.com/JinyuanLiu-CV/SegMiF) | 对比实验 |
| **[MetaFusion：基于目标检测的元特征嵌入的红外和可见光图像融合](https://paperswithcode.com/paper/metafusion-infrared-and-visible-image-fusion)** | 2023 | <details><summary>Show</summary><p> 融合红外和可见光图像可以为后续的目标检测任务提供更多的纹理细节。相反，检测任务提供对象语义信息来改进红外和可见光图像的融合。因此，使用相互促进的联合融合和检测学习越来越受到关注。然而，这两个不同级别任务之间的特征差距阻碍了进展。针对这一问题，本文提出了一种基于目标检测元特征嵌入的红外和可见光图像融合。核心思想是元特征嵌入模型旨在根据融合网络能力生成对象语义特征，因此语义特征自然与融合特征兼容。它通过模拟元学习进行了优化。此外，我们进一步实现了融合和检测任务之间的相互促进学习，以提高其性能。在三个公共数据集上的综合实验证明了我们方法的有效性。 </p></details> | CVPR2023 |  [Code Link](https://github.com/wdzhao123/MetaFusion) | 对比实验 |
| **[LRRNet：一种用于红外和可见光图像的新型表示学习引导融合网络](https://ieeexplore.ieee.org/document/10105495)** | 2023 | <details><summary>Show</summary><p> 基于深度学习的融合方法在图像融合任务中取得了良好的性能。这归因于网络架构在融合过程中起着非常重要的作用。然而，一般来说，很难指定一个好的融合架构，因此融合网络的设计仍然是黑艺术，而不是科学。为了解决这个问题，我们在数学上制定了融合任务，并在其最优解和可以实现它的网络架构之间建立联系。这种方法导致本文提出了一种新颖的构建轻量级融合网络的方法。它通过试错策略避免了耗时的经验网络设计。特别是我们对融合任务采用了可学习的表示方法，其中融合网络架构的构建由产生可学习模型的优化算法指导。低秩表示 (LRR) 目标是我们可学习模型的基础。将解心的矩阵乘法转化为卷积运算，将优化过程替换为特殊的前馈网络。基于这种新的网络架构，构建了一个端到端轻量级融合网络来融合红外和可见光图像。它的成功训练是由一个细节到语义信息的损失函数来促进的，该函数旨在保留图像细节并增强源图像的显着特征。我们的实验表明，所提出的融合网络在公共数据集上表现出比最先进的融合方法更好的融合性能。有趣的是，我们的网络比其他现有方法需要更少的训练参数。T </p></details> | TPAMI2023 |  [Code Link](https://ieeexplore.ieee.org/document/10105495) | 对比实验 |
| **[Dif-Fusion: 基于扩散模型的红外与可见光图像融合](https://ieeexplore.ieee.org/document/10286359)** | 2023 | <details><summary>Show</summary><p> 颜色在人类视知觉中扮演重要角色，反映物体的光谱。然而，现有的红外和可见光图像融合方法很少探索如何直接处理多光谱 / 通道数据，实现高色彩保真度。针对上述问题，提出了一种基于扩散模型的多通道输入数据分布生成方法 Dif-Fusion，该方法增强了多源信息聚合的能力，提高了颜色的保真度。具体地说，在现有的融合方法中，我们没有将多通道图像转换为单通道数据，而是在潜在空间中利用正向和反向扩散过程，通过去噪网络来创建多通道数据分布。然后，利用去噪网络提取可见光和红外信息的多通道扩散特征。最后，将多通道扩散特征反馈给多通道融合模块，直接生成三通道融合图像。为了保留纹理和强度信息，我们提出了多通道梯度损失和强度损失。随着目前测量纹理和强度保真度的评价指标，我们引入 Delta E 作为一种新的评价指标来量化颜色保真度。大量的实验表明，我们的方法是更有效的比其他国家的最先进的图像融合方法，特别是在彩色保真度。 </p></details> | TIP2023 |  [Code Link](https://github.com/GeoVectorMatrix/Dif-Fusion) | 适用彩色图像 |
| **[UAAFusion: 基于归因分析的深度展开多模态图像融合网络](https://ieeexplore.ieee.org/document/10769519)** |  | <details><summary>Show</summary><p> 多模态图像融合将来自多个来源的信息合成到单个图像中，从而促进语义分割等下游任务。目前的方法主要侧重于通过复杂的映射在视觉显示层获取信息丰富的融合图像。尽管一些方法试图共同优化图像融合和下游任务，但这些努力往往缺乏直接的指导或交互，仅用于协助预定义的融合损失。为了解决这个问题，我们提出了一个 “展开归因分析融合网络” （UAAFusion），使用归因分析更有效地定制融合图像以进行语义分割，增强融合和分割之间的交互。具体来说，我们利用归因分析技术来探索源图像中语义区域对任务区分的贡献。同时，我们的融合算法从源图像中整合了更多有益的特征，从而允许分割来指导融合过程。我们的方法构建了一个模型驱动的展开网络，该网络使用来自归因分析的优化目标，并根据分割网络的当前状态计算归因融合损失。我们还开发了一种新的归因分析路径函数，专门针对我们展开网络中的融合任务量身定制。在每个网络阶段都集成了归因注意力机制，使融合网络能够优先考虑对高级识别任务至关重要的区域和像素。此外，为了减轻传统展开网络中的信息损失，我们的网络中加入了一个内存增强模块，以改善各个网络层之间的信息流。广泛的实验证明了我们的方法在图像融合和语义分割方面的优越性。 </p></details> |  |  [Code](https://github.com/HaowenBai/UAAFusion) | 已跑通 |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |

### 医学图像融合(MIF)
| **题目** | **日期** | **摘要** | **录用信息** | **代码** | **备注**
| --- | --- | --- | --- | --- | --- |
| **[BSAFusion：用于未对齐医学图像融合的双向逐步特征对齐网络(Arxiv)](http://arxiv.org/abs/2412.08050v2)** | 2024-12-13 | <details><summary>Show</summary><p>如果未对齐的多模态医学图像可以在统一的处理框架内使用单阶段方法同时对齐和融合，不仅可以实现双重任务的相互促进，还有助于降低模型的复杂性。然而，该模型的设计面临着特征融合和对齐要求不兼容的挑战;具体来说，特征对齐需要相应特征之间的一致性，而特征融合需要特征之间相互补充。为了应对这一挑战，本文提出了一种称为双向逐步特征对齐和融合 （BSFA-F） 策略的未对齐医学图像融合方法。为了减少模态差异对跨模态特征匹配的负面影响，我们将模态无差异特征表示 （MDF-FR） 方法纳入 BSFA-F 中。MDF-FR 利用模态特征表示头 （MFRH） 来集成输入图像的全局信息。通过将当前图像的 MFRH 中包含的信息注入到其他模态图像中，在保留不同图像携带的互补信息的同时，有效地减少了模态差异对特征对齐的影响。在特征对齐方面，BSFA-F 采用基于两点之间矢量位移路径独立性的双向逐步对齐变形场预测策略。该策略解决了单步对齐中大跨度和变形场预测不准确的问题。最后，Multi-Modal Feature Fusion 模块实现了对齐特征的融合。多个数据集的实验结果证明了我们方法的有效性。源代码可在 https://github.com/slrl123/BSAFusion 上获得。</p></details> | AAAI2025 | [Code Link](https://github.com/slrl123/BSAFusion) | Huafeng Li |
| **[MMIF-INet: 可逆网络的多模态医学图像融合](https://www.sciencedirect.com/science/article/abs/pii/S1566253524004445)** | 2024-09 | <details><summary>Show</summary><p> 多模态医学图像融合 (MMIF) 技术旨在生成全面反映组织、器官和代谢信息的融合图像，从而辅助医学诊断，提高临床诊断的可靠性。然而，大多数方法在特征提取和融合过程中都存在信息丢失问题，很少探索如何直接处理多通道数据。针对上述问题，本文提出了一种新的可逆融合网络 (MMIF-INet) ，该网络接受三通道彩色图像作为模型的输入，并以过程可逆的方式生成多通道数据分布。具体来说，离散小波变换 (dWT) 用于下采样，目的是将源图像对分解为高频和低频分量。同时，可逆块 (IB) 促进了初步的特征融合，实现了跨域互补信息和多源信息的无损集成。IB 和 DWT 的结合保证了初始融合的可逆性和跨不同尺度的语义特征提取。为了适应融合任务，采用了多尺度融合模块，集成了来自不同模式和多尺度特征的不同组件。最后，设计了一种混合损耗，从结构、梯度、强度和色度的角度对模型训练进行约束，从而实现对源图像的亮度、颜色和详细信息的有效保留。在多个医学数据集上的实验表明，MMIF-INet 在视觉质量、定量度量和融合效率方面优于现有的方法，特别是在色彩保真度方面。扩展到红外 - 可见光图像融合，七个最佳评价标准进一步证实了 MMIF-INet 的优越融合性能。 </p></details> | Inf.Fusion2024 |  [Code Link](https://github.com/HeDan-11/MMIF-INet) | 适用彩色图像融合 |
| **[MM-Net：一种基于MixFormer的解剖和功能图像融合多尺度网络](https://ieeexplore.ieee.org/document/10471275)** | 2024-03-25 | <details><summary>Show</summary><p> 解剖和功能图像融合是各种医学和生物应用中的一项重要技术。近年来，基于深度学习的方法已成为多模态图像融合领域的主流方向。然而，现有的基于dl的融合方法很难同时有效地捕获局部特征和全局上下文信息。此外，在图像融合中，特征的尺度多样性是关键问题，在大多数现有工作中往往缺乏足够的关注。在本文中，为了解决上述问题，我们提出了一种基于MixFormer的多尺度网络，称为MM-Net，用于解剖和功能图像融合。在我们的方法中，引入了一个改进的基于MixFormer的主干，从源图像中在多个尺度上充分提取局部特征和全局上下文信息。基于多源空间注意的跨模态特征融合(CMFF)模块，将不同源图像的特征在多个尺度上融合。融合特征的尺度多样性通过一系列多尺度特征交互(MSFI)模块和特征聚合上采样(FAU)模块进一步丰富。此外，设计了一个由空间域和频域分量组成的损失函数来训练所提出的融合模型。实验结果表明，我们的方法在定性和定量比较上都优于几种最先进的融合方法，所提出的融合模型具有良好的泛化能力。 </p></details> | TIP2024 |  [Code Link](https://github.com/yuliu316316) | Yu Liu，亮度恢复较差，功能图像融合 |
| **[ALMFNet:学习搜索一种用于医学图像融合的轻量级广义网络](https://ieeexplore.ieee.org/abstract/document/10360160)** | 2023-12-14 | <details><summary>Show</summary><p> 图像融合在综合医学成像管道中是必不可少的。通过采用深度学习技术，医学图像融合在过去几年中取得了巨大的进展。然而，现有的方法对特定类型的医学图像融合任务做出了努力，在泛化方面可能会遇到困难。此外，它们中的大多数对每个神经进行应变，以增加深度宽度来设计各种架构，将障碍置于运行效率中。为了解决上述问题，我们提出了一种自动搜索轻量级多源融合网络，即ALMFnet，旨在以网络架构搜索的方式结合软件和硬件知识进行医学图像融合。具体来说，开发了由两个不同的特征提取模块和一个融合模块组成的ALMFnet，用于在广义模型中提取和细化多源特征。此外，受协作原则的启发，我们引入了硬件约束来充分搜索每个特定组件，进一步降低了所获得模型的复杂性。此外，为了保留病理图像区域的重要细节，我们在所开发的方法中引入了一个分割掩码。实验结果表明，我们的广义模型不仅在定量分数方面优于以前的方法，而且在模型复杂度方面也优于以前的方法 </p></details> | TCSVT2024 |  [Code Link](https://github.com/RollingPlain/ALMFnet) | Jinyuan Liu |
| **[MsgFusion：用于多模态脑图像融合的医学语义引导双分支网络](https://ieeexplore.ieee.org/document/10120980)** | 2023-04 | <details><summary>Show</summary><p> 多模态图像融合在医学图像分析和应用中起着至关重要的作用，其中计算机断层扫描 (CT)、磁共振 (MR)、单光子发射计算机断层扫描 (SPECT) 和正电子发射断层扫描 (PET) 是常用的模式，尤其是对于脑疾病诊断。现有的融合方法大多没有考虑医学图像的特点，采用类似的策略和评估标准进行自然图像融合。虽然独特的医学语义信息（MS-Info）隐藏在不同的模式中，但融合结果的最终临床评估被忽略了。我们的 MsgFusion 首先在 MR/CT/PET/SPECT 图像和图像特征的关键 MS-Info 之间建立关系，使用两个分支和图像融合框架的设计来指导 CNN 特征提取。对于 MR 图像，我们结合空间域特征和频域特征 (SF) 开发了一个分支。对于 PET/SPECT/CT 图像，我们集成了灰色颜色空间特征并调整 HSV 颜色空间特征 (GV) 来开发另一个分支。还提出了一种基于分类的分层融合策略来重建融合图像以持续和增强反映解剖结构和功能代谢的显着 MS-Info。融合实验是在许多对 MR-PET/SPECT 和 MR-CT 图像上进行的。根据七个经典客观质量评估和 30 名临床医生的一项新的主观临床质量评估，所提出的 MsgFusion 的融合结果优于现有的代表性方法。 </p></details> | TMM2023 |  []() | 实验分析设计较好 |
| **[GeSeNet：一种用于医学图像融合的通用耦掩模集成语义引导网络](https://ieeexplore.ieee.org/abstract/document/10190200)** | 2023-07 | <details><summary>Show</summary><p> 目前，多模态医学图像融合技术已成为研究人员和医生预测疾病和研究病理学的重要手段。然而，如何在保证时间效率的前提下，如何从不同的模态源图像中保留更多独特的特征是一个棘手的问题。为了解决这个问题，我们提出了一种灵活的语义引导架构，该架构具有端到端的掩码优化框架，称为 GeSeNet。具体来说，设计了一个区域掩码模块来加深重要信息的学习，同时修剪冗余计算以减少运行时间。提出了一种边缘增强模块和全局细化模块来修改提取的特征，以提高边缘纹理并调整整体视觉性能。此外，我们引入了一个语义模块，该模块与所提出的融合网络级联，将语义信息传递到我们生成的结果中。在我们提出的方法和十种最先进的方法之间部署了足够的定性和定量比较实验（即 MRI-CT、MRI-PET 和 MRISPECT），这表明我们生成的图像带来了方式。此外，我们还进行了操作效率比较和消融实验，以证明我们提出的方法可以在多模态医学图像融合领域表现出色。 </p></details> | TNNLS2023 |  [Code Link](https://github.com/lok18/GeSeNet) | Jinyuan Liu，各方面均衡，适合做对比方法 |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |


### 统一模型(IVIF和MIF)
| **题目** | **日期** | **摘要** | **录用信息** | **代码** | **备注**
| --- | --- | --- | --- | --- | --- |
| **[MLFuse: 多场景特征联合学习在多模态图像融合中的应用](https://ieeexplore.ieee.org/document/10856398)** | 2025 | <details><summary>Show</summary><p> 多模态图像融合 (MMIF) 需要合成具有详细纹理和突出对象的图像。现有方法倾向于使用一般特征提取来处理不同的融合任务。然而，由于缺乏有针对性的学习途径，这些方法难以跨越各种模式的融合障碍。在这项工作中，我们提出了一个多场景特征联合学习架构 MLFuse，它利用多模态图像的共性来解构融合过程。具体来说，我们构建了一个跨模态知识增强网络，该网络采用多路径标定策略来促进不同图像之间的信息交流。此外，两个专业网络的发展，以维护显着和纹理信息的融合结果。空间 - 光谱域优化网络可以借助空间注意和光谱注意来学习源图像上下文的重要关系。该边缘引导学习网络利用各种接收域的卷积运算来获取图像纹理信息。通过聚合三个网络的输出，得到了期望的融合结果。大量的实验证明了 MLFuse 在红外-可见光图像融合和医学图像融合中的优越性。下游任务 (即目标检测和语义分割) 的出色结果进一步验证了我们方法的高质量融合性能。 </p></details> | TMM2025 |  [Code Link](https://github.com/jialei-sc/MLFuse) | Jinyuan Liu, 同一组参数实现统一融合 |
| **[TIMFusion: 用于图像融合的任务导向、隐式搜索和元初始化的深度模型(Arxiv)](http://arxiv.org/abs/2305.15862v1)** | 2023-05-25 | <details><summary>Show</summary><p>图像融合在各种基于多传感器的视觉系统中发挥着关键作用，特别是对于提高视觉质量和/或提取聚合特征以进行感知。然而，大多数现有的方法只是将图像融合视为一项单独的任务，从而忽略了它与这些下游视觉问题的潜在关系。此外，设计适当的融合架构通常需要大量的工程劳动力。它还缺乏提高当前融合方法的灵活性和泛化能力的机制。为了缓解这些问题，我们建立了一个任务导向、隐式搜索和元初始化 （TIM） 深度模型，以解决具有挑战性的真实场景中的图像融合问题。具体来说，我们首先提出了一种约束策略，以整合来自下游任务的信息，以指导图像融合的无监督学习过程。然后，在这个框架内，我们设计了一个隐式搜索方案，以高效地自动发现融合模型的紧凑架构。此外，还引入了一种前置元初始化技术，以利用发散融合数据来支持对不同类型图像融合任务的快速适应。不同类别的图像融合问题和相关下游任务（例如，视觉增强和语义理解）的定性和定量实验结果证实了我们 TIM 的灵活性和有效性。源代码将在 https://github.com/LiuZhu-CV/TIMFusion 上提供。</p></details> | TPAMI2024 | [Code Link](https://github.com/LiuZhu-CV/TIMFusion) | Jinyuan Liu, 不同参数, 医学图像细节丢失 |
| **[TC-MoA: 用于通用图像融合的任务自定义适配器混合(Arxiv)](http://arxiv.org/abs/2403.12494v2)** | 2024-03-24 | <details><summary>Show</summary><p>通用图像融合旨在整合来自多源图像的重要信息。但是，由于跨任务差距很大，相应的融合机制在实践中差异很大，导致跨子任务的性能有限。为了解决这个问题，我们提出了一种新的任务定制适配器混合 （TC-MoA） 用于通用图像融合，在统一模型中自适应地提示各种融合任务。我们从专家混合 （MoE） 中借鉴了见解，将专家视为有效的调优适配器，以提示预先训练的基础模型。这些适配器在不同的任务之间共享，并受到互信息正则化的约束，确保与不同任务的兼容性，同时为多源图像提供互补性。特定于任务的路由网络自定义这些适配器，以从具有动态主导强度的不同来源提取特定于任务的信息，执行自适应视觉特征提示融合。值得注意的是，我们的 TC-MoA 控制了不同聚变任务的主导强度偏差，成功地将多个聚变任务统一到一个模型中。大量实验表明，TC-MoA 在学习共性方面优于竞争方法，同时保持了与一般图像融合（多模态、多曝光和多焦点）的兼容性，并且在更多的泛化实验中也表现出了惊人的可控性。该代码可在 https://github.com/YangSun22/TC-MoA 获取。</p></details> | CVPR2024 | [Code Link](https://github.com/YangSun22/TC-MoA) | 主观效果都挺好的，参数大但是速度还可以，医学图像颜色有点失真，其他的都挺好的 |
| **[EMMA：等变多模态图像融合(Arxiv)](http://arxiv.org/abs/2305.11443v2)** | 2024 | <details><summary>Show</summary><p>多模态图像融合是一种将来自不同传感器或模态的信息组合在一起的技术，使融合图像能够保留来自每种模态的互补特征，例如功能亮点和纹理细节。然而，由于地面实况融合数据的稀缺性，这种融合模型的有效训练具有挑战性。为了解决这个问题，我们提出了用于端到端自我监督学习的等变多模态 imAge 融合 （EMMA） 范式。我们的方法植根于自然成像响应对某些转换是等变的先验知识。因此，我们引入了一种新的训练范式，其中包括一个融合模块、一个伪传感模块和一个等变融合模块。这些组件使网络训练能够遵循自然传感成像过程的原理，同时满足等变成像先验。大量实验证实，EMMA 可产生高质量的红外可见光和医学图像融合结果，同时促进下游多模态分割和检测任务。</p></details> | CVPR2024 | [Code Link](https://github.com/Zhaozixiang1228/MMIF-EMMA) | Zixiang Zhao，MIF主观不佳，客观OK |
| **[MMDRFuse：用于多模态图像融合的具有动态刷新功能的提炼迷你模型(Arxiv)](http://arxiv.org/abs/2408.15641v1)** | 2024 | <details><summary>Show</summary><p>近年来，多模态图像融合 （MMIF） 已应用于多个领域，吸引了众多学者致力于提高融合性能。然而，普遍的关注点主要集中在架构设计上，而不是训练策略上。作为一项低级视觉任务，图像融合应该快速提供输出图像以供观察和支持下游任务。因此，应避免多余的计算和存储开销。在这项工作中，提出了一种具有动态刷新策略 （MMDRFuse） 的轻量级 Distilled Mini-Model 来实现这一目标。为了追求模型简洁性，通过三个精心设计的监督获得了一个极小的卷积网络，总共有 113 个可训练参数 （0.44 KB）。首先，通过强调外部空间特征的一致性来构建可消化的蒸馏，为目标网络提供具有平衡细节和显著性的软监督。其次，我们开发一个综合损失来平衡来自源图像的像素、梯度和感知线索。第三，使用创新的动态刷新训练策略在训练期间协作历史参数和当前监督，以及自适应调整功能以优化融合网络。在多个公共数据集上的广泛实验表明，我们的方法在模型效率和复杂度方面表现出有希望的优势，在多图像融合任务和下游行人检测应用中具有优异的性能。这项工作的代码在 https://github.com/yanglinDeng/MMDRFuse 上公开提供。</p></details> | ACMMM2024</p></details> | [Code Link](https://github.com/yanglinDeng/MMDRFuse) | 效果一般，目前速度最快 |
| **[CDDFuse: 多模态图像融合的相关驱动双分支特征分解](https://openaccess.thecvf.com/content/CVPR2023/html/Zhao_CDDFuse_Correlation-Driven_Dual-Branch_Feature_Decomposition_for_Multi-Modality_Image_Fusion_CVPR_2023_paper.html)** | 2023 | <details><summary>Show</summary><p> 多模态(MM)图像融合旨在渲染融合图像，以保持不同模态的优点，如功能突出和详细的纹理。为了解决跨模态特征建模和分解理想的模态特定和模态共享特征的挑战，我们提出了一种新的相关驱动特征分解融合(CDDFuse)网络。首先，CDDFuse 使用 Restormer 块来提取跨模态浅层特征。然后，我们引入了一个双分支 Transformer-CNN 特征提取器，它具有 Lite Transformer (LT) 块，利用远程注意力来处理低频全局特征和可逆神经网络 (INN) 块，专注于提取高频局部信息。进一步提出了一种相关驱动损失，使低频特征与基于嵌入信息不相关的高频特征相关。然后，基于lt的全局融合和基于inn的局部融合层输出融合图像。大量的实验表明，我们的CDDFuse在多个融合任务中取得了很好的结果，包括红外可见光图像融合和医学图像融合。我们还表明，CDDFuse可以在统一的基准测试中提高下游红外可见语义分割和目标检测的性能。 </p></details> | CVPR2023 |  [Code Link](https://github.com/Zhaozixiang1228/MMIF-CDDFuse) | Zixiang Zhao，均衡，对比实验首选 |
| **[MUFusion: 一种通用的基于存储单元的无监督图像融合网络](https://www.sciencedirect.com/science/article/abs/pii/S1566253522002202)** | 2023 | <details><summary>Show</summary><p> 现有的图像融合方法都致力于利用单一的深层网络来解决不同的图像融合问题，近年来取得了很好的效果。然而，在这些方法中，由于缺乏地面真实输出，在训练过程中只能利用源图像的外观来生成融合图像，从而导致次优解的产生。为此，我们提出了一种自我进化的训练公式，引入了一种新的记忆单元结构 (MUFusion)。在这个单元中，我们利用在训练过程中获得的中间融合结果进一步协同监督融合图像。这样，我们的融合结果不仅可以从原始输入图像中学习，而且可以从网络本身的中间输出中获益。在此基础上，设计了由内容损失和内存损失两个损失项组成的自适应统一损失函数。特别是基于源图像的活动级映射计算内容丢失，从而约束输出图像包含特定信息。另一方面，在模型输出的基础上得到内存损失，从而使网络产生更高质量的融合结果。考虑到手工制作的活动水平图不能始终如一地反映准确的显著性判断，我们在它们之间放置了两个自适应权重项，以防止这种退化现象。一般来说，我们的 MUFusion 可以有效地处理一系列图像融合任务，包括红外与可见光图像融合、多聚焦图像融合、多曝光图像融合和医学图像融合。特别地，源图像是在信道维度中串联的。然后，采用两个尺度的密集连通特征提取网络对源图像进行深度特征提取。然后，通过特征提取网络中的两个跳跃连接的特征重建块得到融合结果。通过对 4 个图像融合子任务的定性和定量实验，证明了该算法相对于最先进的融合方法的优越性。 </p></details> | Inf.Fusion2023 | [Code Link](https://github.com/AWCXV/MUFusion) | 不同参数，融合结果奇怪，噪声多，EN、AG高；M3FD上速度非常慢，可能并不适合做对比 |
| **[SwinFusion:通过Swin Transformer进行通用图像融合的跨域远程学习](https://ieeexplore.ieee.org/document/9812535)** | 2022 | <details><summary>Show</summary><p> 本研究提出了一种新的基于跨域远程学习和Swin Transformer的通用图像融合框架，称为SwinFusion。一方面，设计了一个注意力引导的跨域模块来实现互补信息和全局交互的充分集成。更具体地说，所提出的方法涉及基于自注意力的域内融合单元和基于交叉注意力的域间融合单元，该单元在同一域和跨域中挖掘和集成长依赖关系。通过远程依赖建模，该网络能够完全实现特定领域的信息提取和跨域互补信息集成，并从全局角度保持适当的表观强度。特别是，我们将移位窗口机制引入到自我注意和交叉注意中，这使得我们的模型能够接收任意大小的图像。另一方面，多场景图像融合问题被推广到具有结构维护、细节保存和适当强度控制的统一框架。此外，由SSIM损失、纹理损失和强度损失组成的精细损失函数驱动网络保留丰富的纹理细节和结构信息，并给出最佳的表观强度。在多模态图像融合和数字摄影图像融合上的大量实验表明，与最先进的统一图像融合算法和特定任务替代方案相比，我们的 SwinFusion 的优越性。 </p></details> | JAS2022 |  [Code Link](https://github.com/Linfeng-Tang/SwinFusion) | Jiayi Ma，均衡，Qabf和SSIM较高 |
| **[SDNet：一种用于实时图像融合的通用挤压和分解网络](https://link.springer.com/article/10.1007/s11263-021-01501-8)** | 2021 | <details><summary>Show</summary><p> 本文提出了一种挤压分解网络(SDNet)，实时实现多模态和数字摄影图像融合。首先，我们将多个融合问题转化为梯度和强度信息的提取和重构，并相应地设计了一个通用的损失函数形式，它由强度项和梯度项组成。对于梯度项，我们引入了一个自适应决策块，根据像素尺度上的纹理丰富度来决定梯度分布的优化目标，从而引导融合图像包含更丰富的纹理细节。对于强度项，我们调整了每个强度损失项的权重，以改变来自不同图像的强度信息的比例，使其可以适应多个图像融合任务。其次，我们将挤压分解的思想引入到图像融合中。具体来说，我们不仅考虑从源图像到融合结果的挤压过程，还考虑了从融合结果到源图像的分解过程。由于分解图像的质量直接取决于融合结果，它可以迫使融合结果包含更多的场景细节。实验结果表明，我们的方法在各种融合任务中的主观视觉效果和定量指标方面优于最先进的方法。此外，我们的方法比最先进的方法快得多，可以处理实时融合任务。 </p></details> | IJCV2021 |  [Code Link](https://github.com/HaoZhang1018/SDNet) | Jiayi Ma，TF框架 |
| **[U2Fusion：统一的无监督图像融合网络](https://ieeexplore.ieee.org/abstract/document/9151265)** | 2020 | <details><summary>Show</summary><p> 本研究提出了一种新颖的统一且无监督的端到端图像融合网络，称为 U2Fusion，它能够解决不同的融合问题，包括多模态、多曝光和多焦点情况。使用特征提取和信息测量，U2Fusion 自动估计相应源图像的重要性并得出自适应信息保存度。因此，不同的融合任务被统一在同一个框架中。基于自适应度，训练网络以保持融合结果和源图像之间的自适应相似性。因此，大大减少了将深度学习应用于图像融合的绊脚石，例如对地面实况和专门设计的指标的要求。通过为不同任务顺序训练单个模型时避免先前融合能力的损失，我们获得了适用于多个融合任务的统一模型。此外，还发布了一个新的对齐的红外和可见图像数据集 RoadScene（可在https://github.com/hanna-xu/RoadScene 获取），为基准评估提供新的选择。三种典型图像融合任务的定性和定量实验结果验证了U2Fusion的有效性和普适性。 </p></details> | TPAMI2020 |  [Code Link](https://github.com/hanna-xu/RoadScene) |  Jiayi Ma，TF框架 |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |
| **[]()** |  | <details><summary>Show</summary><p>  </p></details> |  |  []() |
